"""
PySpark feature definitions for transaction data.

This example demonstrates how to use mlforge with PySpark for
distributed feature computation. Feature functions receive and
return pyspark.sql.DataFrame objects.

Requirements:
    - Java 8+ runtime
    - pip install mlforge[pyspark]
"""

from datetime import timedelta

from pyspark.sql import DataFrame as SparkDataFrame
from pyspark.sql import functions as F

import mlforge as mlf

### Source Definition

source = mlf.Source("data/transactions.parquet")

### Entity Definitions

merchant = mlf.Entity(
    name="merchant",
    join_key="merchant_id",
    from_columns=["merchant"],
)

account = mlf.Entity(
    name="account",
    join_key="account_id",
    from_columns=["cc_num"],
)

user = mlf.Entity(
    name="user",
    join_key="user_id",
    from_columns=["first", "last", "dob"],
)

### Metrics

spend_metrics = mlf.Rolling(
    windows=[timedelta(days=7), "30d", "90d"],
    aggregations={"amt": ["count", "mean", "sum"]},
)


### MERCHANT FEATURES (PySpark)


@mlf.feature(
    source=source,
    entities=[merchant],
    tags=["merchants"],
    description="Total spend by merchant ID (PySpark)",
    timestamp="transaction_date",
    interval=timedelta(days=1),
    metrics=[spend_metrics],
    engine="pyspark",
)
def merchant_spend_pyspark(df: SparkDataFrame) -> SparkDataFrame:
    """Compute merchant spending features using PySpark."""
    # merchant_id is auto-generated by the engine from the merchant entity
    return df.select(
        F.col("merchant_id"),
        F.to_timestamp(F.col("trans_date_trans_time"), "yyyy-MM-dd HH:mm:ss").alias(
            "transaction_date"
        ),
        F.col("amt"),
    )


### ACCOUNT FEATURES (PySpark)


@mlf.feature(
    source=source,
    entities=[account],
    tags=["accounts"],
    description="Total spend by account ID (PySpark)",
    timestamp="transaction_date",
    interval="7d",
    metrics=[spend_metrics],
    engine="pyspark",
)
def account_spend_pyspark(df: SparkDataFrame) -> SparkDataFrame:
    """Compute account spending features using PySpark."""
    # account_id is auto-generated by the engine from the account entity
    return df.select(
        F.col("account_id"),
        F.to_timestamp(F.col("trans_date_trans_time"), "yyyy-MM-dd HH:mm:ss").alias(
            "transaction_date"
        ),
        F.col("amt"),
    )


### USER FEATURES (PySpark)


@mlf.feature(
    source=source,
    entities=[user],
    tags=["users"],
    description="Total spend by user ID (PySpark)",
    timestamp="transaction_date",
    interval=timedelta(days=30),
    metrics=[spend_metrics],
    engine="pyspark",
)
def user_spend_pyspark(df: SparkDataFrame) -> SparkDataFrame:
    """Compute user spending features using PySpark."""
    # user_id is auto-generated by the engine from the user entity
    return df.select(
        F.col("user_id"),
        F.to_timestamp(F.col("trans_date_trans_time"), "yyyy-MM-dd HH:mm:ss").alias(
            "transaction_date"
        ),
        F.col("amt"),
    )

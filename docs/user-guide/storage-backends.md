# Storage Backends

mlforge supports multiple storage backends for persisting built features. Configure backends via `mlforge.yaml` profiles or directly in your Definitions.

## Overview

| Backend | Use Case | Setup |
|---------|----------|-------|
| **LocalStore** | Development, single-machine | None |
| **S3Store** | AWS production | AWS credentials |
| **GCSStore** | GCP production | GCP credentials |
| **UnityCatalogStore** | Databricks production | Databricks workspace |

---

## Configuration via Profiles

The recommended way to configure storage is via `mlforge.yaml`:

```yaml
default_profile: dev

profiles:
  dev:
    offline_store:
      KIND: local
      path: ./feature_store

  staging:
    offline_store:
      KIND: s3
      bucket: staging-features
      prefix: v1

  production:
    offline_store:
      KIND: s3
      bucket: prod-features
      prefix: v1
      region: us-west-2
```

Switch between profiles:

```bash
mlforge build --profile production
# or
export MLFORGE_PROFILE=production
mlforge build
```

---

## LocalStore

The default storage backend that writes features to the local filesystem as Parquet files.

### Profile Configuration

```yaml
profiles:
  dev:
    offline_store:
      KIND: local
      path: ./feature_store
```

### Python Configuration

```python
import mlforge as mlf

defs = mlf.Definitions(
    name="my-project",
    features=[features],
    offline_store=mlf.LocalStore(path="./feature_store")
)
```

### Storage Structure

Features are stored in a versioned directory structure:

```
feature_store/
├── user_spend/
│   ├── 1.0.0/
│   │   ├── data.parquet          # Feature data
│   │   └── .meta.json            # Version metadata
│   ├── 1.0.1/
│   │   ├── data.parquet
│   │   └── .meta.json
│   ├── _latest.json              # Pointer to latest version
│   └── .gitignore                # Auto-generated
└── merchant_revenue/
    ├── 1.0.0/
    │   ├── data.parquet
    │   └── .meta.json
    └── _latest.json
```

### Git Integration

LocalStore auto-generates `.gitignore` to exclude data files:

```gitignore
# Auto-generated by mlforge
*/data.parquet
```

Commit metadata to Git, rebuild data with `mlforge sync`:

```bash
git pull
mlforge sync  # Rebuilds data from metadata
```

### When to Use

- Local development and debugging
- Small datasets that fit on disk
- CI/CD pipelines with ephemeral storage
- Single-machine deployments

---

## S3Store

Cloud storage backend for Amazon S3.

### Profile Configuration

```yaml
profiles:
  production:
    offline_store:
      KIND: s3
      bucket: my-features
      prefix: prod/v1
      region: us-west-2  # optional
```

### Python Configuration

```python
import mlforge as mlf

defs = mlf.Definitions(
    name="my-project",
    features=[features],
    offline_store=mlf.S3Store(
        bucket="my-features",
        prefix="prod/v1",
        region="us-west-2"
    )
)
```

### AWS Credentials

S3Store uses standard AWS credential resolution:

=== "Environment Variables"
    ```bash
    export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
    export AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
    export AWS_DEFAULT_REGION=us-west-2
    ```

=== "AWS CLI"
    ```bash
    aws configure
    ```

=== "IAM Role"
    When running on AWS (EC2/ECS/Lambda), use IAM roles - no credentials needed.

### IAM Policy

Minimal policy for feature store access:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::my-features",
        "arn:aws:s3:::my-features/*"
      ]
    }
  ]
}
```

### When to Use

- Production deployments on AWS
- Team collaboration (shared storage)
- Large datasets
- Multi-environment workflows

---

## GCSStore

Cloud storage backend for Google Cloud Storage.

### Profile Configuration

```yaml
profiles:
  production:
    offline_store:
      KIND: gcs
      bucket: my-features
      prefix: prod/v1
```

### Python Configuration

```python
import mlforge as mlf

defs = mlf.Definitions(
    name="my-project",
    features=[features],
    offline_store=mlf.GCSStore(
        bucket="my-features",
        prefix="prod/v1"
    )
)
```

### Installation

GCSStore requires the `gcs` extra:

```bash
pip install mlforge-sdk[gcs]
```

### Authentication

=== "Application Default Credentials"
    ```bash
    gcloud auth application-default login
    ```

=== "Service Account"
    ```bash
    export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
    ```

=== "Workload Identity"
    When running on GCP (GKE, Cloud Run), use Workload Identity - no credentials needed.

### When to Use

- GCP-based deployments
- Integration with BigQuery, Vertex AI
- Teams already using GCP

---

## UnityCatalogStore

Databricks Unity Catalog storage backend using Delta Lake tables.

### Prerequisites

- Databricks workspace with Unity Catalog enabled
- Active SparkSession (run in Databricks notebook or job)
- `engine="pyspark"` in Definitions

### Installation

```bash
pip install mlforge-sdk[databricks]
```

### Python Configuration

```python
import mlforge as mlf
from mlforge.stores import UnityCatalogStore

defs = mlf.Definitions(
    name="my-project",
    features=[user_spend],
    offline_store=UnityCatalogStore(
        catalog="main",
        schema="features",
    ),
    engine="pyspark",
)
```

### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `catalog` | str | required | Unity Catalog catalog name |
| `schema` | str | "features" | Schema (database) name |
| `volume` | str | None | Optional volume for artifacts |

### Storage Structure

Features are stored as Delta tables with version tracking:

```
Unity Catalog:
├── {catalog}
│   └── {schema}
│       ├── user_spend (Delta table)
│       │   └── versions via Delta time travel
│       ├── merchant_risk (Delta table)
│       └── _mlforge_metadata (Delta table)
│           └── Tracks version mapping: mlforge semver → Delta version
```

### Versioning

UnityCatalogStore maps mlforge semantic versions to Delta table versions:

| mlforge Version | Delta Version | Description |
|-----------------|---------------|-------------|
| 1.0.0 | 0 | Initial build |
| 1.0.1 | 1 | Data change |
| 1.1.0 | 2 | Schema change |

Read specific versions using Delta time travel:

```python
# Read latest
df = store.read("user_spend")

# Read specific version
df = store.read("user_spend", feature_version="1.0.0")
```

### Metadata Table

The `_mlforge_metadata` table tracks:

- Feature name and version
- Delta table version mapping
- Schema and content hashes
- Row counts and timestamps
- Entity and source information

### When to Use

- Databricks-native deployments
- Unity Catalog for governance and lineage
- Delta Lake for ACID transactions
- Integration with Databricks ML workflows

### Example: Full Databricks Setup

```python
import mlforge as mlf
from mlforge.stores import UnityCatalogStore, DatabricksOnlineStore

# Offline store: Delta tables in Unity Catalog
offline_store = UnityCatalogStore(
    catalog="main",
    schema="features",
)

# Online store: Databricks Online Tables (auto-syncs from Delta)
online_store = DatabricksOnlineStore(
    catalog="main",
    schema="features_online",
    sync_mode="triggered",
)

defs = mlf.Definitions(
    name="fraud-detection",
    features=[user_spend, merchant_risk],
    offline_store=offline_store,
    online_store=online_store,
    engine="pyspark",
)

# Build features
defs.build(online=True)
```

---

## Multi-Environment Setup

Use profiles to manage different environments:

```yaml
# mlforge.yaml
default_profile: dev

profiles:
  dev:
    offline_store:
      KIND: local
      path: ./feature_store

  staging:
    offline_store:
      KIND: s3
      bucket: staging-features
      prefix: features

  production:
    offline_store:
      KIND: s3
      bucket: ${oc.env:PROD_BUCKET}
      prefix: features
      region: ${oc.env:AWS_REGION}
```

Build to different environments:

```bash
# Development
mlforge build

# Staging
mlforge build --profile staging

# Production (with env vars)
PROD_BUCKET=prod-features AWS_REGION=us-west-2 mlforge build --profile production
```

---

## Performance Considerations

| Backend | Pros | Cons |
|---------|------|------|
| **LocalStore** | Fast (no network), simple setup | Limited by disk, not distributed |
| **S3Store** | Unlimited storage, high durability | Network latency, transfer costs |
| **GCSStore** | GCP integration, unlimited storage | Network latency, GCP-only |
| **UnityCatalogStore** | Governance, Delta features | Databricks-only, requires Spark |

### Optimization Tips

1. **Colocate compute and storage** - Same region reduces latency
2. **Batch builds** - Build all features in one `mlforge build` call
3. **Use IAM roles** - Avoid credential management on cloud platforms

---

## Next Steps

- [Online Stores](online-stores.md) - Configure Redis, DynamoDB, or Databricks Online Tables
- [Building Features](building-features.md) - Build features to storage
- [Retrieving Features](retrieving-features.md) - Read features from storage
- [Store API Reference](../api/store.md) - Detailed API documentation

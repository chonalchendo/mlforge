{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"mlforge","text":"<p>A simple feature store SDK for machine learning workflows.</p> <p>mlforge provides a lightweight, Python-first approach to feature engineering and management. It focuses on simplicity while delivering the essential capabilities you need: feature definition, building, and point-in-time correct retrieval.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Simple API - Define features with a decorator, build with one command</li> <li>Point-in-time correctness - Automatic temporal joins prevent data leakage</li> <li>Feature versioning - Automatic semantic versioning with content-hash tracking</li> <li>Multiple compute engines - Choose Polars or DuckDB based on your needs</li> <li>Online &amp; offline stores - Redis for real-time serving, Parquet for batch</li> <li>Type-safe - Built on Polars with unified type system across engines</li> <li>CLI included - Build, list, sync, and manage features from the command line</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from mlforge import feature, Definitions, LocalStore\nimport polars as pl\n\n# Define a feature\n@feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"transaction_time\",\n    description=\"User spending statistics\"\n)\ndef user_spend_stats(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.group_by(\"user_id\").agg(\n        pl.col(\"amount\").mean().alias(\"avg_spend\"),\n        pl.col(\"amount\").count().alias(\"transaction_count\")\n    )\n\n# Register and build\ndefs = Definitions(\n    name=\"my-project\",\n    features=[user_spend_stats],\n    offline_store=LocalStore(\"./feature_store\")\n)\n\ndefs.build()\n</code></pre> <p>Retrieve features with point-in-time correctness:</p> <pre><code>from mlforge import get_training_data\nimport polars as pl\n\n# Load your entity data\nentities = pl.read_parquet(\"data/labels.parquet\")\n\n# Get features joined correctly (supports versioning)\ntraining_data = get_training_data(\n    features=[\n        \"user_spend_stats\",           # Latest version\n        (\"merchant_risk\", \"1.0.0\"),   # Pinned version\n    ],\n    entity_df=entities,\n    timestamp=\"event_time\"\n)\n</code></pre> <p>Real-time inference with Redis:</p> <pre><code>from mlforge import get_online_features\nfrom mlforge.online import RedisStore\n\nstore = RedisStore(host=\"localhost\")\n\n# Retrieve features for inference\nfeatures_df = get_online_features(\n    features=[\"user_spend_stats\"],\n    entity_df=request_df,\n    store=store,\n    entities=[with_user_id],\n)\n</code></pre>"},{"location":"#why-mlforge","title":"Why mlforge?","text":"<p>Built for simplicity - No infrastructure to manage, no complex configuration. Just Python functions and Parquet files.</p> <p>Point-in-time correctness by default - Temporal features are automatically joined using asof joins, preventing data leakage in your training sets.</p> <p>Local development - Perfect for prototyping, small projects, or teams that don't need distributed infrastructure.</p>"},{"location":"#installation","title":"Installation","text":"pipuv <pre><code>pip install mlforge-sdk\n</code></pre> <pre><code>uv add mlforge-sdk\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Build your first feature in 5 minutes</li> <li>Defining Features - Learn the <code>@feature</code> decorator</li> <li>CLI Reference - Command-line tools for building features</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>mlforge provides a command-line interface for building and listing features.</p>"},{"location":"cli/#installation-verification","title":"Installation Verification","text":"<p>Check that the CLI is installed:</p> <pre><code>mlforge --help\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#build","title":"build","text":"<p>Materialize features to offline storage.</p> <pre><code>mlforge build [TARGET] [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"<ul> <li><code>TARGET</code> (optional): Path to definitions file. Auto-discovers <code>definitions.py</code> if not specified.</li> </ul>"},{"location":"cli/#options","title":"Options","text":"<ul> <li><code>--features NAMES</code>: Comma-separated list of feature names to build. Defaults to all features.</li> <li><code>--tags TAGS</code>: Comma-separated list of feature tags to build. Mutually exclusive with <code>--features</code>.</li> <li><code>--force</code>, <code>-f</code>: Overwrite existing features. Defaults to <code>False</code>.</li> <li><code>--online</code>: Build to online store (e.g., Redis) instead of offline store. Defaults to <code>False</code>.</li> <li><code>--no-preview</code>: Disable feature preview output. Defaults to <code>False</code> (preview enabled).</li> <li><code>--preview-rows N</code>: Number of preview rows to display. Defaults to <code>5</code>.</li> <li><code>--verbose</code>, <code>-v</code>: Enable debug logging. Defaults to <code>False</code>.</li> </ul>"},{"location":"cli/#examples","title":"Examples","text":"<p>Build all features:</p> <pre><code>mlforge build definitions.py\n</code></pre> <p>Build all features (auto-discovers definitions.py):</p> <pre><code>mlforge build\n</code></pre> <p>Build specific features:</p> <pre><code>mlforge build definitions.py --features user_age,user_tenure\n</code></pre> <p>Build features by tag:</p> <pre><code>mlforge build --tags user_metrics,demographics\n</code></pre> <p>Force rebuild all features:</p> <pre><code>mlforge build definitions.py --force\n</code></pre> <p>Build with verbose logging:</p> <pre><code>mlforge build definitions.py --verbose\n</code></pre> <p>Build without preview:</p> <pre><code>mlforge build definitions.py --no-preview\n</code></pre> <p>Custom preview size:</p> <pre><code>mlforge build definitions.py --preview-rows 10\n</code></pre> <p>Build to online store (Redis):</p> <pre><code>mlforge build --online\n</code></pre> <p>This extracts the latest value per entity and writes to the configured online store.</p>"},{"location":"cli/#output","title":"Output","text":"<p>The command displays:</p> <ol> <li>Progress messages for each feature</li> <li>Preview of materialized data (unless <code>--no-preview</code>)</li> <li>Summary of built features</li> <li>Storage paths for each feature</li> </ol> <p>Example output:</p> <pre><code>Materializing user_total_spend\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 total_spend \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 150.0       \u2502\n\u2502 u2      \u2502 250.0       \u2502\n\u2502 u3      \u2502 100.0       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nMaterializing user_avg_spend\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 avg_spend \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 50.0      \u2502\n\u2502 u2      \u2502 83.3      \u2502\n\u2502 u3      \u2502 100.0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nBuilt features:\n  user_total_spend -&gt; feature_store/user_total_spend.parquet\n  user_avg_spend -&gt; feature_store/user_avg_spend.parquet\n\nBuilt 2 features\n</code></pre>"},{"location":"cli/#error-handling","title":"Error Handling","text":"<p>DefinitionsLoadError: If the definitions file cannot be loaded:</p> <pre><code>$ mlforge build missing.py\nError: Could not load definitions from missing.py\n</code></pre> <p>FeatureMaterializationError: If a feature function fails:</p> <pre><code>$ mlforge build definitions.py\nError: Feature 'user_age' failed: Feature function returned None\nHint: Make sure your feature function returns a DataFrame.\n</code></pre>"},{"location":"cli/#inspect","title":"inspect","text":"<p>Display detailed metadata for a specific feature.</p> <pre><code>mlforge inspect FEATURE_NAME [TARGET]\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"<ul> <li><code>FEATURE_NAME</code> (required): Name of the feature to inspect</li> <li><code>TARGET</code> (optional): Path to definitions file. Auto-discovers <code>definitions.py</code> if not specified.</li> </ul>"},{"location":"cli/#examples_1","title":"Examples","text":"<p>Inspect a specific feature:</p> <pre><code>mlforge inspect user_spend\n</code></pre> <p>Inspect with custom definitions file:</p> <pre><code>mlforge inspect user_spend definitions.py\n</code></pre>"},{"location":"cli/#output_1","title":"Output","text":"<p>Displays detailed feature metadata including:</p> <ul> <li>Feature configuration (entity, keys, timestamp, interval)</li> <li>Storage details (path, source, row count)</li> <li>Column information with types and aggregations</li> <li>Tags and description</li> <li>Last build timestamp</li> </ul> <p>Example output:</p> <pre><code>\u250c\u2500 Feature: user_spend \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total spend features for users                              \u2502\n\u2502                                                              \u2502\n\u2502 Path: ./feature_store/user_spend.parquet                   \u2502\n\u2502 Source: data/transactions.parquet                           \u2502\n\u2502 Entity: user_id                                              \u2502\n\u2502 Keys: user_id                                                \u2502\n\u2502 Timestamp: transaction_date                                  \u2502\n\u2502 Interval: 1d                                                 \u2502\n\u2502 Tags: users, spending                                        \u2502\n\u2502 Row Count: 50,000                                            \u2502\n\u2502 Last Updated: 2024-01-16T08:30:00Z                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nColumns\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                     \u2502 Type   \u2502 Input  \u2502 Aggregation \u2502 Window \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 user_id                  \u2502 Utf8   \u2502 -      \u2502 -           \u2502 -      \u2502\n\u2502 transaction_date         \u2502 Date   \u2502 -      \u2502 -           \u2502 -      \u2502\n\u2502 amt__sum__7d             \u2502 Float64\u2502 amt    \u2502 sum         \u2502 7d     \u2502\n\u2502 amt__count__7d           \u2502 UInt32 \u2502 amt    \u2502 count       \u2502 7d     \u2502\n\u2502 amt__mean__30d           \u2502 Float64\u2502 amt    \u2502 mean        \u2502 30d    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/#error-handling_1","title":"Error Handling","text":"<p>No metadata found: If the feature hasn't been built yet:</p> <pre><code>$ mlforge inspect user_spend\nError: No metadata found for feature 'user_spend'.\nRun 'mlforge build' to generate metadata.\n</code></pre>"},{"location":"cli/#manifest","title":"manifest","text":"<p>Display or regenerate the feature store manifest.</p> <pre><code>mlforge manifest [TARGET] [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_2","title":"Arguments","text":"<ul> <li><code>TARGET</code> (optional): Path to definitions file. Auto-discovers <code>definitions.py</code> if not specified.</li> </ul>"},{"location":"cli/#options_1","title":"Options","text":"<ul> <li><code>--regenerate</code>: Rebuild manifest.json from individual .meta.json files. Defaults to <code>False</code>.</li> </ul>"},{"location":"cli/#examples_2","title":"Examples","text":"<p>Display manifest summary:</p> <pre><code>mlforge manifest\n</code></pre> <p>Regenerate consolidated manifest file:</p> <pre><code>mlforge manifest --regenerate\n</code></pre> <p>With custom definitions file:</p> <pre><code>mlforge manifest definitions.py --regenerate\n</code></pre>"},{"location":"cli/#output_2","title":"Output","text":"<p>Without <code>--regenerate</code> - Displays a summary table:</p> <pre><code>Feature Store Manifest\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Feature          \u2502 Entity   \u2502 Rows   \u2502 Columns \u2502 Last Updated        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 merchant_spend   \u2502 merchant \u2502 15,482 \u2502 8       \u2502 2024-01-16T08:30:00\u2502\n\u2502 user_spend       \u2502 user_id  \u2502 50,000 \u2502 12      \u2502 2024-01-16T08:25:00\u2502\n\u2502 account_spend    \u2502 account  \u2502 8,234  \u2502 10      \u2502 2024-01-16T08:28:00\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>With <code>--regenerate</code> - Creates <code>manifest.json</code> and displays confirmation:</p> <pre><code>$ mlforge manifest --regenerate\nRegenerated manifest.json with 3 features\n</code></pre> <p>The consolidated manifest is written to:</p> <ul> <li>LocalStore: <code>&lt;store_path&gt;/manifest.json</code></li> <li>S3Store: <code>s3://&lt;bucket&gt;/&lt;prefix&gt;/manifest.json</code></li> </ul>"},{"location":"cli/#error-handling_2","title":"Error Handling","text":"<p>No features found: If no features have been built:</p> <pre><code>$ mlforge manifest\nWarning: No feature metadata found. Run 'mlforge build' first.\n</code></pre>"},{"location":"cli/#validate","title":"validate","text":"<p>Run validation on features without building them.</p> <pre><code>mlforge validate [TARGET] [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_3","title":"Arguments","text":"<ul> <li><code>TARGET</code> (optional): Path to definitions file. Auto-discovers <code>definitions.py</code> if not specified.</li> </ul>"},{"location":"cli/#options_2","title":"Options","text":"<ul> <li><code>--features NAMES</code>: Comma-separated list of feature names to validate. Defaults to all features.</li> <li><code>--tags TAGS</code>: Comma-separated list of feature tags to validate. Mutually exclusive with <code>--features</code>.</li> <li><code>--verbose</code>, <code>-v</code>: Enable debug logging. Defaults to <code>False</code>.</li> </ul>"},{"location":"cli/#examples_3","title":"Examples","text":"<p>Validate all features:</p> <pre><code>mlforge validate definitions.py\n</code></pre> <p>Validate specific features:</p> <pre><code>mlforge validate --features merchant_transactions,user_transactions\n</code></pre> <p>Validate features by tag:</p> <pre><code>mlforge validate --tags transactions\n</code></pre>"},{"location":"cli/#output_3","title":"Output","text":"<p>Displays validation results for each feature:</p> <pre><code>Validating merchant_transactions...\n\u2713 All validations passed for merchant_transactions\n\nValidating user_transactions...\n\u2717 Validation failed for user_transactions\n  - Column 'amount': 3 values &lt; 0 (greater_than_or_equal(0))\n\nValidated 2 features (1 passed, 1 failed)\n</code></pre>"},{"location":"cli/#error-handling_3","title":"Error Handling","text":"<p>ValidationError: If any feature fails validation:</p> <pre><code>$ mlforge validate definitions.py\nError: Validation failed for 1 feature(s)\nExit code: 1\n</code></pre> <p>The command exits with code 1 if any validations fail, making it suitable for CI/CD pipelines.</p>"},{"location":"cli/#sync","title":"sync","text":"<p>Rebuild features from metadata files (for Git-based collaboration).</p> <pre><code>mlforge sync [TARGET] [OPTIONS]\n</code></pre> <p>LocalStore Only</p> <p>The sync command only works with <code>LocalStore</code>. Cloud stores (S3, etc.) already share data between teammates, so syncing is not needed.</p>"},{"location":"cli/#arguments_4","title":"Arguments","text":"<ul> <li><code>TARGET</code> (optional): Path to definitions file. Auto-discovers <code>definitions.py</code> if not specified.</li> </ul>"},{"location":"cli/#options_3","title":"Options","text":"<ul> <li><code>--features NAMES</code>: Comma-separated list of feature names to sync. Defaults to all features with missing data.</li> <li><code>--dry-run</code>: Show what would be synced without actually rebuilding. Defaults to <code>False</code>.</li> <li><code>--force</code>: Rebuild even if source data hash differs. Defaults to <code>False</code>.</li> <li><code>--verbose</code>, <code>-v</code>: Enable debug logging. Defaults to <code>False</code>.</li> </ul>"},{"location":"cli/#how-it-works","title":"How It Works","text":"<p>The sync command helps teams collaborate on feature definitions via Git:</p> <ol> <li>Metadata is committed to Git: <code>.meta.json</code> and <code>_latest.json</code> files</li> <li>Data files are ignored: <code>data.parquet</code> files are excluded via auto-generated <code>.gitignore</code></li> <li>Teammates rebuild locally: Run <code>mlforge sync</code> to recreate data from metadata</li> </ol> <p>For each feature, sync will:</p> <ol> <li>Check if metadata exists but data file is missing</li> <li>Compute hash of current source data file</li> <li>Compare with <code>source_hash</code> stored in metadata</li> <li>If hashes match \u2192 rebuild data from feature function</li> <li>If hashes differ \u2192 error (use <code>--force</code> to override)</li> </ol>"},{"location":"cli/#examples_4","title":"Examples","text":"<p>Preview what would be synced:</p> <pre><code>mlforge sync --dry-run\n</code></pre> <p>Sync all features with missing data:</p> <pre><code>mlforge sync\n</code></pre> <p>Sync specific features:</p> <pre><code>mlforge sync --features user_spend,merchant_spend\n</code></pre> <p>Force sync even if source data changed:</p> <pre><code>mlforge sync --force\n</code></pre> <p>With custom definitions file:</p> <pre><code>mlforge sync definitions.py --features user_spend\n</code></pre>"},{"location":"cli/#output_4","title":"Output","text":"<p>Dry-run mode - Shows what would be synced:</p> <pre><code>[Dry Run] Would sync 2 features:\n  - user_spend (v1.2.0)\n  - merchant_spend (v2.0.1)\n\nRun without --dry-run to sync\n</code></pre> <p>Normal mode - Rebuilds features and shows progress:</p> <pre><code>Syncing user_spend (v1.2.0)...\n\u2713 Source hash matches (abc123def456)\n\u2713 Rebuilt user_spend\n\nSyncing merchant_spend (v2.0.1)...\n\u2713 Source hash matches (789abc012def)\n\u2713 Rebuilt merchant_spend\n\nSynced 2 features\n</code></pre> <p>No features to sync:</p> <pre><code>No features need syncing\n</code></pre>"},{"location":"cli/#error-handling_4","title":"Error Handling","text":"<p>Source data changed: If source hash differs from metadata:</p> <pre><code>$ mlforge sync --features user_spend\nError: Source data has changed for feature 'user_spend' (v1.2.0)\n\nExpected hash: abc123def456\nCurrent hash:  xyz789abc012\n\nThis means the source data file has been modified since this version\nwas built. Rebuilding with different source data may produce different\nresults.\n\nOptions:\n  - Restore the original source data file\n  - Use --force to rebuild anyway (creates new version)\n  - Check with your team if source data should have changed\n</code></pre> <p>Not a LocalStore: If using S3Store or other cloud storage:</p> <pre><code>$ mlforge sync\nError: Sync only works with LocalStore.\nCloud stores (S3Store) already share data between teammates.\n</code></pre> <p>Missing source file: If the source data file doesn't exist:</p> <pre><code>$ mlforge sync --features user_spend\nError: Source file not found: data/transactions.parquet\nCannot verify source hash or rebuild feature.\n</code></pre>"},{"location":"cli/#use-cases","title":"Use Cases","text":"<p>After pulling changes from Git:</p> <pre><code>git pull\nmlforge sync\n</code></pre> <p>Setting up new development environment:</p> <pre><code>git clone https://github.com/team/ml-features.git\ncd ml-features\nmlforge sync\n</code></pre> <p>Checking if features are out of sync:</p> <pre><code>mlforge sync --dry-run\n</code></pre>"},{"location":"cli/#when-not-to-use-sync","title":"When NOT to Use Sync","text":"<ul> <li>Cloud stores: Data is already shared via S3/GCS</li> <li>Source data changed intentionally: Use <code>mlforge build --force</code> to create a new version</li> <li>Initial setup: Use <code>mlforge build</code> for first-time feature creation</li> </ul>"},{"location":"cli/#versions","title":"versions","text":"<p>List all versions of a feature.</p> <pre><code>mlforge versions FEATURE_NAME [TARGET]\n</code></pre>"},{"location":"cli/#arguments_5","title":"Arguments","text":"<ul> <li><code>FEATURE_NAME</code> (required): Name of the feature to list versions for</li> <li><code>TARGET</code> (optional): Path to definitions file. Auto-discovers <code>definitions.py</code> if not specified.</li> </ul>"},{"location":"cli/#examples_5","title":"Examples","text":"<p>List versions of a feature:</p> <pre><code>mlforge versions user_spend\n</code></pre>"},{"location":"cli/#output_5","title":"Output","text":"<p>Displays a table of all versions:</p> <pre><code>Versions of user_spend\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Version \u2502 Created             \u2502 Updated             \u2502 Rows       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1.0.0   \u2502 2024-01-10T08:00:00\u2502 2024-01-10T08:00:00\u2502 50,000     \u2502\n\u2502 1.1.0   \u2502 2024-01-15T10:30:00\u2502 2024-01-15T10:30:00\u2502 52,500     \u2502\n\u2502 2.0.0   \u2502 2024-01-20T14:00:00\u2502 2024-01-20T14:00:00\u2502 55,000     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nLatest: 2.0.0\n</code></pre>"},{"location":"cli/#list","title":"list","text":"<p>Display all registered features in a table.</p> <pre><code>mlforge list [TARGET] [OPTIONS]\n</code></pre>"},{"location":"cli/#arguments_6","title":"Arguments","text":"<ul> <li><code>TARGET</code> (optional): Path to definitions file. Auto-discovers <code>definitions.py</code> if not specified.</li> </ul>"},{"location":"cli/#options_4","title":"Options","text":"<ul> <li><code>--tags TAGS</code>: Comma-separated list of tags to filter features by. Defaults to showing all features.</li> </ul>"},{"location":"cli/#examples_6","title":"Examples","text":"<p>List all features:</p> <pre><code>mlforge list definitions.py\n</code></pre> <p>List from current directory (auto-discovers definitions.py):</p> <pre><code>mlforge list\n</code></pre> <p>List features by tag:</p> <pre><code>mlforge list --tags user_metrics\n</code></pre>"},{"location":"cli/#output_6","title":"Output","text":"<p>Displays a formatted table with feature metadata:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                 \u2502 Keys             \u2502 Source                   \u2502 Tags         \u2502 Description               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 user_total_spend     \u2502 [user_id]        \u2502 data/transactions.parquet\u2502 user_metrics \u2502 Total spend by user       \u2502\n\u2502 user_spend_mean_30d  \u2502 [user_id]        \u2502 data/transactions.parquet\u2502 user_metrics \u2502 30d rolling avg spend     \u2502\n\u2502 merchant_total_spend \u2502 [merchant_id]    \u2502 data/transactions.parquet\u2502 -            \u2502 Total spend by merchant   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/#global-options","title":"Global Options","text":"<p>These options work with any command:</p>"},{"location":"cli/#-verbose-v","title":"--verbose, -v","text":"<p>Enable debug logging:</p> <pre><code>mlforge build definitions.py --verbose\nmlforge list definitions.py --verbose\n</code></pre> <p>Debug output includes:</p> <ul> <li>Module loading details</li> <li>Feature registration logs</li> <li>Source data loading information</li> <li>Storage operations</li> </ul> <p>Example verbose output:</p> <pre><code>DEBUG: Loading definitions from definitions.py\nDEBUG: Registered feature: user_total_spend\nDEBUG: Registered feature: user_avg_spend\nINFO: Materializing user_total_spend\nDEBUG: Loading source: data/transactions.parquet\nDEBUG: Writing to: feature_store/user_total_spend.parquet\n</code></pre>"},{"location":"cli/#definitions-file","title":"Definitions File","text":"<p>The <code>TARGET</code> parameter specifies a Python file containing a <code>Definitions</code> object. If not provided, mlforge will automatically search for <code>definitions.py</code> in your project directory.</p>"},{"location":"cli/#auto-discovery","title":"Auto-Discovery","text":"<p>When <code>TARGET</code> is omitted, mlforge searches for <code>definitions.py</code>:</p> <pre><code># Automatically finds definitions.py in current directory or subdirectories\nmlforge build\nmlforge list\n</code></pre> <p>The search starts from the project root (identified by <code>pyproject.toml</code>, <code>.git</code>, etc.) and looks recursively, skipping common directories like <code>.venv</code> and <code>node_modules</code>.</p>"},{"location":"cli/#structure","title":"Structure","text":"<pre><code># definitions.py\nfrom mlforge import Definitions, LocalStore\nimport features\n\ndefs = Definitions(\n    name=\"my-project\",\n    features=[features],\n    offline_store=LocalStore(\"./feature_store\")\n)\n</code></pre>"},{"location":"cli/#naming-convention","title":"Naming Convention","text":"<p>The <code>Definitions</code> object must be named <code>defs</code>:</p> <pre><code># Good\ndefs = Definitions(...)\n\n# Bad - won't be found\ndefinitions = Definitions(...)\nfeature_store = Definitions(...)\n</code></pre>"},{"location":"cli/#module-vs-file-path","title":"Module vs. File Path","text":"<p>You can use either a file path or a module path:</p> <pre><code># File path (recommended)\nmlforge build definitions.py\nmlforge build path/to/definitions.py\n\n# Module path (if installed)\nmlforge build mypackage.definitions\n</code></pre>"},{"location":"cli/#exit-codes","title":"Exit Codes","text":"<p>The CLI uses these exit codes:</p> <ul> <li><code>0</code>: Success</li> <li><code>1</code>: Error (load failure, materialization failure, etc.)</li> </ul> <p>Use in scripts:</p> <pre><code>mlforge build definitions.py\nif [ $? -eq 0 ]; then\n    echo \"Build succeeded\"\nelse\n    echo \"Build failed\"\n    exit 1\nfi\n</code></pre>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"<p>Currently, mlforge does not use environment variables for configuration. All settings are specified via:</p> <ol> <li>Command-line options</li> <li>Definitions file configuration</li> </ol>"},{"location":"cli/#shell-completion","title":"Shell Completion","text":"<p>mlforge uses <code>cyclopts</code> for CLI parsing. Shell completion may be supported in future versions.</p>"},{"location":"cli/#integration-examples","title":"Integration Examples","text":""},{"location":"cli/#makefile","title":"Makefile","text":"<pre><code>.PHONY: features\nfeatures:\n    mlforge build definitions.py --force\n\n.PHONY: list-features\nlist-features:\n    mlforge list definitions.py\n\n.PHONY: build-prod\nbuild-prod:\n    mlforge build definitions.py --no-preview\n</code></pre>"},{"location":"cli/#cicd-pipeline","title":"CI/CD Pipeline","text":"<pre><code># .github/workflows/build-features.yml\nname: Build Features\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n        with:\n          python-version: \"3.13\"\n      - name: Install dependencies\n        run: pip install mlforge-sdk\n      - name: Build features\n        run: mlforge build definitions.py --no-preview\n</code></pre>"},{"location":"cli/#pre-commit-hook","title":"Pre-commit Hook","text":"<pre><code># .git/hooks/pre-commit\n#!/bin/bash\nmlforge build definitions.py --no-preview\nif [ $? -ne 0 ]; then\n    echo \"Feature build failed. Fix errors before committing.\"\n    exit 1\nfi\n</code></pre>"},{"location":"cli/#next-steps","title":"Next Steps","text":"<ul> <li>Building Features - Detailed build guide</li> <li>Defining Features - Feature definition reference</li> <li>API Reference - Python API documentation</li> </ul>"},{"location":"api/core/","title":"Core API","text":"<p>The core module provides the main abstractions for defining and managing features.</p>"},{"location":"api/core/#classes","title":"Classes","text":""},{"location":"api/core/#mlforge.core.Feature","title":"mlforge.core.Feature  <code>dataclass</code>","text":"<p>Container for a feature definition and its transformation function.</p> <p>Features are created using the @feature decorator and contain metadata about the feature's source, keys, and timestamp requirements.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Feature name derived from the decorated function</p> <code>source</code> <code>str</code> <p>Path to the data source file (parquet/csv)</p> <code>keys</code> <code>list[str]</code> <p>Column names that uniquely identify entities</p> <code>tags</code> <code>list[str] | None</code> <p>Feature tags to group features together</p> <code>timestamp</code> <code>str | None</code> <p>Column name for temporal features, enables point-in-time joins</p> <code>description</code> <code>str | None</code> <p>Human-readable feature description</p> <code>interval</code> <code>str | None</code> <p>Time interval for rolling aggregations (e.g., \"1h\", \"1d\")</p> <code>metrics</code> <code>list[MetricKind] | None</code> <p>Aggregation metrics to compute over rolling windows</p> <code>engine</code> <code>str | None</code> <p>Compute engine to use for this feature (\"polars\" or \"duckdb\")</p> <code>fn</code> <code>FeatureFunction</code> <p>The transformation function that computes the feature</p> Example <p>@feature(keys=[\"user_id\"], source=\"data/users.parquet\") def user_age(df):     return df.with_columns(...)</p> <p>@feature(keys=[\"user_id\"], source=\"data/transactions.parquet\", engine=\"duckdb\") def user_spend(df):     return df.select(\"user_id\", \"amount\")</p> Source code in <code>src/mlforge/core.py</code> <pre><code>@dataclass\nclass Feature:\n    \"\"\"\n    Container for a feature definition and its transformation function.\n\n    Features are created using the @feature decorator and contain metadata\n    about the feature's source, keys, and timestamp requirements.\n\n    Attributes:\n        name: Feature name derived from the decorated function\n        source: Path to the data source file (parquet/csv)\n        keys: Column names that uniquely identify entities\n        tags: Feature tags to group features together\n        timestamp: Column name for temporal features, enables point-in-time joins\n        description: Human-readable feature description\n        interval: Time interval for rolling aggregations (e.g., \"1h\", \"1d\")\n        metrics: Aggregation metrics to compute over rolling windows\n        engine: Compute engine to use for this feature (\"polars\" or \"duckdb\")\n        fn: The transformation function that computes the feature\n\n    Example:\n        @feature(keys=[\"user_id\"], source=\"data/users.parquet\")\n        def user_age(df):\n            return df.with_columns(...)\n\n        @feature(keys=[\"user_id\"], source=\"data/transactions.parquet\", engine=\"duckdb\")\n        def user_spend(df):\n            return df.select(\"user_id\", \"amount\")\n    \"\"\"\n\n    fn: FeatureFunction\n    name: str\n    source: str\n    keys: list[str]\n    tags: list[str] | None\n    timestamp: str | None\n    description: str | None\n    interval: str | None\n    metrics: list[metrics_.MetricKind] | None\n    validators: dict[str, list[validators_.Validator]] | None\n    engine: str | None\n\n    def __call__(self, *args, **kwargs) -&gt; pl.DataFrame:\n        \"\"\"\n        Execute the feature transformation function.\n\n        All arguments are passed through to the underlying feature function.\n\n        Returns:\n            DataFrame with computed feature columns\n        \"\"\"\n        return self.fn(*args, **kwargs)\n</code></pre>"},{"location":"api/core/#mlforge.core.Feature.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs) -&gt; pl.DataFrame\n</code></pre> <p>Execute the feature transformation function.</p> <p>All arguments are passed through to the underlying feature function.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with computed feature columns</p> Source code in <code>src/mlforge/core.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; pl.DataFrame:\n    \"\"\"\n    Execute the feature transformation function.\n\n    All arguments are passed through to the underlying feature function.\n\n    Returns:\n        DataFrame with computed feature columns\n    \"\"\"\n    return self.fn(*args, **kwargs)\n</code></pre>"},{"location":"api/core/#mlforge.core.Definitions","title":"mlforge.core.Definitions","text":"<p>Central registry for feature store projects.</p> <p>Manages feature registration, discovery from modules, and materialization to offline storage. Acts as the main entry point for defining and building features.</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>Project identifier</p> <code>offline_store</code> <p>Storage backend instance for persisting features</p> <code>online_store</code> <p>Optional online store for real-time feature serving</p> <code>features</code> <code>dict[str, Feature]</code> <p>Dictionary mapping feature names to Feature objects</p> <code>default_engine</code> <p>Default compute engine for features without explicit engine. Defaults to duckdb.</p> Example <p>from mlforge import Definitions, LocalStore from mlforge.online import RedisStore import my_features</p> <p>defs = Definitions(     name=\"my-project\",     features=[my_features],     offline_store=LocalStore(\"./feature_store\"),     online_store=RedisStore(host=\"localhost\"), )</p> Source code in <code>src/mlforge/core.py</code> <pre><code>class Definitions:\n    \"\"\"\n    Central registry for feature store projects.\n\n    Manages feature registration, discovery from modules, and materialization\n    to offline storage. Acts as the main entry point for defining and building\n    features.\n\n    Attributes:\n        name: Project identifier\n        offline_store: Storage backend instance for persisting features\n        online_store: Optional online store for real-time feature serving\n        features: Dictionary mapping feature names to Feature objects\n        default_engine: Default compute engine for features without explicit engine. Defaults to duckdb.\n\n    Example:\n        from mlforge import Definitions, LocalStore\n        from mlforge.online import RedisStore\n        import my_features\n\n        defs = Definitions(\n            name=\"my-project\",\n            features=[my_features],\n            offline_store=LocalStore(\"./feature_store\"),\n            online_store=RedisStore(host=\"localhost\"),\n        )\n\n        # Build to offline store (default)\n        defs.build(feature_names=[\"user_spend\"])\n\n        # Build to online store\n        defs.build(feature_names=[\"user_spend\"], online=True)\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        features: list[Feature | ModuleType],\n        offline_store: store.OfflineStoreKind,\n        online_store: online.OnlineStoreKind | None = None,\n        default_engine: Literal[\"polars\", \"duckdb\"] = \"duckdb\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize a feature store registry.\n\n        Args:\n            name: Project name\n            features: List of Feature objects or modules containing features\n            offline_store: Storage backend for materialized features\n            online_store: Optional online store for real-time serving. Defaults to None.\n            default_engine: Default execution engine for features without explicit engine.\n                Defaults to \"duckdb\" which is optimized for large datasets with rolling\n                windows. Use \"polars\" for small datasets or when you prefer staying in\n                the Polars ecosystem. Individual features can override this via the\n                engine parameter in the @feature decorator.\n\n        Example:\n            defs = Definitions(\n                name=\"fraud-detection\",\n                features=[user_features, transaction_features],\n                offline_store=LocalStore(\"./features\"),\n                online_store=RedisStore(host=\"localhost\"),\n            )\n\n            # Use Polars for small datasets\n            defs = Definitions(\n                name=\"fraud-detection\",\n                features=[user_features],\n                offline_store=LocalStore(\"./features\"),\n                default_engine=\"polars\"\n            )\n        \"\"\"\n        self.name = name\n        self.offline_store = offline_store\n        self.online_store = online_store\n        self.features: dict[str, Feature] = {}\n        self.default_engine = default_engine\n        self._engines: dict[str, engines.EngineKind] = {}\n\n        for item in features or []:\n            self._register(item)\n\n    def _get_engine(self, engine: str) -&gt; engines.EngineKind:\n        \"\"\"\n        Get or create an engine instance by name.\n\n        Engines are cached to avoid recreating them for each feature.\n\n        Args:\n            engine: Engine identifier string (\"polars\" or \"duckdb\")\n\n        Returns:\n            Initialized engine instance\n\n        Raises:\n            ValueError: If engine name is not recognized\n        \"\"\"\n        if engine in self._engines:\n            return self._engines[engine]\n\n        match engine:\n            case \"polars\":\n                from mlforge.engines import PolarsEngine\n\n                self._engines[engine] = PolarsEngine()\n            case \"duckdb\":\n                from mlforge.engines import DuckDBEngine\n\n                self._engines[engine] = DuckDBEngine()\n            case _:\n                raise ValueError(f\"Unknown engine: {engine}\")\n\n        return self._engines[engine]\n\n    def _get_engine_for_feature(self, feature: Feature) -&gt; engines.EngineKind:\n        \"\"\"\n        Get the appropriate engine for a feature.\n\n        Uses the feature's engine if specified, otherwise falls back to\n        the Definitions default engine.\n\n        Args:\n            feature: Feature to get engine for\n\n        Returns:\n            Engine instance for the feature\n        \"\"\"\n        engine_name = feature.engine or self.default_engine\n        return self._get_engine(engine_name)\n\n    def build(\n        self,\n        feature_names: list[str] | None = None,\n        tag_names: list[str] | None = None,\n        feature_version: str | None = None,\n        force: bool = False,\n        preview: bool = True,\n        preview_rows: int = 5,\n        online: bool = False,\n    ) -&gt; dict[str, Path | str | int]:\n        \"\"\"\n        Compute and persist features to offline storage with versioning.\n\n        Loads source data, applies feature transformations, validates results,\n        and writes to the configured storage backend. Automatically determines\n        the appropriate version based on schema and configuration changes.\n\n        Args:\n            feature_names: Specific features to materialize. Defaults to None (all).\n            tag_names: Specific features to materialize by tag. Defaults to None (all).\n            feature_version: Explicit version override (e.g., \"2.0.0\"). If None, auto-detects.\n            force: Overwrite existing features. Defaults to False.\n            preview: Display preview of materialized data. Defaults to True.\n            preview_rows: Number of preview rows to show. Defaults to 5.\n            online: Write to online store instead of offline. Defaults to False.\n                Requires online_store to be configured. Extracts latest values\n                per entity and writes to the online store.\n\n        Returns:\n            Dictionary mapping feature names to their storage file paths (offline)\n            or record counts (online)\n\n        Raises:\n            ValueError: If specified feature name is not registered, or if\n                online=True but no online_store is configured\n            FeatureMaterializationError: If feature function fails or returns invalid data\n\n        Example:\n            # Auto-versioning (default)\n            paths = defs.build(feature_names=[\"user_age\", \"user_spend\"])\n\n            # Explicit version\n            paths = defs.build(feature_names=[\"user_spend\"], feature_version=\"2.0.0\")\n\n            # Build to online store\n            counts = defs.build(feature_names=[\"user_spend\"], online=True)\n        \"\"\"\n        if online:\n            online_results = self._build_online(\n                feature_names, tag_names, preview, preview_rows\n            )\n            # Cast int to Path | str | int union for consistent return type\n            return {k: v for k, v in online_results.items()}\n\n        selected_features = self._resolve_features_to_build(\n            feature_names, tag_names\n        )\n        results: dict[str, Path | str | int] = {}\n        failed_features: list[str] = []\n\n        for feature in selected_features:\n            # Get previous metadata for change detection\n            previous_meta = self.offline_store.read_metadata(feature.name)\n\n            # Determine target version\n            if feature_version is not None:\n                # Explicit version override\n                target_version = feature_version\n                change_summary = version.ChangeSummary(\n                    change_type=version.ChangeType.PATCH,\n                    reason=\"explicit_version\",\n                    details=[],\n                )\n            else:\n                # Auto-detect version\n                target_version, change_summary = self._determine_version(\n                    feature, previous_meta\n                )\n\n            # Check if this version already exists (unless force)\n            if not force and self.offline_store.exists(\n                feature.name, target_version\n            ):\n                logger.debug(\n                    f\"Skipping {feature.name} v{target_version} (already exists)\"\n                )\n                continue\n\n            try:\n                engine = self._get_engine_for_feature(feature)\n                result = engine.execute(feature)\n            except errors.FeatureValidationError as e:\n                logger.error(str(e))\n                failed_features.append(feature.name)\n                continue\n\n            result_df = result.to_polars()\n            self._validate_result(feature.name, result_df)\n\n            # Write with version\n            write_metadata = self.offline_store.write(\n                feature.name, result, feature_version=target_version\n            )\n\n            # Compute hashes for metadata\n            # Use base_schema (before metrics) for consistent hash computation\n            # Use canonical types for consistent hashing across engines\n            base_schema_canonical = (\n                result.base_schema_canonical() or result.schema_canonical()\n            )\n            schema_columns = [\n                manifest.ColumnMetadata(name=k, dtype=v.to_canonical_string())\n                for k, v in base_schema_canonical.items()\n            ]\n            schema_hash = version.compute_schema_hash(schema_columns)\n            config_hash = version.compute_config_hash(\n                keys=feature.keys,\n                timestamp=feature.timestamp,\n                interval=feature.interval,\n                metrics_config=self._serialize_metrics_config(feature.metrics),\n            )\n            content_hash = version.compute_content_hash(\n                Path(write_metadata[\"path\"])\n            )\n            source_hash = version.compute_source_hash(feature.source)\n\n            # Build and write feature metadata\n            now = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n            feature_metadata = self._build_feature_metadata(\n                feature=feature,\n                write_metadata=write_metadata,\n                schema=result.schema(),\n                base_schema=result.base_schema(),\n                schema_source=result._schema_source(),\n                target_version=target_version,\n                created_at=previous_meta.created_at if previous_meta else now,\n                updated_at=now,\n                content_hash=content_hash,\n                schema_hash=schema_hash,\n                config_hash=config_hash,\n                source_hash=source_hash,\n                change_summary=change_summary,\n            )\n            self.offline_store.write_metadata(feature.name, feature_metadata)\n\n            result_path = self.offline_store.path_for(\n                feature.name, target_version\n            )\n\n            if preview:\n                log.print_feature_preview(\n                    f\"{feature.name} v{target_version}\",\n                    result_df,\n                    max_rows=preview_rows,\n                )\n\n            results[feature.name] = result_path\n\n        if failed_features:\n            logger.warning(\n                f\"Build completed with validation failures: {failed_features}\"\n            )\n\n        return results\n\n    def _build_online(\n        self,\n        feature_names: list[str] | None,\n        tag_names: list[str] | None,\n        preview: bool,\n        preview_rows: int,\n    ) -&gt; dict[str, int]:\n        \"\"\"\n        Build features to the online store.\n\n        Extracts the latest value per entity from each feature and writes\n        to the configured online store. Requires timestamp column to determine\n        \"latest\" values.\n\n        Args:\n            feature_names: Specific features to materialize\n            tag_names: Specific features to materialize by tag\n            preview: Display preview of data being written\n            preview_rows: Number of preview rows to show\n\n        Returns:\n            Dictionary mapping feature names to record counts written\n\n        Raises:\n            ValueError: If no online_store is configured\n        \"\"\"\n        if self.online_store is None:\n            raise ValueError(\n                \"Cannot build to online store: no online_store configured. \"\n                \"Pass online_store=RedisStore(...) to Definitions().\"\n            )\n\n        selected_features = self._resolve_features_to_build(\n            feature_names, tag_names\n        )\n        results: dict[str, int] = {}\n\n        for feature in selected_features:\n            # Execute feature computation\n            engine = self._get_engine_for_feature(feature)\n            result = engine.execute(feature)\n            result_df = result.to_polars()\n\n            # Extract latest values per entity\n            latest_df = self._extract_latest_per_entity(feature, result_df)\n\n            if preview:\n                log.print_feature_preview(\n                    f\"{feature.name} (online)\",\n                    latest_df,\n                    max_rows=preview_rows,\n                )\n\n            # Write to online store\n            records = latest_df.to_dicts()\n            count = self.online_store.write_batch(\n                feature_name=feature.name,\n                records=records,\n                entity_key_columns=feature.keys,\n            )\n\n            logger.info(\n                f\"Wrote {count} records to online store: {feature.name}\"\n            )\n            results[feature.name] = count\n\n        return results\n\n    def _extract_latest_per_entity(\n        self,\n        feature: Feature,\n        df: pl.DataFrame,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Extract the latest row per entity from a feature DataFrame.\n\n        If the feature has a timestamp column, sorts by timestamp descending\n        and takes the first row per entity key combination. The timestamp\n        column is dropped from the output since online stores only need\n        entity keys and feature values.\n\n        Args:\n            feature: Feature definition with keys and timestamp info\n            df: Feature DataFrame with computed values\n\n        Returns:\n            DataFrame with one row per unique entity (without timestamp)\n        \"\"\"\n        if feature.timestamp:\n            # Sort by timestamp descending, take first per entity\n            result = (\n                df.sort(feature.timestamp, descending=True)\n                .group_by(feature.keys)\n                .first()\n                .drop(feature.timestamp)\n            )\n        else:\n            # No timestamp - take last row per entity\n            result = df.group_by(feature.keys).last()\n\n        return result\n\n    def _determine_version(\n        self,\n        feature: Feature,\n        previous_meta: manifest.FeatureMetadata | None,\n    ) -&gt; tuple[str, version.ChangeSummary]:\n        \"\"\"\n        Determine next version based on change detection.\n\n        Args:\n            feature: Feature being built\n            previous_meta: Metadata from previous version (None if first build)\n\n        Returns:\n            Tuple of (version_string, ChangeSummary)\n        \"\"\"\n        if previous_meta is None:\n            # First build\n            return \"1.0.0\", version.build_change_summary(\n                version.ChangeType.INITIAL, None, []\n            )\n\n        # Load and process to get current schema (before metrics)\n        engine = self._get_engine_for_feature(feature)\n        source_df = engine._load_source(feature.source)\n\n        if isinstance(source_df, duckdb.DuckDBPyRelation):\n            source_df_ = source_df.to_arrow_table()\n            source_df = pl.from_arrow(source_df_)\n\n        preview_df = feature(source_df)\n\n        if isinstance(preview_df, pl.LazyFrame):\n            preview_df = preview_df.collect()\n\n        current_columns = list(preview_df.columns)\n        previous_columns = [c.name for c in previous_meta.columns]\n\n        # Compute current hashes using canonical types for cross-engine consistency\n        current_schema_columns = [\n            manifest.ColumnMetadata(\n                name=c,\n                dtype=types_.from_polars(\n                    preview_df.schema[c]\n                ).to_canonical_string(),\n            )\n            for c in current_columns\n        ]\n        current_schema_hash = version.compute_schema_hash(\n            current_schema_columns\n        )\n        current_config_hash = version.compute_config_hash(\n            keys=feature.keys,\n            timestamp=feature.timestamp,\n            interval=feature.interval,\n            metrics_config=self._serialize_metrics_config(feature.metrics),\n        )\n\n        # Detect change type\n        change_type = version.detect_change_type(\n            previous_columns=previous_columns,\n            current_columns=current_columns,\n            previous_schema_hash=previous_meta.schema_hash,\n            current_schema_hash=current_schema_hash,\n            previous_config_hash=previous_meta.config_hash,\n            current_config_hash=current_config_hash,\n        )\n\n        # Determine target version\n        if change_type == version.ChangeType.INITIAL:\n            target_version = \"1.0.0\"\n        else:\n            target_version = version.bump_version(\n                previous_meta.version, change_type\n            )\n\n        change_summary = version.build_change_summary(\n            change_type, previous_columns, current_columns\n        )\n\n        return target_version, change_summary\n\n    def _serialize_metrics_config(\n        self, metrics: list[metrics_.MetricKind] | None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Serialize metrics configuration for hashing.\n\n        Args:\n            metrics: List of metric configurations\n\n        Returns:\n            List of serialized metric dictionaries\n        \"\"\"\n        if not metrics:\n            return []\n\n        configs = []\n        for metric in metrics:\n            if hasattr(metric, \"to_dict\"):\n                configs.append(metric.to_dict())\n            else:\n                # Fallback: use repr\n                configs.append({\"repr\": repr(metric)})\n        return configs\n\n    def validate(\n        self,\n        feature_names: list[str] | None = None,\n        tag_names: list[str] | None = None,\n    ) -&gt; list[validation_.FeatureValidationResult]:\n        \"\"\"\n        Run validation checks on features without building.\n\n        Loads source data, applies feature transformations, and runs validators\n        on the output. Does not compute metrics or write to storage.\n\n        Args:\n            feature_names: Specific features to validate. Defaults to None (all).\n            tag_names: Specific features to validate by tag. Defaults to None (all).\n\n        Returns:\n            List of FeatureValidationResult objects, one per validated feature.\n            Features without validators are skipped.\n\n        Raises:\n            ValueError: If specified feature name is not registered\n\n        Example:\n            results = defs.validate(feature_names=[\"user_spend\"])\n            for result in results:\n                if not result.passed:\n                    print(f\"{result.feature_name} failed validation\")\n        \"\"\"\n        selected_features = self._resolve_features_to_build(\n            feature_names, tag_names\n        )\n        results: list[validation_.FeatureValidationResult] = []\n\n        for feature in selected_features:\n            if not feature.validators:\n                logger.debug(f\"Skipping {feature.name} (no validators)\")\n                continue\n\n            try:\n                # Load and process data (without metrics)\n                engine = self._get_engine_for_feature(feature)\n                source_df = engine._load_source(feature.source)\n                processed_df = feature(source_df)\n\n                # Collect if LazyFrame\n                if isinstance(processed_df, pl.LazyFrame):\n                    processed_df = processed_df.collect()\n\n                # Run validators\n                result = validation_.validate_feature(\n                    feature.name, processed_df, feature.validators\n                )\n                results.append(result)\n\n            except Exception as e:\n                logger.error(f\"Error validating {feature.name}: {e}\")\n                # Create a failed result for the feature\n                results.append(\n                    validation_.FeatureValidationResult(\n                        feature_name=feature.name,\n                        column_results=[\n                            validation_.ColumnValidationResult(\n                                column=\"&lt;error&gt;\",\n                                validator_name=\"execution\",\n                                result=validators_.ValidationResult(\n                                    passed=False,\n                                    message=str(e),\n                                ),\n                            )\n                        ],\n                    )\n                )\n\n        return results\n\n    def sync(\n        self,\n        feature_names: list[str] | None = None,\n        dry_run: bool = False,\n        force: bool = False,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Rebuild features that have metadata but no data.\n\n        Scans the store for features with metadata (.meta.json) but no data\n        (data.parquet missing). For each such feature, verifies source data\n        hasn't changed, then rebuilds the feature.\n\n        This is used for the Git workflow where teammates share metadata\n        but not data files. After pulling metadata, running sync will\n        rebuild the features locally from source data.\n\n        Args:\n            feature_names: Specific features to sync. Defaults to None (all).\n            dry_run: If True, only report what would be synced. Defaults to False.\n            force: If True, rebuild even if source data hash differs. Defaults to False.\n\n        Returns:\n            Dictionary with:\n                - needs_sync: List of feature names that need syncing\n                - source_changed: List of feature names with changed source data\n                - synced: List of feature names that were synced (empty if dry_run)\n\n        Raises:\n            SourceDataChangedError: If source data has changed and force=False\n\n        Example:\n            # Check what needs syncing\n            result = defs.sync(dry_run=True)\n            print(result[\"needs_sync\"])\n\n            # Rebuild missing data\n            result = defs.sync()\n            print(f\"Synced {len(result['synced'])} features\")\n        \"\"\"\n        needs_sync: list[str] = []\n        source_changed: list[str] = []\n        synced: list[str] = []\n\n        # Get list of features to check\n        features_to_check = (\n            [self._get_feature(name) for name in feature_names]\n            if feature_names\n            else self.list_features()\n        )\n\n        for feature in features_to_check:\n            # Read metadata for latest version\n            metadata = self.offline_store.read_metadata(feature.name)\n            if metadata is None:\n                # No metadata - nothing to sync\n                continue\n\n            latest_version = self.offline_store.get_latest_version(feature.name)\n            if latest_version is None:\n                continue\n\n            # Check if data file exists\n            if self.offline_store.exists(feature.name, latest_version):\n                # Data exists - nothing to sync\n                continue\n\n            # Metadata exists but data doesn't - needs sync\n            needs_sync.append(feature.name)\n\n            # Check source hash\n            if metadata.source_hash:\n                try:\n                    current_source_hash = version.compute_source_hash(\n                        feature.source\n                    )\n                    if current_source_hash != metadata.source_hash:\n                        source_changed.append(feature.name)\n                        if not force and not dry_run:\n                            raise errors.SourceDataChangedError(\n                                feature_name=feature.name,\n                                expected_hash=metadata.source_hash,\n                                current_hash=current_source_hash,\n                                source_path=feature.source,\n                            )\n                except FileNotFoundError:\n                    # Source file not found - can't sync\n                    logger.warning(\n                        f\"Source file not found for {feature.name}: {feature.source}\"\n                    )\n                    continue\n\n        if dry_run:\n            return {\n                \"needs_sync\": needs_sync,\n                \"source_changed\": source_changed,\n                \"synced\": [],\n            }\n\n        # Rebuild features that need syncing\n        for feature_name in needs_sync:\n            if feature_name in source_changed and not force:\n                # Skip - source changed and not forcing\n                continue\n\n            # Get the version from metadata\n            metadata = self.offline_store.read_metadata(feature_name)\n            if metadata is None:\n                continue\n\n            # Rebuild with the same version\n            try:\n                self.build(\n                    feature_names=[feature_name],\n                    feature_version=metadata.version,\n                    force=True,\n                    preview=False,\n                )\n                synced.append(feature_name)\n                logger.info(f\"Synced {feature_name} v{metadata.version}\")\n            except Exception as e:\n                logger.error(f\"Failed to sync {feature_name}: {e}\")\n\n        return {\n            \"needs_sync\": needs_sync,\n            \"source_changed\": source_changed,\n            \"synced\": synced,\n        }\n\n    def _validate_result(self, feature_name: str, result_df: Any) -&gt; None:\n        \"\"\"\n        Validate that a feature function returned a valid DataFrame.\n\n        Args:\n            feature_name: Name of the feature being validated\n            result_df: Result from feature function\n\n        Raises:\n            FeatureMaterializationError: If result is None or not a DataFrame\n        \"\"\"\n        if result_df is None:\n            raise errors.FeatureMaterializationError(\n                feature_name=feature_name,\n                message=\"Feature function returned None\",\n                hint=\"Make sure your feature function returns a DataFrame.\",\n            )\n\n        if not isinstance(result_df, pl.DataFrame):\n            raise errors.FeatureMaterializationError(\n                feature_name=feature_name,\n                message=f\"Expected DataFrame, got {type(result_df).__name__}\",\n            )\n\n    def _resolve_features_to_build(\n        self,\n        feature_names: list[str] | None,\n        tag_names: list[str] | None,\n    ) -&gt; list[Feature]:\n        \"\"\"\n        Resolve which features to build based on parameters.\n\n        Args:\n            feature_names: Specific feature names to build, or None for all\n            tag_names: Feature tags to filter by, or None\n\n        Returns:\n            List of Feature objects to materialize\n\n        Raises:\n            ValueError: If both feature_names and tag_names are specified,\n                       or if any feature/tag name is invalid\n        \"\"\"\n        if feature_names and tag_names:\n            raise ValueError(\n                \"Cannot specify both --features and --tags. Choose one or the other.\"\n            )\n\n        if feature_names:\n            return [self._get_feature(name) for name in feature_names]\n\n        if tag_names:\n            self._validate_tags(tag_names)\n            return self.list_features(tags=tag_names)\n\n        return self.list_features()\n\n    def _validate_tags(self, tag_names: list[str]) -&gt; None:\n        \"\"\"\n        Validate that all tag names exist in registered features.\n\n        Args:\n            tag_names: List of tag names to validate\n\n        Raises:\n            ValueError: If any tag is not found in registered features\n        \"\"\"\n        available_tags = set(self.list_tags())\n        invalid_tags = [t for t in tag_names if t not in available_tags]\n        if invalid_tags:\n            logger.debug(\n                f\"Invalid tags: {invalid_tags}. Available: {available_tags}\"\n            )\n            raise ValueError(\n                f\"Unknown tags: {invalid_tags}. Available: {sorted(available_tags)}\"\n            )\n\n    def list_features(self, tags: list[str] | None = None) -&gt; list[Feature]:\n        \"\"\"\n        Return all registered features.\n\n        Args:\n            tags: Pass a list of tags to return the features for. Defaults to None.\n\n        Returns:\n            List of all Feature objects in the registry\n        \"\"\"\n        features = list(self.features.values())\n\n        if not tags:\n            return features\n\n        return [\n            feat\n            for feat in features\n            if feat.tags and any(tag in tags for tag in feat.tags)\n        ]\n\n    def list_tags(self) -&gt; list[str]:\n        \"\"\"\n        Return all tags from registered features.\n\n        Returns:\n            Flat list of tag strings. May contain duplicates if the same\n            tag is used by multiple features.\n\n        Example:\n            tags = defs.list_tags()  # [\"users\", \"transactions\", \"users\"]\n            unique_tags = set(defs.list_tags())  # {\"users\", \"transactions\"}\n        \"\"\"\n        features = self.list_features()\n        return [tag for feat in features if feat.tags for tag in feat.tags]\n\n    def _get_feature(self, name: str) -&gt; Feature:\n        \"\"\"\n        Get a feature by name.\n\n        Args:\n            name: Feature name to retrieve\n\n        Returns:\n            Feature object\n\n        Raises:\n            ValueError: If feature name is not registered\n        \"\"\"\n        if name not in self.features:\n            raise ValueError(f\"Unknown feature: {name}\")\n        return self.features[name]\n\n    def _register(self, obj: Feature | ModuleType) -&gt; None:\n        \"\"\"\n        Register a Feature or discover features from a module.\n\n        Args:\n            obj: Feature instance or module containing Feature objects\n\n        Raises:\n            TypeError: If obj is neither a Feature nor a module\n        \"\"\"\n        if isinstance(obj, Feature):\n            self._add_feature(obj)\n        elif isinstance(obj, ModuleType):\n            self._register_module(obj)\n        else:\n            raise TypeError(\n                f\"Expected Feature or module, got {type(obj).__name__}\"\n            )\n\n    def _add_feature(self, feature: Feature) -&gt; None:\n        \"\"\"\n        Add a single feature to the registry.\n\n        Args:\n            feature: Feature instance to register\n\n        Raises:\n            ValueError: If a feature with the same name already exists\n        \"\"\"\n        if feature.name in self.features:\n            raise ValueError(f\"Duplicate feature name: {feature.name}\")\n\n        logger.debug(f\"Registered feature: {feature.name}\")\n        self.features[feature.name] = feature\n\n    def _register_module(self, module: ModuleType) -&gt; None:\n        \"\"\"\n        Discover and register all Features in a module.\n\n        Args:\n            module: Python module to scan for Feature objects\n        \"\"\"\n        features_found = 0\n\n        for obj in vars(module).values():\n            if isinstance(obj, Feature):\n                self._add_feature(obj)\n                features_found += 1\n\n        if features_found == 0:\n            logger.warning(f\"No features found in module: {module.__name__}\")\n\n    def _build_feature_metadata(\n        self,\n        feature: Feature,\n        write_metadata: dict[str, Any],\n        schema: dict[str, str],\n        base_schema: dict[str, str] | None = None,\n        schema_source: str = \"polars\",\n        target_version: str = \"1.0.0\",\n        created_at: str = \"\",\n        updated_at: str = \"\",\n        content_hash: str = \"\",\n        schema_hash: str = \"\",\n        config_hash: str = \"\",\n        source_hash: str = \"\",\n        change_summary: version.ChangeSummary | None = None,\n    ) -&gt; manifest.FeatureMetadata:\n        \"\"\"\n        Build FeatureMetadata from feature definition and write results.\n\n        Args:\n            feature: The Feature definition object\n            write_metadata: Metadata returned from store.write()\n            schema: Column name to dtype mapping from result (final schema after metrics)\n            base_schema: Column name to dtype mapping before metrics were applied\n            schema_source: Engine source for type normalization (\"polars\" or \"duckdb\")\n            target_version: Semantic version string\n            created_at: ISO 8601 timestamp when version was created\n            updated_at: ISO 8601 timestamp of this build\n            content_hash: Hash of parquet file content\n            schema_hash: Hash of column schema\n            config_hash: Hash of feature configuration\n            source_hash: Hash of source data file for reproducibility\n            change_summary: Summary of changes from previous version\n\n        Returns:\n            FeatureMetadata object ready for persistence\n        \"\"\"\n        base_columns, feature_columns = manifest.derive_column_metadata(\n            feature, schema, base_schema, schema_source\n        )\n\n        now = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n\n        return manifest.FeatureMetadata(\n            name=feature.name,\n            version=target_version,\n            path=write_metadata[\"path\"],\n            entity=feature.keys[0],\n            keys=feature.keys,\n            source=feature.source,\n            row_count=write_metadata[\"row_count\"],\n            created_at=created_at or now,\n            updated_at=updated_at or now,\n            content_hash=content_hash,\n            schema_hash=schema_hash,\n            config_hash=config_hash,\n            source_hash=source_hash,\n            timestamp=feature.timestamp,\n            interval=feature.interval,\n            columns=base_columns,\n            features=feature_columns,\n            tags=feature.tags or [],\n            description=feature.description,\n            change_summary=change_summary.to_dict() if change_summary else None,\n        )\n</code></pre>"},{"location":"api/core/#mlforge.core.Definitions--build-to-offline-store-default","title":"Build to offline store (default)","text":"<p>defs.build(feature_names=[\"user_spend\"])</p>"},{"location":"api/core/#mlforge.core.Definitions--build-to-online-store","title":"Build to online store","text":"<p>defs.build(feature_names=[\"user_spend\"], online=True)</p>"},{"location":"api/core/#mlforge.core.Definitions.__init__","title":"__init__","text":"<pre><code>__init__(\n    name: str,\n    features: list[Feature | ModuleType],\n    offline_store: OfflineStoreKind,\n    online_store: OnlineStoreKind | None = None,\n    default_engine: Literal[\"polars\", \"duckdb\"] = \"duckdb\",\n) -&gt; None\n</code></pre> <p>Initialize a feature store registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Project name</p> required <code>features</code> <code>list[Feature | ModuleType]</code> <p>List of Feature objects or modules containing features</p> required <code>offline_store</code> <code>OfflineStoreKind</code> <p>Storage backend for materialized features</p> required <code>online_store</code> <code>OnlineStoreKind | None</code> <p>Optional online store for real-time serving. Defaults to None.</p> <code>None</code> <code>default_engine</code> <code>Literal['polars', 'duckdb']</code> <p>Default execution engine for features without explicit engine. Defaults to \"duckdb\" which is optimized for large datasets with rolling windows. Use \"polars\" for small datasets or when you prefer staying in the Polars ecosystem. Individual features can override this via the engine parameter in the @feature decorator.</p> <code>'duckdb'</code> Example <p>defs = Definitions(     name=\"fraud-detection\",     features=[user_features, transaction_features],     offline_store=LocalStore(\"./features\"),     online_store=RedisStore(host=\"localhost\"), )</p> Source code in <code>src/mlforge/core.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    features: list[Feature | ModuleType],\n    offline_store: store.OfflineStoreKind,\n    online_store: online.OnlineStoreKind | None = None,\n    default_engine: Literal[\"polars\", \"duckdb\"] = \"duckdb\",\n) -&gt; None:\n    \"\"\"\n    Initialize a feature store registry.\n\n    Args:\n        name: Project name\n        features: List of Feature objects or modules containing features\n        offline_store: Storage backend for materialized features\n        online_store: Optional online store for real-time serving. Defaults to None.\n        default_engine: Default execution engine for features without explicit engine.\n            Defaults to \"duckdb\" which is optimized for large datasets with rolling\n            windows. Use \"polars\" for small datasets or when you prefer staying in\n            the Polars ecosystem. Individual features can override this via the\n            engine parameter in the @feature decorator.\n\n    Example:\n        defs = Definitions(\n            name=\"fraud-detection\",\n            features=[user_features, transaction_features],\n            offline_store=LocalStore(\"./features\"),\n            online_store=RedisStore(host=\"localhost\"),\n        )\n\n        # Use Polars for small datasets\n        defs = Definitions(\n            name=\"fraud-detection\",\n            features=[user_features],\n            offline_store=LocalStore(\"./features\"),\n            default_engine=\"polars\"\n        )\n    \"\"\"\n    self.name = name\n    self.offline_store = offline_store\n    self.online_store = online_store\n    self.features: dict[str, Feature] = {}\n    self.default_engine = default_engine\n    self._engines: dict[str, engines.EngineKind] = {}\n\n    for item in features or []:\n        self._register(item)\n</code></pre>"},{"location":"api/core/#mlforge.core.Definitions.__init__--use-polars-for-small-datasets","title":"Use Polars for small datasets","text":"<p>defs = Definitions(     name=\"fraud-detection\",     features=[user_features],     offline_store=LocalStore(\"./features\"),     default_engine=\"polars\" )</p>"},{"location":"api/core/#mlforge.core.Definitions.build","title":"build","text":"<pre><code>build(\n    feature_names: list[str] | None = None,\n    tag_names: list[str] | None = None,\n    feature_version: str | None = None,\n    force: bool = False,\n    preview: bool = True,\n    preview_rows: int = 5,\n    online: bool = False,\n) -&gt; dict[str, Path | str | int]\n</code></pre> <p>Compute and persist features to offline storage with versioning.</p> <p>Loads source data, applies feature transformations, validates results, and writes to the configured storage backend. Automatically determines the appropriate version based on schema and configuration changes.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list[str] | None</code> <p>Specific features to materialize. Defaults to None (all).</p> <code>None</code> <code>tag_names</code> <code>list[str] | None</code> <p>Specific features to materialize by tag. Defaults to None (all).</p> <code>None</code> <code>feature_version</code> <code>str | None</code> <p>Explicit version override (e.g., \"2.0.0\"). If None, auto-detects.</p> <code>None</code> <code>force</code> <code>bool</code> <p>Overwrite existing features. Defaults to False.</p> <code>False</code> <code>preview</code> <code>bool</code> <p>Display preview of materialized data. Defaults to True.</p> <code>True</code> <code>preview_rows</code> <code>int</code> <p>Number of preview rows to show. Defaults to 5.</p> <code>5</code> <code>online</code> <code>bool</code> <p>Write to online store instead of offline. Defaults to False. Requires online_store to be configured. Extracts latest values per entity and writes to the online store.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Path | str | int]</code> <p>Dictionary mapping feature names to their storage file paths (offline)</p> <code>dict[str, Path | str | int]</code> <p>or record counts (online)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If specified feature name is not registered, or if online=True but no online_store is configured</p> <code>FeatureMaterializationError</code> <p>If feature function fails or returns invalid data</p> Example Source code in <code>src/mlforge/core.py</code> <pre><code>def build(\n    self,\n    feature_names: list[str] | None = None,\n    tag_names: list[str] | None = None,\n    feature_version: str | None = None,\n    force: bool = False,\n    preview: bool = True,\n    preview_rows: int = 5,\n    online: bool = False,\n) -&gt; dict[str, Path | str | int]:\n    \"\"\"\n    Compute and persist features to offline storage with versioning.\n\n    Loads source data, applies feature transformations, validates results,\n    and writes to the configured storage backend. Automatically determines\n    the appropriate version based on schema and configuration changes.\n\n    Args:\n        feature_names: Specific features to materialize. Defaults to None (all).\n        tag_names: Specific features to materialize by tag. Defaults to None (all).\n        feature_version: Explicit version override (e.g., \"2.0.0\"). If None, auto-detects.\n        force: Overwrite existing features. Defaults to False.\n        preview: Display preview of materialized data. Defaults to True.\n        preview_rows: Number of preview rows to show. Defaults to 5.\n        online: Write to online store instead of offline. Defaults to False.\n            Requires online_store to be configured. Extracts latest values\n            per entity and writes to the online store.\n\n    Returns:\n        Dictionary mapping feature names to their storage file paths (offline)\n        or record counts (online)\n\n    Raises:\n        ValueError: If specified feature name is not registered, or if\n            online=True but no online_store is configured\n        FeatureMaterializationError: If feature function fails or returns invalid data\n\n    Example:\n        # Auto-versioning (default)\n        paths = defs.build(feature_names=[\"user_age\", \"user_spend\"])\n\n        # Explicit version\n        paths = defs.build(feature_names=[\"user_spend\"], feature_version=\"2.0.0\")\n\n        # Build to online store\n        counts = defs.build(feature_names=[\"user_spend\"], online=True)\n    \"\"\"\n    if online:\n        online_results = self._build_online(\n            feature_names, tag_names, preview, preview_rows\n        )\n        # Cast int to Path | str | int union for consistent return type\n        return {k: v for k, v in online_results.items()}\n\n    selected_features = self._resolve_features_to_build(\n        feature_names, tag_names\n    )\n    results: dict[str, Path | str | int] = {}\n    failed_features: list[str] = []\n\n    for feature in selected_features:\n        # Get previous metadata for change detection\n        previous_meta = self.offline_store.read_metadata(feature.name)\n\n        # Determine target version\n        if feature_version is not None:\n            # Explicit version override\n            target_version = feature_version\n            change_summary = version.ChangeSummary(\n                change_type=version.ChangeType.PATCH,\n                reason=\"explicit_version\",\n                details=[],\n            )\n        else:\n            # Auto-detect version\n            target_version, change_summary = self._determine_version(\n                feature, previous_meta\n            )\n\n        # Check if this version already exists (unless force)\n        if not force and self.offline_store.exists(\n            feature.name, target_version\n        ):\n            logger.debug(\n                f\"Skipping {feature.name} v{target_version} (already exists)\"\n            )\n            continue\n\n        try:\n            engine = self._get_engine_for_feature(feature)\n            result = engine.execute(feature)\n        except errors.FeatureValidationError as e:\n            logger.error(str(e))\n            failed_features.append(feature.name)\n            continue\n\n        result_df = result.to_polars()\n        self._validate_result(feature.name, result_df)\n\n        # Write with version\n        write_metadata = self.offline_store.write(\n            feature.name, result, feature_version=target_version\n        )\n\n        # Compute hashes for metadata\n        # Use base_schema (before metrics) for consistent hash computation\n        # Use canonical types for consistent hashing across engines\n        base_schema_canonical = (\n            result.base_schema_canonical() or result.schema_canonical()\n        )\n        schema_columns = [\n            manifest.ColumnMetadata(name=k, dtype=v.to_canonical_string())\n            for k, v in base_schema_canonical.items()\n        ]\n        schema_hash = version.compute_schema_hash(schema_columns)\n        config_hash = version.compute_config_hash(\n            keys=feature.keys,\n            timestamp=feature.timestamp,\n            interval=feature.interval,\n            metrics_config=self._serialize_metrics_config(feature.metrics),\n        )\n        content_hash = version.compute_content_hash(\n            Path(write_metadata[\"path\"])\n        )\n        source_hash = version.compute_source_hash(feature.source)\n\n        # Build and write feature metadata\n        now = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n        feature_metadata = self._build_feature_metadata(\n            feature=feature,\n            write_metadata=write_metadata,\n            schema=result.schema(),\n            base_schema=result.base_schema(),\n            schema_source=result._schema_source(),\n            target_version=target_version,\n            created_at=previous_meta.created_at if previous_meta else now,\n            updated_at=now,\n            content_hash=content_hash,\n            schema_hash=schema_hash,\n            config_hash=config_hash,\n            source_hash=source_hash,\n            change_summary=change_summary,\n        )\n        self.offline_store.write_metadata(feature.name, feature_metadata)\n\n        result_path = self.offline_store.path_for(\n            feature.name, target_version\n        )\n\n        if preview:\n            log.print_feature_preview(\n                f\"{feature.name} v{target_version}\",\n                result_df,\n                max_rows=preview_rows,\n            )\n\n        results[feature.name] = result_path\n\n    if failed_features:\n        logger.warning(\n            f\"Build completed with validation failures: {failed_features}\"\n        )\n\n    return results\n</code></pre>"},{"location":"api/core/#mlforge.core.Definitions.build--auto-versioning-default","title":"Auto-versioning (default)","text":"<p>paths = defs.build(feature_names=[\"user_age\", \"user_spend\"])</p>"},{"location":"api/core/#mlforge.core.Definitions.build--explicit-version","title":"Explicit version","text":"<p>paths = defs.build(feature_names=[\"user_spend\"], feature_version=\"2.0.0\")</p>"},{"location":"api/core/#mlforge.core.Definitions.build--build-to-online-store","title":"Build to online store","text":"<p>counts = defs.build(feature_names=[\"user_spend\"], online=True)</p>"},{"location":"api/core/#mlforge.core.Definitions.list_features","title":"list_features","text":"<pre><code>list_features(\n    tags: list[str] | None = None,\n) -&gt; list[Feature]\n</code></pre> <p>Return all registered features.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>list[str] | None</code> <p>Pass a list of tags to return the features for. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Feature]</code> <p>List of all Feature objects in the registry</p> Source code in <code>src/mlforge/core.py</code> <pre><code>def list_features(self, tags: list[str] | None = None) -&gt; list[Feature]:\n    \"\"\"\n    Return all registered features.\n\n    Args:\n        tags: Pass a list of tags to return the features for. Defaults to None.\n\n    Returns:\n        List of all Feature objects in the registry\n    \"\"\"\n    features = list(self.features.values())\n\n    if not tags:\n        return features\n\n    return [\n        feat\n        for feat in features\n        if feat.tags and any(tag in tags for tag in feat.tags)\n    ]\n</code></pre>"},{"location":"api/core/#mlforge.core.Definitions.list_tags","title":"list_tags","text":"<pre><code>list_tags() -&gt; list[str]\n</code></pre> <p>Return all tags from registered features.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Flat list of tag strings. May contain duplicates if the same</p> <code>list[str]</code> <p>tag is used by multiple features.</p> Example <p>tags = defs.list_tags()  # [\"users\", \"transactions\", \"users\"] unique_tags = set(defs.list_tags())  # {\"users\", \"transactions\"}</p> Source code in <code>src/mlforge/core.py</code> <pre><code>def list_tags(self) -&gt; list[str]:\n    \"\"\"\n    Return all tags from registered features.\n\n    Returns:\n        Flat list of tag strings. May contain duplicates if the same\n        tag is used by multiple features.\n\n    Example:\n        tags = defs.list_tags()  # [\"users\", \"transactions\", \"users\"]\n        unique_tags = set(defs.list_tags())  # {\"users\", \"transactions\"}\n    \"\"\"\n    features = self.list_features()\n    return [tag for feat in features if feat.tags for tag in feat.tags]\n</code></pre>"},{"location":"api/core/#mlforge.core.Definitions.sync","title":"sync","text":"<pre><code>sync(\n    feature_names: list[str] | None = None,\n    dry_run: bool = False,\n    force: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Rebuild features that have metadata but no data.</p> <p>Scans the store for features with metadata (.meta.json) but no data (data.parquet missing). For each such feature, verifies source data hasn't changed, then rebuilds the feature.</p> <p>This is used for the Git workflow where teammates share metadata but not data files. After pulling metadata, running sync will rebuild the features locally from source data.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list[str] | None</code> <p>Specific features to sync. Defaults to None (all).</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only report what would be synced. Defaults to False.</p> <code>False</code> <code>force</code> <code>bool</code> <p>If True, rebuild even if source data hash differs. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with: - needs_sync: List of feature names that need syncing - source_changed: List of feature names with changed source data - synced: List of feature names that were synced (empty if dry_run)</p> <p>Raises:</p> Type Description <code>SourceDataChangedError</code> <p>If source data has changed and force=False</p> Example Source code in <code>src/mlforge/core.py</code> <pre><code>def sync(\n    self,\n    feature_names: list[str] | None = None,\n    dry_run: bool = False,\n    force: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Rebuild features that have metadata but no data.\n\n    Scans the store for features with metadata (.meta.json) but no data\n    (data.parquet missing). For each such feature, verifies source data\n    hasn't changed, then rebuilds the feature.\n\n    This is used for the Git workflow where teammates share metadata\n    but not data files. After pulling metadata, running sync will\n    rebuild the features locally from source data.\n\n    Args:\n        feature_names: Specific features to sync. Defaults to None (all).\n        dry_run: If True, only report what would be synced. Defaults to False.\n        force: If True, rebuild even if source data hash differs. Defaults to False.\n\n    Returns:\n        Dictionary with:\n            - needs_sync: List of feature names that need syncing\n            - source_changed: List of feature names with changed source data\n            - synced: List of feature names that were synced (empty if dry_run)\n\n    Raises:\n        SourceDataChangedError: If source data has changed and force=False\n\n    Example:\n        # Check what needs syncing\n        result = defs.sync(dry_run=True)\n        print(result[\"needs_sync\"])\n\n        # Rebuild missing data\n        result = defs.sync()\n        print(f\"Synced {len(result['synced'])} features\")\n    \"\"\"\n    needs_sync: list[str] = []\n    source_changed: list[str] = []\n    synced: list[str] = []\n\n    # Get list of features to check\n    features_to_check = (\n        [self._get_feature(name) for name in feature_names]\n        if feature_names\n        else self.list_features()\n    )\n\n    for feature in features_to_check:\n        # Read metadata for latest version\n        metadata = self.offline_store.read_metadata(feature.name)\n        if metadata is None:\n            # No metadata - nothing to sync\n            continue\n\n        latest_version = self.offline_store.get_latest_version(feature.name)\n        if latest_version is None:\n            continue\n\n        # Check if data file exists\n        if self.offline_store.exists(feature.name, latest_version):\n            # Data exists - nothing to sync\n            continue\n\n        # Metadata exists but data doesn't - needs sync\n        needs_sync.append(feature.name)\n\n        # Check source hash\n        if metadata.source_hash:\n            try:\n                current_source_hash = version.compute_source_hash(\n                    feature.source\n                )\n                if current_source_hash != metadata.source_hash:\n                    source_changed.append(feature.name)\n                    if not force and not dry_run:\n                        raise errors.SourceDataChangedError(\n                            feature_name=feature.name,\n                            expected_hash=metadata.source_hash,\n                            current_hash=current_source_hash,\n                            source_path=feature.source,\n                        )\n            except FileNotFoundError:\n                # Source file not found - can't sync\n                logger.warning(\n                    f\"Source file not found for {feature.name}: {feature.source}\"\n                )\n                continue\n\n    if dry_run:\n        return {\n            \"needs_sync\": needs_sync,\n            \"source_changed\": source_changed,\n            \"synced\": [],\n        }\n\n    # Rebuild features that need syncing\n    for feature_name in needs_sync:\n        if feature_name in source_changed and not force:\n            # Skip - source changed and not forcing\n            continue\n\n        # Get the version from metadata\n        metadata = self.offline_store.read_metadata(feature_name)\n        if metadata is None:\n            continue\n\n        # Rebuild with the same version\n        try:\n            self.build(\n                feature_names=[feature_name],\n                feature_version=metadata.version,\n                force=True,\n                preview=False,\n            )\n            synced.append(feature_name)\n            logger.info(f\"Synced {feature_name} v{metadata.version}\")\n        except Exception as e:\n            logger.error(f\"Failed to sync {feature_name}: {e}\")\n\n    return {\n        \"needs_sync\": needs_sync,\n        \"source_changed\": source_changed,\n        \"synced\": synced,\n    }\n</code></pre>"},{"location":"api/core/#mlforge.core.Definitions.sync--check-what-needs-syncing","title":"Check what needs syncing","text":"<p>result = defs.sync(dry_run=True) print(result[\"needs_sync\"])</p>"},{"location":"api/core/#mlforge.core.Definitions.sync--rebuild-missing-data","title":"Rebuild missing data","text":"<p>result = defs.sync() print(f\"Synced {len(result['synced'])} features\")</p>"},{"location":"api/core/#mlforge.core.Definitions.validate","title":"validate","text":"<pre><code>validate(\n    feature_names: list[str] | None = None,\n    tag_names: list[str] | None = None,\n) -&gt; list[validation_.FeatureValidationResult]\n</code></pre> <p>Run validation checks on features without building.</p> <p>Loads source data, applies feature transformations, and runs validators on the output. Does not compute metrics or write to storage.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list[str] | None</code> <p>Specific features to validate. Defaults to None (all).</p> <code>None</code> <code>tag_names</code> <code>list[str] | None</code> <p>Specific features to validate by tag. Defaults to None (all).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[FeatureValidationResult]</code> <p>List of FeatureValidationResult objects, one per validated feature.</p> <code>list[FeatureValidationResult]</code> <p>Features without validators are skipped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If specified feature name is not registered</p> Example <p>results = defs.validate(feature_names=[\"user_spend\"]) for result in results:     if not result.passed:         print(f\"{result.feature_name} failed validation\")</p> Source code in <code>src/mlforge/core.py</code> <pre><code>def validate(\n    self,\n    feature_names: list[str] | None = None,\n    tag_names: list[str] | None = None,\n) -&gt; list[validation_.FeatureValidationResult]:\n    \"\"\"\n    Run validation checks on features without building.\n\n    Loads source data, applies feature transformations, and runs validators\n    on the output. Does not compute metrics or write to storage.\n\n    Args:\n        feature_names: Specific features to validate. Defaults to None (all).\n        tag_names: Specific features to validate by tag. Defaults to None (all).\n\n    Returns:\n        List of FeatureValidationResult objects, one per validated feature.\n        Features without validators are skipped.\n\n    Raises:\n        ValueError: If specified feature name is not registered\n\n    Example:\n        results = defs.validate(feature_names=[\"user_spend\"])\n        for result in results:\n            if not result.passed:\n                print(f\"{result.feature_name} failed validation\")\n    \"\"\"\n    selected_features = self._resolve_features_to_build(\n        feature_names, tag_names\n    )\n    results: list[validation_.FeatureValidationResult] = []\n\n    for feature in selected_features:\n        if not feature.validators:\n            logger.debug(f\"Skipping {feature.name} (no validators)\")\n            continue\n\n        try:\n            # Load and process data (without metrics)\n            engine = self._get_engine_for_feature(feature)\n            source_df = engine._load_source(feature.source)\n            processed_df = feature(source_df)\n\n            # Collect if LazyFrame\n            if isinstance(processed_df, pl.LazyFrame):\n                processed_df = processed_df.collect()\n\n            # Run validators\n            result = validation_.validate_feature(\n                feature.name, processed_df, feature.validators\n            )\n            results.append(result)\n\n        except Exception as e:\n            logger.error(f\"Error validating {feature.name}: {e}\")\n            # Create a failed result for the feature\n            results.append(\n                validation_.FeatureValidationResult(\n                    feature_name=feature.name,\n                    column_results=[\n                        validation_.ColumnValidationResult(\n                            column=\"&lt;error&gt;\",\n                            validator_name=\"execution\",\n                            result=validators_.ValidationResult(\n                                passed=False,\n                                message=str(e),\n                            ),\n                        )\n                    ],\n                )\n            )\n\n    return results\n</code></pre>"},{"location":"api/core/#decorators","title":"Decorators","text":""},{"location":"api/core/#mlforge.core.feature","title":"mlforge.core.feature","text":"<pre><code>feature(\n    keys: list[str],\n    source: str,\n    description: str | None = None,\n    tags: list[str] | None = None,\n    timestamp: str | None = None,\n    interval: str | timedelta | None = None,\n    metrics: list[MetricKind] | None = None,\n    validators: dict[str, list[Validator]] | None = None,\n    engine: Literal[\"polars\", \"duckdb\"] | None = None,\n) -&gt; Callable[[FeatureFunction], Feature]\n</code></pre> <p>Decorator that marks a function as a feature definition.</p> <p>Transforms a function into a Feature object that can be registered with Definitions and materialized to storage.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>Column names that uniquely identify entities</p> required <code>source</code> <code>str</code> <p>Path to source data file (parquet or csv)</p> required <code>description</code> <code>str | None</code> <p>Human-readable feature description. Defaults to None.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Tags to group feature with other features. Defaults to None.</p> <code>None</code> <code>timestamp</code> <code>str | None</code> <p>Column name for temporal features. Defaults to None.</p> <code>None</code> <code>interval</code> <code>str | timedelta | None</code> <p>Time interval for rolling computations (e.g., \"1d\" or timedelta(days=1)). Defaults to None.</p> <code>None</code> <code>metrics</code> <code>list[MetricKind] | None</code> <p>Aggregation metrics like Rolling for time-based features. Defaults to None.</p> <code>None</code> <code>validators</code> <code>dict[str, list[Validator]] | None</code> <p>Column validators to run before metrics are computed. Defaults to None. Mapping of column names to lists of validator functions.</p> <code>None</code> <code>engine</code> <code>Literal['polars', 'duckdb'] | None</code> <p>Compute engine to use for this feature. Defaults to None (uses Definitions default). Options: \"polars\" (default), \"duckdb\" (for large datasets).</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[FeatureFunction], Feature]</code> <p>Decorator function that converts a function into a Feature</p> Example <p>@feature(     keys=[\"user_id\"],     source=\"data/transactions.parquet\",     tags=['users'],     timestamp=\"transaction_time\",     description=\"User spending statistics\",     interval=timedelta(days=1),     validators={         \"amount\": [not_null(), greater_than(0)],         \"user_id\": [not_null()],     }, ) def user_spend_stats(df):     return df.group_by(\"user_id\").agg(         pl.col(\"amount\").mean().alias(\"avg_spend\")     )</p> Source code in <code>src/mlforge/core.py</code> <pre><code>def feature(\n    keys: list[str],\n    source: str,\n    description: str | None = None,\n    tags: list[str] | None = None,\n    timestamp: str | None = None,\n    interval: str | timedelta | None = None,\n    metrics: list[metrics_.MetricKind] | None = None,\n    validators: dict[str, list[validators_.Validator]] | None = None,\n    engine: Literal[\"polars\", \"duckdb\"] | None = None,\n) -&gt; Callable[[FeatureFunction], Feature]:\n    \"\"\"\n    Decorator that marks a function as a feature definition.\n\n    Transforms a function into a Feature object that can be registered\n    with Definitions and materialized to storage.\n\n    Args:\n        keys: Column names that uniquely identify entities\n        source: Path to source data file (parquet or csv)\n        description: Human-readable feature description. Defaults to None.\n        tags: Tags to group feature with other features. Defaults to None.\n        timestamp: Column name for temporal features. Defaults to None.\n        interval: Time interval for rolling computations (e.g., \"1d\" or timedelta(days=1)). Defaults to None.\n        metrics: Aggregation metrics like Rolling for time-based features. Defaults to None.\n        validators: Column validators to run before metrics are computed. Defaults to None.\n            Mapping of column names to lists of validator functions.\n        engine: Compute engine to use for this feature. Defaults to None (uses Definitions default).\n            Options: \"polars\" (default), \"duckdb\" (for large datasets).\n\n    Returns:\n        Decorator function that converts a function into a Feature\n\n    Example:\n        @feature(\n            keys=[\"user_id\"],\n            source=\"data/transactions.parquet\",\n            tags=['users'],\n            timestamp=\"transaction_time\",\n            description=\"User spending statistics\",\n            interval=timedelta(days=1),\n            validators={\n                \"amount\": [not_null(), greater_than(0)],\n                \"user_id\": [not_null()],\n            },\n        )\n        def user_spend_stats(df):\n            return df.group_by(\"user_id\").agg(\n                pl.col(\"amount\").mean().alias(\"avg_spend\")\n            )\n\n        # Use DuckDB for large dataset processing\n        @feature(\n            keys=[\"user_id\"],\n            source=\"data/large_transactions.parquet\",\n            engine=\"duckdb\",\n        )\n        def user_large_spend(df):\n            return df.select(\"user_id\", \"amount\")\n    \"\"\"\n    # Convert timedelta to string if provided\n    interval_str = (\n        metrics_.timedelta_to_polars_duration(interval)\n        if isinstance(interval, timedelta)\n        else interval\n    )\n\n    def decorator(fn: FeatureFunction) -&gt; Feature:\n        return Feature(\n            fn=fn,\n            name=fn.__name__,\n            description=description,\n            source=source,\n            keys=keys,\n            tags=tags,\n            timestamp=timestamp,\n            interval=interval_str,\n            metrics=metrics,\n            validators=validators,\n            engine=engine,\n        )\n\n    return decorator\n</code></pre>"},{"location":"api/core/#mlforge.core.feature--use-duckdb-for-large-dataset-processing","title":"Use DuckDB for large dataset processing","text":"<p>@feature(     keys=[\"user_id\"],     source=\"data/large_transactions.parquet\",     engine=\"duckdb\", ) def user_large_spend(df):     return df.select(\"user_id\", \"amount\")</p>"},{"location":"api/core/#protocols","title":"Protocols","text":""},{"location":"api/core/#mlforge.core.FeatureFunction","title":"mlforge.core.FeatureFunction","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the signature for feature transformation functions.</p> Source code in <code>src/mlforge/core.py</code> <pre><code>class FeatureFunction(Protocol):\n    \"\"\"Protocol defining the signature for feature transformation functions.\"\"\"\n\n    __name__: str\n\n    def __call__(self, df: pl.DataFrame) -&gt; pl.DataFrame: ...\n</code></pre>"},{"location":"api/manifest/","title":"Manifest API","text":"<p>The manifest module provides dataclasses and utilities for tracking feature metadata.</p>"},{"location":"api/manifest/#overview","title":"Overview","text":"<p>When features are built, mlforge automatically captures metadata about each feature, including:</p> <ul> <li>Feature configuration (keys, timestamp, interval)</li> <li>Storage details (path, row count)</li> <li>Column information (names, types, aggregations)</li> <li>Build timestamp and source data</li> </ul> <p>This metadata is stored in <code>.meta.json</code> files alongside the feature parquet files and can be queried using the CLI or programmatically.</p>"},{"location":"api/manifest/#dataclasses","title":"Dataclasses","text":""},{"location":"api/manifest/#mlforge.manifest.ColumnMetadata","title":"mlforge.manifest.ColumnMetadata  <code>dataclass</code>","text":"<p>Metadata for a single column in a feature.</p> <p>For columns derived from Rolling metrics, captures the source column, aggregation type, and window size. For other columns, captures dtype. For base columns, captures validator information.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Column name in the output</p> <code>dtype</code> <code>str | None</code> <p>Data type string (e.g., \"Int64\", \"Float64\")</p> <code>input</code> <code>str | None</code> <p>Source column name for aggregations</p> <code>agg</code> <code>str | None</code> <p>Aggregation type (count, mean, sum, etc.)</p> <code>window</code> <code>str | None</code> <p>Time window for rolling aggregations (e.g., \"7d\")</p> <code>validators</code> <code>list[dict[str, Any]] | None</code> <p>List of validator specifications applied to this column</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>@dataclass\nclass ColumnMetadata:\n    \"\"\"\n    Metadata for a single column in a feature.\n\n    For columns derived from Rolling metrics, captures the source column,\n    aggregation type, and window size. For other columns, captures dtype.\n    For base columns, captures validator information.\n\n    Attributes:\n        name: Column name in the output\n        dtype: Data type string (e.g., \"Int64\", \"Float64\")\n        input: Source column name for aggregations\n        agg: Aggregation type (count, mean, sum, etc.)\n        window: Time window for rolling aggregations (e.g., \"7d\")\n        validators: List of validator specifications applied to this column\n    \"\"\"\n\n    name: str\n    dtype: str | None = None\n    input: str | None = None\n    agg: str | None = None\n    window: str | None = None\n    validators: list[dict[str, Any]] | None = None\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary, excluding None values.\"\"\"\n        return {k: v for k, v in asdict(self).items() if v is not None}\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; ColumnMetadata:\n        \"\"\"Create from dictionary.\"\"\"\n        return cls(\n            name=data[\"name\"],\n            dtype=data.get(\"dtype\"),\n            input=data.get(\"input\"),\n            agg=data.get(\"agg\"),\n            window=data.get(\"window\"),\n            validators=data.get(\"validators\"),\n        )\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.ColumnMetadata.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; ColumnMetadata\n</code></pre> <p>Create from dictionary.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; ColumnMetadata:\n    \"\"\"Create from dictionary.\"\"\"\n    return cls(\n        name=data[\"name\"],\n        dtype=data.get(\"dtype\"),\n        input=data.get(\"input\"),\n        agg=data.get(\"agg\"),\n        window=data.get(\"window\"),\n        validators=data.get(\"validators\"),\n    )\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.ColumnMetadata.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary, excluding None values.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary, excluding None values.\"\"\"\n    return {k: v for k, v in asdict(self).items() if v is not None}\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.FeatureMetadata","title":"mlforge.manifest.FeatureMetadata  <code>dataclass</code>","text":"<p>Metadata for a single materialized feature.</p> <p>Captures all information about a feature from both its definition and the results of building it.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Feature identifier</p> <code>path</code> <code>str</code> <p>Storage path for the parquet file</p> <code>entity</code> <code>str</code> <p>Primary entity key (first key in keys list)</p> <code>keys</code> <code>list[str]</code> <p>All entity key columns</p> <code>source</code> <code>str</code> <p>Source data file path</p> <code>row_count</code> <code>int</code> <p>Number of rows in materialized feature</p> <code>updated_at</code> <code>str</code> <p>ISO 8601 timestamp of last build (renamed from last_updated in v0.5.0)</p> <code>version</code> <code>str</code> <p>Semantic version string (v0.5.0)</p> <code>created_at</code> <code>str</code> <p>ISO 8601 timestamp when version was first created (v0.5.0)</p> <code>content_hash</code> <code>str</code> <p>Hash of data.parquet for integrity verification (v0.5.0)</p> <code>schema_hash</code> <code>str</code> <p>Hash of column names + dtypes for change detection (v0.5.0)</p> <code>config_hash</code> <code>str</code> <p>Hash of keys, timestamp, interval, metrics config (v0.5.0)</p> <code>source_hash</code> <code>str</code> <p>Hash of source data file for reproducibility verification (v0.5.0)</p> <code>timestamp</code> <code>str | None</code> <p>Timestamp column for temporal features</p> <code>interval</code> <code>str | None</code> <p>Time interval for rolling aggregations</p> <code>columns</code> <code>list[ColumnMetadata]</code> <p>Base column metadata (from feature function before metrics)</p> <code>features</code> <code>list[ColumnMetadata]</code> <p>Generated feature column metadata (from metrics)</p> <code>tags</code> <code>list[str]</code> <p>Feature grouping tags</p> <code>description</code> <code>str | None</code> <p>Human-readable description</p> <code>change_summary</code> <code>dict[str, Any] | None</code> <p>Documents why version was bumped (v0.5.0)</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>@dataclass\nclass FeatureMetadata:\n    \"\"\"\n    Metadata for a single materialized feature.\n\n    Captures all information about a feature from both its definition\n    and the results of building it.\n\n    Attributes:\n        name: Feature identifier\n        path: Storage path for the parquet file\n        entity: Primary entity key (first key in keys list)\n        keys: All entity key columns\n        source: Source data file path\n        row_count: Number of rows in materialized feature\n        updated_at: ISO 8601 timestamp of last build (renamed from last_updated in v0.5.0)\n        version: Semantic version string (v0.5.0)\n        created_at: ISO 8601 timestamp when version was first created (v0.5.0)\n        content_hash: Hash of data.parquet for integrity verification (v0.5.0)\n        schema_hash: Hash of column names + dtypes for change detection (v0.5.0)\n        config_hash: Hash of keys, timestamp, interval, metrics config (v0.5.0)\n        source_hash: Hash of source data file for reproducibility verification (v0.5.0)\n        timestamp: Timestamp column for temporal features\n        interval: Time interval for rolling aggregations\n        columns: Base column metadata (from feature function before metrics)\n        features: Generated feature column metadata (from metrics)\n        tags: Feature grouping tags\n        description: Human-readable description\n        change_summary: Documents why version was bumped (v0.5.0)\n    \"\"\"\n\n    name: str\n    path: str\n    entity: str\n    keys: list[str]\n    source: str\n    row_count: int\n    updated_at: str\n\n    # v0.5.0: New required fields for versioning\n    version: str = \"1.0.0\"\n    created_at: str = \"\"\n    content_hash: str = \"\"\n    schema_hash: str = \"\"\n    config_hash: str = \"\"\n    source_hash: str = \"\"\n\n    # Existing optional fields\n    timestamp: str | None = None\n    interval: str | None = None\n    columns: list[ColumnMetadata] = field(default_factory=list)\n    features: list[ColumnMetadata] = field(default_factory=list)\n    tags: list[str] = field(default_factory=list)\n    description: str | None = None\n\n    # v0.5.0: New optional field\n    change_summary: dict[str, Any] | None = None\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        result: dict[str, Any] = {\n            \"name\": self.name,\n            \"version\": self.version,\n            \"path\": self.path,\n            \"entity\": self.entity,\n            \"keys\": self.keys,\n            \"source\": self.source,\n            \"row_count\": self.row_count,\n            \"created_at\": self.created_at,\n            \"updated_at\": self.updated_at,\n            \"content_hash\": self.content_hash,\n            \"schema_hash\": self.schema_hash,\n            \"config_hash\": self.config_hash,\n            \"source_hash\": self.source_hash,\n        }\n        if self.timestamp:\n            result[\"timestamp\"] = self.timestamp\n        if self.interval:\n            result[\"interval\"] = self.interval\n        if self.columns:\n            result[\"columns\"] = [col.to_dict() for col in self.columns]\n        if self.features:\n            result[\"features\"] = [col.to_dict() for col in self.features]\n        if self.tags:\n            result[\"tags\"] = self.tags\n        if self.description:\n            result[\"description\"] = self.description\n        if self.change_summary:\n            result[\"change_summary\"] = self.change_summary\n        return result\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; FeatureMetadata:\n        \"\"\"Create from dictionary.\"\"\"\n        columns = [ColumnMetadata.from_dict(c) for c in data.get(\"columns\", [])]\n        features = [\n            ColumnMetadata.from_dict(c) for c in data.get(\"features\", [])\n        ]\n\n        # Handle backward compatibility: last_updated \u2192 updated_at\n        updated_at = data.get(\"updated_at\") or data.get(\"last_updated\", \"\")\n\n        return cls(\n            name=data[\"name\"],\n            version=data.get(\"version\", \"1.0.0\"),\n            path=data[\"path\"],\n            entity=data[\"entity\"],\n            keys=data[\"keys\"],\n            source=data[\"source\"],\n            row_count=data[\"row_count\"],\n            created_at=data.get(\"created_at\", \"\"),\n            updated_at=updated_at,\n            content_hash=data.get(\"content_hash\", \"\"),\n            schema_hash=data.get(\"schema_hash\", \"\"),\n            config_hash=data.get(\"config_hash\", \"\"),\n            source_hash=data.get(\"source_hash\", \"\"),\n            timestamp=data.get(\"timestamp\"),\n            interval=data.get(\"interval\"),\n            columns=columns,\n            features=features,\n            tags=data.get(\"tags\", []),\n            description=data.get(\"description\"),\n            change_summary=data.get(\"change_summary\"),\n        )\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.FeatureMetadata.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; FeatureMetadata\n</code></pre> <p>Create from dictionary.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; FeatureMetadata:\n    \"\"\"Create from dictionary.\"\"\"\n    columns = [ColumnMetadata.from_dict(c) for c in data.get(\"columns\", [])]\n    features = [\n        ColumnMetadata.from_dict(c) for c in data.get(\"features\", [])\n    ]\n\n    # Handle backward compatibility: last_updated \u2192 updated_at\n    updated_at = data.get(\"updated_at\") or data.get(\"last_updated\", \"\")\n\n    return cls(\n        name=data[\"name\"],\n        version=data.get(\"version\", \"1.0.0\"),\n        path=data[\"path\"],\n        entity=data[\"entity\"],\n        keys=data[\"keys\"],\n        source=data[\"source\"],\n        row_count=data[\"row_count\"],\n        created_at=data.get(\"created_at\", \"\"),\n        updated_at=updated_at,\n        content_hash=data.get(\"content_hash\", \"\"),\n        schema_hash=data.get(\"schema_hash\", \"\"),\n        config_hash=data.get(\"config_hash\", \"\"),\n        source_hash=data.get(\"source_hash\", \"\"),\n        timestamp=data.get(\"timestamp\"),\n        interval=data.get(\"interval\"),\n        columns=columns,\n        features=features,\n        tags=data.get(\"tags\", []),\n        description=data.get(\"description\"),\n        change_summary=data.get(\"change_summary\"),\n    )\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.FeatureMetadata.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for JSON serialization.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n    result: dict[str, Any] = {\n        \"name\": self.name,\n        \"version\": self.version,\n        \"path\": self.path,\n        \"entity\": self.entity,\n        \"keys\": self.keys,\n        \"source\": self.source,\n        \"row_count\": self.row_count,\n        \"created_at\": self.created_at,\n        \"updated_at\": self.updated_at,\n        \"content_hash\": self.content_hash,\n        \"schema_hash\": self.schema_hash,\n        \"config_hash\": self.config_hash,\n        \"source_hash\": self.source_hash,\n    }\n    if self.timestamp:\n        result[\"timestamp\"] = self.timestamp\n    if self.interval:\n        result[\"interval\"] = self.interval\n    if self.columns:\n        result[\"columns\"] = [col.to_dict() for col in self.columns]\n    if self.features:\n        result[\"features\"] = [col.to_dict() for col in self.features]\n    if self.tags:\n        result[\"tags\"] = self.tags\n    if self.description:\n        result[\"description\"] = self.description\n    if self.change_summary:\n        result[\"change_summary\"] = self.change_summary\n    return result\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.Manifest","title":"mlforge.manifest.Manifest  <code>dataclass</code>","text":"<p>Consolidated manifest containing all feature metadata.</p> <p>Aggregates individual feature metadata into a single view. Generated on demand from per-feature .meta.json files.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>str</code> <p>Schema version for compatibility</p> <code>generated_at</code> <code>str</code> <p>ISO 8601 timestamp when manifest was generated</p> <code>features</code> <code>dict[str, FeatureMetadata]</code> <p>Mapping of feature names to their metadata</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>@dataclass\nclass Manifest:\n    \"\"\"\n    Consolidated manifest containing all feature metadata.\n\n    Aggregates individual feature metadata into a single view.\n    Generated on demand from per-feature .meta.json files.\n\n    Attributes:\n        version: Schema version for compatibility\n        generated_at: ISO 8601 timestamp when manifest was generated\n        features: Mapping of feature names to their metadata\n    \"\"\"\n\n    version: str = \"1.0\"\n    generated_at: str = field(\n        default_factory=lambda: datetime.now(timezone.utc)\n        .isoformat()\n        .replace(\"+00:00\", \"Z\")\n    )\n    features: dict[str, FeatureMetadata] = field(default_factory=dict)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        return {\n            \"version\": self.version,\n            \"generated_at\": self.generated_at,\n            \"features\": {\n                name: meta.to_dict() for name, meta in self.features.items()\n            },\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; Manifest:\n        \"\"\"Create from dictionary.\"\"\"\n        features = {\n            name: FeatureMetadata.from_dict(meta)\n            for name, meta in data.get(\"features\", {}).items()\n        }\n        return cls(\n            version=data.get(\"version\", \"1.0\"),\n            generated_at=data.get(\"generated_at\", \"\"),\n            features=features,\n        )\n\n    def add_feature(self, metadata: FeatureMetadata) -&gt; None:\n        \"\"\"Add or update a feature in the manifest.\"\"\"\n        self.features[metadata.name] = metadata\n\n    def remove_feature(self, name: str) -&gt; None:\n        \"\"\"Remove a feature from the manifest.\"\"\"\n        self.features.pop(name, None)\n\n    def get_feature(self, name: str) -&gt; FeatureMetadata | None:\n        \"\"\"Get metadata for a specific feature.\"\"\"\n        return self.features.get(name)\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.Manifest.add_feature","title":"add_feature","text":"<pre><code>add_feature(metadata: FeatureMetadata) -&gt; None\n</code></pre> <p>Add or update a feature in the manifest.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def add_feature(self, metadata: FeatureMetadata) -&gt; None:\n    \"\"\"Add or update a feature in the manifest.\"\"\"\n    self.features[metadata.name] = metadata\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.Manifest.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; Manifest\n</code></pre> <p>Create from dictionary.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; Manifest:\n    \"\"\"Create from dictionary.\"\"\"\n    features = {\n        name: FeatureMetadata.from_dict(meta)\n        for name, meta in data.get(\"features\", {}).items()\n    }\n    return cls(\n        version=data.get(\"version\", \"1.0\"),\n        generated_at=data.get(\"generated_at\", \"\"),\n        features=features,\n    )\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.Manifest.get_feature","title":"get_feature","text":"<pre><code>get_feature(name: str) -&gt; FeatureMetadata | None\n</code></pre> <p>Get metadata for a specific feature.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def get_feature(self, name: str) -&gt; FeatureMetadata | None:\n    \"\"\"Get metadata for a specific feature.\"\"\"\n    return self.features.get(name)\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.Manifest.remove_feature","title":"remove_feature","text":"<pre><code>remove_feature(name: str) -&gt; None\n</code></pre> <p>Remove a feature from the manifest.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def remove_feature(self, name: str) -&gt; None:\n    \"\"\"Remove a feature from the manifest.\"\"\"\n    self.features.pop(name, None)\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.Manifest.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for JSON serialization.</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n    return {\n        \"version\": self.version,\n        \"generated_at\": self.generated_at,\n        \"features\": {\n            name: meta.to_dict() for name, meta in self.features.items()\n        },\n    }\n</code></pre>"},{"location":"api/manifest/#functions","title":"Functions","text":""},{"location":"api/manifest/#mlforge.manifest.derive_column_metadata","title":"mlforge.manifest.derive_column_metadata","text":"<pre><code>derive_column_metadata(\n    feature: Feature,\n    schema: dict[str, str],\n    base_schema: dict[str, str] | None = None,\n    schema_source: str = \"polars\",\n) -&gt; tuple[list[ColumnMetadata], list[ColumnMetadata]]\n</code></pre> <p>Derive column metadata from feature definition and result schema.</p> <p>Separates base columns (keys, timestamp, other non-metric columns) from generated feature columns (rolling metrics). Uses base_schema when available for accurate separation, falls back to regex parsing for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Feature</code> <p>The Feature definition object</p> required <code>schema</code> <code>dict[str, str]</code> <p>Dictionary mapping column names to dtype strings (final schema after metrics)</p> required <code>base_schema</code> <code>dict[str, str] | None</code> <p>Dictionary mapping column names to dtype strings (before metrics). When provided, enables accurate column separation. Defaults to None.</p> <code>None</code> <code>schema_source</code> <code>str</code> <p>Engine source for type normalization (\"polars\" or \"duckdb\")</p> <code>'polars'</code> <p>Returns:</p> Type Description <code>tuple[list[ColumnMetadata], list[ColumnMetadata]]</code> <p>Tuple of (base_columns, feature_columns) where: - base_columns: Keys, timestamp, and other non-metric columns with validators - feature_columns: Rolling metric columns with aggregation metadata</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def derive_column_metadata(\n    feature: Feature,\n    schema: dict[str, str],\n    base_schema: dict[str, str] | None = None,\n    schema_source: str = \"polars\",\n) -&gt; tuple[list[ColumnMetadata], list[ColumnMetadata]]:\n    \"\"\"\n    Derive column metadata from feature definition and result schema.\n\n    Separates base columns (keys, timestamp, other non-metric columns) from\n    generated feature columns (rolling metrics). Uses base_schema when available\n    for accurate separation, falls back to regex parsing for backward compatibility.\n\n    Args:\n        feature: The Feature definition object\n        schema: Dictionary mapping column names to dtype strings (final schema after metrics)\n        base_schema: Dictionary mapping column names to dtype strings (before metrics).\n            When provided, enables accurate column separation. Defaults to None.\n        schema_source: Engine source for type normalization (\"polars\" or \"duckdb\")\n\n    Returns:\n        Tuple of (base_columns, feature_columns) where:\n            - base_columns: Keys, timestamp, and other non-metric columns with validators\n            - feature_columns: Rolling metric columns with aggregation metadata\n    \"\"\"\n    if base_schema:\n        return _derive_with_base_schema(\n            feature, schema, base_schema, schema_source\n        )\n    return _derive_legacy(feature, schema, schema_source)\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.write_metadata_file","title":"mlforge.manifest.write_metadata_file","text":"<pre><code>write_metadata_file(\n    path: Path, metadata: FeatureMetadata\n) -&gt; None\n</code></pre> <p>Write feature metadata to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to write the .meta.json file</p> required <code>metadata</code> <code>FeatureMetadata</code> <p>FeatureMetadata to serialize</p> required Source code in <code>src/mlforge/manifest.py</code> <pre><code>def write_metadata_file(path: Path, metadata: FeatureMetadata) -&gt; None:\n    \"\"\"\n    Write feature metadata to a JSON file.\n\n    Args:\n        path: Path to write the .meta.json file\n        metadata: FeatureMetadata to serialize\n    \"\"\"\n    with open(path, \"w\") as f:\n        json.dump(metadata.to_dict(), f, indent=2)\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.read_metadata_file","title":"mlforge.manifest.read_metadata_file","text":"<pre><code>read_metadata_file(path: Path) -&gt; FeatureMetadata | None\n</code></pre> <p>Read feature metadata from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the .meta.json file</p> required <p>Returns:</p> Type Description <code>FeatureMetadata | None</code> <p>FeatureMetadata if file exists and is valid, None otherwise</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def read_metadata_file(path: Path) -&gt; FeatureMetadata | None:\n    \"\"\"\n    Read feature metadata from a JSON file.\n\n    Args:\n        path: Path to the .meta.json file\n\n    Returns:\n        FeatureMetadata if file exists and is valid, None otherwise\n    \"\"\"\n    if not path.exists():\n        return None\n\n    try:\n        with open(path) as f:\n            data = json.load(f)\n    except json.JSONDecodeError as e:\n        logger.warning(f\"Invalid JSON in {path}: {e}\")\n        return None\n\n    try:\n        return FeatureMetadata.from_dict(data)\n    except KeyError as e:\n        logger.warning(f\"Schema mismatch in {path}: missing key {e}\")\n        return None\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.write_manifest_file","title":"mlforge.manifest.write_manifest_file","text":"<pre><code>write_manifest_file(\n    path: Path | str, manifest: Manifest\n) -&gt; None\n</code></pre> <p>Write consolidated manifest to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to write the manifest.json file</p> required <code>manifest</code> <code>Manifest</code> <p>Manifest to serialize</p> required Source code in <code>src/mlforge/manifest.py</code> <pre><code>def write_manifest_file(path: Path | str, manifest: Manifest) -&gt; None:\n    \"\"\"\n    Write consolidated manifest to a JSON file.\n\n    Args:\n        path: Path to write the manifest.json file\n        manifest: Manifest to serialize\n    \"\"\"\n    with open(path, \"w\") as f:\n        json.dump(manifest.to_dict(), f, indent=2)\n</code></pre>"},{"location":"api/manifest/#mlforge.manifest.read_manifest_file","title":"mlforge.manifest.read_manifest_file","text":"<pre><code>read_manifest_file(path: Path) -&gt; Manifest | None\n</code></pre> <p>Read consolidated manifest from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the manifest.json file</p> required <p>Returns:</p> Type Description <code>Manifest | None</code> <p>Manifest if file exists and is valid, None otherwise</p> Source code in <code>src/mlforge/manifest.py</code> <pre><code>def read_manifest_file(path: Path) -&gt; Manifest | None:\n    \"\"\"\n    Read consolidated manifest from a JSON file.\n\n    Args:\n        path: Path to the manifest.json file\n\n    Returns:\n        Manifest if file exists and is valid, None otherwise\n    \"\"\"\n    if not path.exists():\n        return None\n\n    try:\n        with open(path) as f:\n            data = json.load(f)\n    except json.JSONDecodeError as e:\n        logger.warning(f\"Invalid JSON in {path}: {e}\")\n        return None\n\n    try:\n        return Manifest.from_dict(data)\n    except KeyError as e:\n        logger.warning(f\"Schema mismatch in {path}: missing key {e}\")\n        return None\n</code></pre>"},{"location":"api/manifest/#usage-examples","title":"Usage Examples","text":""},{"location":"api/manifest/#reading-feature-metadata","title":"Reading Feature Metadata","text":"<pre><code>from mlforge import LocalStore\n\nstore = LocalStore(\"./feature_store\")\n\n# Read metadata for a specific feature\nmetadata = store.read_metadata(\"user_spend\")\n\nif metadata:\n    print(f\"Feature: {metadata.name}\")\n    print(f\"Rows: {metadata.row_count:,}\")\n    print(f\"Last updated: {metadata.last_updated}\")\n    print(f\"Columns: {len(metadata.columns)}\")\n</code></pre>"},{"location":"api/manifest/#listing-all-metadata","title":"Listing All Metadata","text":"<pre><code>from mlforge import LocalStore\n\nstore = LocalStore(\"./feature_store\")\n\n# Get all feature metadata\nall_metadata = store.list_metadata()\n\nfor meta in all_metadata:\n    print(f\"{meta.name}: {meta.row_count:,} rows\")\n</code></pre>"},{"location":"api/manifest/#creating-a-consolidated-manifest","title":"Creating a Consolidated Manifest","text":"<pre><code>from mlforge import LocalStore\nfrom mlforge.manifest import Manifest, write_manifest_file\nfrom datetime import datetime, timezone\n\nstore = LocalStore(\"./feature_store\")\n\n# Create manifest from all features\nmanifest = Manifest(\n    generated_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n)\n\nfor meta in store.list_metadata():\n    manifest.add_feature(meta)\n\n# Write to file\nwrite_manifest_file(\"manifest.json\", manifest)\n</code></pre>"},{"location":"api/manifest/#inspecting-column-metadata","title":"Inspecting Column Metadata","text":"<pre><code>from mlforge import LocalStore\n\nstore = LocalStore(\"./feature_store\")\nmetadata = store.read_metadata(\"user_spend\")\n\nif metadata and metadata.columns:\n    for col in metadata.columns:\n        if col.agg:\n            # Rolling aggregation column\n            print(f\"{col.name}: {col.agg}({col.input}) over {col.window}\")\n        else:\n            # Regular column\n            print(f\"{col.name}: {col.dtype}\")\n</code></pre>"},{"location":"api/manifest/#metadata-schema","title":"Metadata Schema","text":""},{"location":"api/manifest/#feature-metadata-json","title":"Feature Metadata JSON","text":"<p>Per-feature metadata is stored in <code>_metadata/&lt;feature_name&gt;.meta.json</code>:</p> <pre><code>{\n  \"name\": \"merchant_spend\",\n  \"path\": \"merchant_spend.parquet\",\n  \"entity\": \"merchant_id\",\n  \"keys\": [\"merchant_id\"],\n  \"source\": \"data/transactions.parquet\",\n  \"row_count\": 15482,\n  \"last_updated\": \"2024-01-16T08:30:00Z\",\n  \"timestamp\": \"transaction_date\",\n  \"interval\": \"1d\",\n  \"columns\": [\n    {\"name\": \"merchant_id\", \"dtype\": \"Utf8\"},\n    {\"name\": \"transaction_date\", \"dtype\": \"Date\"},\n    {\n      \"name\": \"amt__count__7d\",\n      \"dtype\": \"UInt32\",\n      \"input\": \"amt\",\n      \"agg\": \"count\",\n      \"window\": \"7d\"\n    },\n    {\n      \"name\": \"amt__sum__7d\",\n      \"dtype\": \"Float64\",\n      \"input\": \"amt\",\n      \"agg\": \"sum\",\n      \"window\": \"7d\"\n    }\n  ],\n  \"tags\": [\"merchants\"],\n  \"description\": \"Merchant spend aggregations\"\n}\n</code></pre>"},{"location":"api/manifest/#consolidated-manifest-json","title":"Consolidated Manifest JSON","text":"<p>The manifest consolidates all feature metadata into a single file:</p> <pre><code>{\n  \"version\": \"1.0\",\n  \"generated_at\": \"2024-01-16T08:30:00Z\",\n  \"features\": {\n    \"merchant_spend\": {\n      \"name\": \"merchant_spend\",\n      \"path\": \"merchant_spend.parquet\",\n      ...\n    },\n    \"user_spend\": {\n      \"name\": \"user_spend\",\n      \"path\": \"user_spend.parquet\",\n      ...\n    }\n  }\n}\n</code></pre>"},{"location":"api/manifest/#column-naming-convention","title":"Column Naming Convention","text":"<p>For features with Rolling metrics, columns follow this pattern:</p> <pre><code>{feature_name}__{column}__{aggregation}__{interval}__{window}\n</code></pre> <p>Examples:</p> <ul> <li><code>user_spend__amt__sum__1d__7d</code> - Sum of <code>amt</code> over 7-day window with 1-day interval</li> <li><code>user_spend__amt__count__1d__30d</code> - Count of <code>amt</code> over 30-day window with 1-day interval</li> </ul> <p>The <code>derive_column_metadata()</code> function parses these column names to extract:</p> <ul> <li><code>input</code>: Source column name (<code>amt</code>)</li> <li><code>agg</code>: Aggregation type (<code>sum</code>, <code>count</code>, etc.)</li> <li><code>window</code>: Time window (<code>7d</code>, <code>30d</code>, etc.)</li> </ul>"},{"location":"api/manifest/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - <code>inspect</code> and <code>manifest</code> commands</li> <li>Building Features - How metadata is generated during builds</li> <li>Store API - Store metadata methods</li> </ul>"},{"location":"api/online/","title":"Online Store API","text":"<p>The online module provides abstract base classes and implementations for real-time feature serving.</p>"},{"location":"api/online/#classes","title":"Classes","text":""},{"location":"api/online/#onlinestore-abstract-base-class","title":"OnlineStore (Abstract Base Class)","text":""},{"location":"api/online/#mlforge.online.OnlineStore","title":"mlforge.online.OnlineStore","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for online feature storage backends.</p> <p>Online stores provide low-latency read/write access to feature values for real-time ML inference. Unlike offline stores (which store full feature history), online stores typically only hold the latest values.</p> Key design <ul> <li>Simple key-value model: entity keys -&gt; feature values</li> <li>JSON serialization for human-readable debugging</li> <li>Batch operations for efficient bulk access</li> </ul> Source code in <code>src/mlforge/online.py</code> <pre><code>class OnlineStore(ABC):\n    \"\"\"\n    Abstract base class for online feature storage backends.\n\n    Online stores provide low-latency read/write access to feature values\n    for real-time ML inference. Unlike offline stores (which store full\n    feature history), online stores typically only hold the latest values.\n\n    Key design:\n        - Simple key-value model: entity keys -&gt; feature values\n        - JSON serialization for human-readable debugging\n        - Batch operations for efficient bulk access\n    \"\"\"\n\n    @abstractmethod\n    def write(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n        values: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        Write feature values for a single entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values (e.g., {\"user_id\": \"123\"})\n            values: Feature column values (e.g., {\"amount__sum__7d\": 1500.0})\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def write_batch(\n        self,\n        feature_name: str,\n        records: list[dict[str, Any]],\n        entity_key_columns: list[str],\n    ) -&gt; int:\n        \"\"\"\n        Write feature values for multiple entities.\n\n        Args:\n            feature_name: Name of the feature\n            records: List of records, each containing entity keys and feature values\n            entity_key_columns: Column names that form the entity key\n\n        Returns:\n            Number of records written\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def read(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Read feature values for a single entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values\n\n        Returns:\n            Feature values dict, or None if not found\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def read_batch(\n        self,\n        feature_name: str,\n        entity_keys: list[dict[str, str]],\n    ) -&gt; list[dict[str, Any] | None]:\n        \"\"\"\n        Read feature values for multiple entities.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: List of entity key dicts\n\n        Returns:\n            List of feature value dicts (None for missing entities)\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def delete(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n    ) -&gt; bool:\n        \"\"\"\n        Delete feature values for a single entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values\n\n        Returns:\n            True if deleted, False if not found\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def exists(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n    ) -&gt; bool:\n        \"\"\"\n        Check if feature values exist for an entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values\n\n        Returns:\n            True if exists, False otherwise\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/online/#mlforge.online.OnlineStore.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(\n    feature_name: str, entity_keys: dict[str, str]\n) -&gt; bool\n</code></pre> <p>Delete feature values for a single entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False if not found</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@abstractmethod\ndef delete(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n) -&gt; bool:\n    \"\"\"\n    Delete feature values for a single entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values\n\n    Returns:\n        True if deleted, False if not found\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/online/#mlforge.online.OnlineStore.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists(\n    feature_name: str, entity_keys: dict[str, str]\n) -&gt; bool\n</code></pre> <p>Check if feature values exist for an entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if exists, False otherwise</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@abstractmethod\ndef exists(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n) -&gt; bool:\n    \"\"\"\n    Check if feature values exist for an entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values\n\n    Returns:\n        True if exists, False otherwise\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/online/#mlforge.online.OnlineStore.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read(\n    feature_name: str, entity_keys: dict[str, str]\n) -&gt; dict[str, Any] | None\n</code></pre> <p>Read feature values for a single entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Feature values dict, or None if not found</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@abstractmethod\ndef read(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Read feature values for a single entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values\n\n    Returns:\n        Feature values dict, or None if not found\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/online/#mlforge.online.OnlineStore.read_batch","title":"read_batch  <code>abstractmethod</code>","text":"<pre><code>read_batch(\n    feature_name: str, entity_keys: list[dict[str, str]]\n) -&gt; list[dict[str, Any] | None]\n</code></pre> <p>Read feature values for multiple entities.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>list[dict[str, str]]</code> <p>List of entity key dicts</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any] | None]</code> <p>List of feature value dicts (None for missing entities)</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@abstractmethod\ndef read_batch(\n    self,\n    feature_name: str,\n    entity_keys: list[dict[str, str]],\n) -&gt; list[dict[str, Any] | None]:\n    \"\"\"\n    Read feature values for multiple entities.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: List of entity key dicts\n\n    Returns:\n        List of feature value dicts (None for missing entities)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/online/#mlforge.online.OnlineStore.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(\n    feature_name: str,\n    entity_keys: dict[str, str],\n    values: dict[str, Any],\n) -&gt; None\n</code></pre> <p>Write feature values for a single entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values (e.g., {\"user_id\": \"123\"})</p> required <code>values</code> <code>dict[str, Any]</code> <p>Feature column values (e.g., {\"amount__sum__7d\": 1500.0})</p> required Source code in <code>src/mlforge/online.py</code> <pre><code>@abstractmethod\ndef write(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n    values: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    Write feature values for a single entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values (e.g., {\"user_id\": \"123\"})\n        values: Feature column values (e.g., {\"amount__sum__7d\": 1500.0})\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/online/#mlforge.online.OnlineStore.write_batch","title":"write_batch  <code>abstractmethod</code>","text":"<pre><code>write_batch(\n    feature_name: str,\n    records: list[dict[str, Any]],\n    entity_key_columns: list[str],\n) -&gt; int\n</code></pre> <p>Write feature values for multiple entities.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>records</code> <code>list[dict[str, Any]]</code> <p>List of records, each containing entity keys and feature values</p> required <code>entity_key_columns</code> <code>list[str]</code> <p>Column names that form the entity key</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of records written</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@abstractmethod\ndef write_batch(\n    self,\n    feature_name: str,\n    records: list[dict[str, Any]],\n    entity_key_columns: list[str],\n) -&gt; int:\n    \"\"\"\n    Write feature values for multiple entities.\n\n    Args:\n        feature_name: Name of the feature\n        records: List of records, each containing entity keys and feature values\n        entity_key_columns: Column names that form the entity key\n\n    Returns:\n        Number of records written\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/online/#redisstore","title":"RedisStore","text":""},{"location":"api/online/#mlforge.online.RedisStore","title":"mlforge.online.RedisStore","text":"<p>               Bases: <code>OnlineStore</code></p> <p>Redis-backed online feature store.</p> <p>Stores feature values as JSON in Redis with optional TTL. Uses Redis pipelines for efficient batch operations.</p> <p>Key format: mlforge:{feature_name}:{entity_key_hash} Value format: JSON serialized feature values</p> <p>Attributes:</p> Name Type Description <code>host</code> <p>Redis server hostname</p> <code>port</code> <p>Redis server port</p> <code>db</code> <p>Redis database number</p> <code>password</code> <p>Redis password (optional)</p> <code>ttl</code> <p>Time-to-live in seconds (optional, None = no expiry)</p> <code>prefix</code> <p>Key prefix (default: \"mlforge\")</p> Example <p>store = RedisStore(host=\"localhost\", port=6379, ttl=3600)</p> Source code in <code>src/mlforge/online.py</code> <pre><code>class RedisStore(OnlineStore):\n    \"\"\"\n    Redis-backed online feature store.\n\n    Stores feature values as JSON in Redis with optional TTL.\n    Uses Redis pipelines for efficient batch operations.\n\n    Key format: mlforge:{feature_name}:{entity_key_hash}\n    Value format: JSON serialized feature values\n\n    Attributes:\n        host: Redis server hostname\n        port: Redis server port\n        db: Redis database number\n        password: Redis password (optional)\n        ttl: Time-to-live in seconds (optional, None = no expiry)\n        prefix: Key prefix (default: \"mlforge\")\n\n    Example:\n        store = RedisStore(host=\"localhost\", port=6379, ttl=3600)\n\n        # Write single entity\n        store.write(\n            feature_name=\"user_spend\",\n            entity_keys={\"user_id\": \"user_123\"},\n            values={\"amount__sum__7d\": 1500.0},\n        )\n\n        # Read single entity\n        features = store.read(\n            feature_name=\"user_spend\",\n            entity_keys={\"user_id\": \"user_123\"},\n        )\n        # Returns: {\"amount__sum__7d\": 1500.0}\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: int = 0,\n        password: str | None = None,\n        ttl: int | None = None,\n        prefix: str = \"mlforge\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize Redis online store.\n\n        Args:\n            host: Redis server hostname. Defaults to \"localhost\".\n            port: Redis server port. Defaults to 6379.\n            db: Redis database number. Defaults to 0.\n            password: Redis password. Defaults to None.\n            ttl: Time-to-live in seconds. Defaults to None (no expiry).\n            prefix: Key prefix for all keys. Defaults to \"mlforge\".\n\n        Raises:\n            ImportError: If redis package is not installed\n        \"\"\"\n        try:\n            import redis\n        except ImportError as e:\n            raise ImportError(\n                \"Redis package not installed. \"\n                \"Install with: pip install mlforge[redis]\"\n            ) from e\n\n        self.host = host\n        self.port = port\n        self.db = db\n        self.password = password\n        self.ttl = ttl\n        self.prefix = prefix\n\n        self._client = redis.Redis(\n            host=host,\n            port=port,\n            db=db,\n            password=password,\n            decode_responses=True,\n        )\n\n    def _build_key(self, feature_name: str, entity_keys: dict[str, str]) -&gt; str:\n        \"\"\"Build Redis key with configured prefix.\"\"\"\n        entity_hash = _compute_entity_hash(entity_keys)\n        return f\"{self.prefix}:{feature_name}:{entity_hash}\"\n\n    @override\n    def write(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n        values: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        Write feature values for a single entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values\n            values: Feature column values to store\n        \"\"\"\n        key = self._build_key(feature_name, entity_keys)\n        value_json = json.dumps(values)\n\n        if self.ttl:\n            self._client.setex(key, self.ttl, value_json)\n        else:\n            self._client.set(key, value_json)\n\n    @override\n    def write_batch(\n        self,\n        feature_name: str,\n        records: list[dict[str, Any]],\n        entity_key_columns: list[str],\n    ) -&gt; int:\n        \"\"\"\n        Write feature values for multiple entities using Redis pipeline.\n\n        Args:\n            feature_name: Name of the feature\n            records: List of records with entity keys and feature values\n            entity_key_columns: Column names that form the entity key\n\n        Returns:\n            Number of records written\n        \"\"\"\n        if not records:\n            return 0\n\n        pipe = self._client.pipeline()\n\n        for record in records:\n            # Extract entity keys\n            entity_keys = {col: str(record[col]) for col in entity_key_columns}\n\n            # Extract feature values (all columns except entity keys)\n            values = {\n                k: v for k, v in record.items() if k not in entity_key_columns\n            }\n\n            key = self._build_key(feature_name, entity_keys)\n            value_json = json.dumps(values)\n\n            if self.ttl:\n                pipe.setex(key, self.ttl, value_json)\n            else:\n                pipe.set(key, value_json)\n\n        pipe.execute()\n        return len(records)\n\n    @override\n    def read(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Read feature values for a single entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values\n\n        Returns:\n            Feature values dict, or None if not found\n        \"\"\"\n        key = self._build_key(feature_name, entity_keys)\n        value = cast(str | None, self._client.get(key))\n\n        if value is None:\n            return None\n\n        return json.loads(value)\n\n    @override\n    def read_batch(\n        self,\n        feature_name: str,\n        entity_keys: list[dict[str, str]],\n    ) -&gt; list[dict[str, Any] | None]:\n        \"\"\"\n        Read feature values for multiple entities using Redis pipeline.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: List of entity key dicts\n\n        Returns:\n            List of feature value dicts (None for missing entities)\n        \"\"\"\n        if not entity_keys:\n            return []\n\n        pipe = self._client.pipeline()\n\n        for keys in entity_keys:\n            key = self._build_key(feature_name, keys)\n            pipe.get(key)\n\n        results = pipe.execute()\n\n        return [\n            json.loads(value) if value is not None else None\n            for value in results\n        ]\n\n    @override\n    def delete(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n    ) -&gt; bool:\n        \"\"\"\n        Delete feature values for a single entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values\n\n        Returns:\n            True if deleted, False if not found\n        \"\"\"\n        key = self._build_key(feature_name, entity_keys)\n        deleted = cast(int, self._client.delete(key))\n        return deleted &gt; 0\n\n    @override\n    def exists(\n        self,\n        feature_name: str,\n        entity_keys: dict[str, str],\n    ) -&gt; bool:\n        \"\"\"\n        Check if feature values exist for an entity.\n\n        Args:\n            feature_name: Name of the feature\n            entity_keys: Entity key columns and values\n\n        Returns:\n            True if exists, False otherwise\n        \"\"\"\n        key = self._build_key(feature_name, entity_keys)\n        count = cast(int, self._client.exists(key))\n        return count &gt; 0\n\n    def ping(self) -&gt; bool:\n        \"\"\"\n        Check Redis connection.\n\n        Returns:\n            True if connected, False otherwise\n        \"\"\"\n        try:\n            return bool(self._client.ping())\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore--write-single-entity","title":"Write single entity","text":"<p>store.write(     feature_name=\"user_spend\",     entity_keys={\"user_id\": \"user_123\"},     values={\"amount__sum__7d\": 1500.0}, )</p>"},{"location":"api/online/#mlforge.online.RedisStore--read-single-entity","title":"Read single entity","text":"<p>features = store.read(     feature_name=\"user_spend\",     entity_keys={\"user_id\": \"user_123\"}, )</p>"},{"location":"api/online/#mlforge.online.RedisStore--returns","title":"Returns:","text":""},{"location":"api/online/#mlforge.online.RedisStore.__init__","title":"__init__","text":"<pre><code>__init__(\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    password: str | None = None,\n    ttl: int | None = None,\n    prefix: str = \"mlforge\",\n) -&gt; None\n</code></pre> <p>Initialize Redis online store.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Redis server hostname. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>Redis server port. Defaults to 6379.</p> <code>6379</code> <code>db</code> <code>int</code> <p>Redis database number. Defaults to 0.</p> <code>0</code> <code>password</code> <code>str | None</code> <p>Redis password. Defaults to None.</p> <code>None</code> <code>ttl</code> <code>int | None</code> <p>Time-to-live in seconds. Defaults to None (no expiry).</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Key prefix for all keys. Defaults to \"mlforge\".</p> <code>'mlforge'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If redis package is not installed</p> Source code in <code>src/mlforge/online.py</code> <pre><code>def __init__(\n    self,\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    password: str | None = None,\n    ttl: int | None = None,\n    prefix: str = \"mlforge\",\n) -&gt; None:\n    \"\"\"\n    Initialize Redis online store.\n\n    Args:\n        host: Redis server hostname. Defaults to \"localhost\".\n        port: Redis server port. Defaults to 6379.\n        db: Redis database number. Defaults to 0.\n        password: Redis password. Defaults to None.\n        ttl: Time-to-live in seconds. Defaults to None (no expiry).\n        prefix: Key prefix for all keys. Defaults to \"mlforge\".\n\n    Raises:\n        ImportError: If redis package is not installed\n    \"\"\"\n    try:\n        import redis\n    except ImportError as e:\n        raise ImportError(\n            \"Redis package not installed. \"\n            \"Install with: pip install mlforge[redis]\"\n        ) from e\n\n    self.host = host\n    self.port = port\n    self.db = db\n    self.password = password\n    self.ttl = ttl\n    self.prefix = prefix\n\n    self._client = redis.Redis(\n        host=host,\n        port=port,\n        db=db,\n        password=password,\n        decode_responses=True,\n    )\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore.delete","title":"delete","text":"<pre><code>delete(\n    feature_name: str, entity_keys: dict[str, str]\n) -&gt; bool\n</code></pre> <p>Delete feature values for a single entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False if not found</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@override\ndef delete(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n) -&gt; bool:\n    \"\"\"\n    Delete feature values for a single entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values\n\n    Returns:\n        True if deleted, False if not found\n    \"\"\"\n    key = self._build_key(feature_name, entity_keys)\n    deleted = cast(int, self._client.delete(key))\n    return deleted &gt; 0\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore.exists","title":"exists","text":"<pre><code>exists(\n    feature_name: str, entity_keys: dict[str, str]\n) -&gt; bool\n</code></pre> <p>Check if feature values exist for an entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if exists, False otherwise</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@override\ndef exists(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n) -&gt; bool:\n    \"\"\"\n    Check if feature values exist for an entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values\n\n    Returns:\n        True if exists, False otherwise\n    \"\"\"\n    key = self._build_key(feature_name, entity_keys)\n    count = cast(int, self._client.exists(key))\n    return count &gt; 0\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore.ping","title":"ping","text":"<pre><code>ping() -&gt; bool\n</code></pre> <p>Check Redis connection.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connected, False otherwise</p> Source code in <code>src/mlforge/online.py</code> <pre><code>def ping(self) -&gt; bool:\n    \"\"\"\n    Check Redis connection.\n\n    Returns:\n        True if connected, False otherwise\n    \"\"\"\n    try:\n        return bool(self._client.ping())\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore.read","title":"read","text":"<pre><code>read(\n    feature_name: str, entity_keys: dict[str, str]\n) -&gt; dict[str, Any] | None\n</code></pre> <p>Read feature values for a single entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Feature values dict, or None if not found</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@override\ndef read(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Read feature values for a single entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values\n\n    Returns:\n        Feature values dict, or None if not found\n    \"\"\"\n    key = self._build_key(feature_name, entity_keys)\n    value = cast(str | None, self._client.get(key))\n\n    if value is None:\n        return None\n\n    return json.loads(value)\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore.read_batch","title":"read_batch","text":"<pre><code>read_batch(\n    feature_name: str, entity_keys: list[dict[str, str]]\n) -&gt; list[dict[str, Any] | None]\n</code></pre> <p>Read feature values for multiple entities using Redis pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>list[dict[str, str]]</code> <p>List of entity key dicts</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any] | None]</code> <p>List of feature value dicts (None for missing entities)</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@override\ndef read_batch(\n    self,\n    feature_name: str,\n    entity_keys: list[dict[str, str]],\n) -&gt; list[dict[str, Any] | None]:\n    \"\"\"\n    Read feature values for multiple entities using Redis pipeline.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: List of entity key dicts\n\n    Returns:\n        List of feature value dicts (None for missing entities)\n    \"\"\"\n    if not entity_keys:\n        return []\n\n    pipe = self._client.pipeline()\n\n    for keys in entity_keys:\n        key = self._build_key(feature_name, keys)\n        pipe.get(key)\n\n    results = pipe.execute()\n\n    return [\n        json.loads(value) if value is not None else None\n        for value in results\n    ]\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore.write","title":"write","text":"<pre><code>write(\n    feature_name: str,\n    entity_keys: dict[str, str],\n    values: dict[str, Any],\n) -&gt; None\n</code></pre> <p>Write feature values for a single entity.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>entity_keys</code> <code>dict[str, str]</code> <p>Entity key columns and values</p> required <code>values</code> <code>dict[str, Any]</code> <p>Feature column values to store</p> required Source code in <code>src/mlforge/online.py</code> <pre><code>@override\ndef write(\n    self,\n    feature_name: str,\n    entity_keys: dict[str, str],\n    values: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    Write feature values for a single entity.\n\n    Args:\n        feature_name: Name of the feature\n        entity_keys: Entity key columns and values\n        values: Feature column values to store\n    \"\"\"\n    key = self._build_key(feature_name, entity_keys)\n    value_json = json.dumps(values)\n\n    if self.ttl:\n        self._client.setex(key, self.ttl, value_json)\n    else:\n        self._client.set(key, value_json)\n</code></pre>"},{"location":"api/online/#mlforge.online.RedisStore.write_batch","title":"write_batch","text":"<pre><code>write_batch(\n    feature_name: str,\n    records: list[dict[str, Any]],\n    entity_key_columns: list[str],\n) -&gt; int\n</code></pre> <p>Write feature values for multiple entities using Redis pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>records</code> <code>list[dict[str, Any]]</code> <p>List of records with entity keys and feature values</p> required <code>entity_key_columns</code> <code>list[str]</code> <p>Column names that form the entity key</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of records written</p> Source code in <code>src/mlforge/online.py</code> <pre><code>@override\ndef write_batch(\n    self,\n    feature_name: str,\n    records: list[dict[str, Any]],\n    entity_key_columns: list[str],\n) -&gt; int:\n    \"\"\"\n    Write feature values for multiple entities using Redis pipeline.\n\n    Args:\n        feature_name: Name of the feature\n        records: List of records with entity keys and feature values\n        entity_key_columns: Column names that form the entity key\n\n    Returns:\n        Number of records written\n    \"\"\"\n    if not records:\n        return 0\n\n    pipe = self._client.pipeline()\n\n    for record in records:\n        # Extract entity keys\n        entity_keys = {col: str(record[col]) for col in entity_key_columns}\n\n        # Extract feature values (all columns except entity keys)\n        values = {\n            k: v for k, v in record.items() if k not in entity_key_columns\n        }\n\n        key = self._build_key(feature_name, entity_keys)\n        value_json = json.dumps(values)\n\n        if self.ttl:\n            pipe.setex(key, self.ttl, value_json)\n        else:\n            pipe.set(key, value_json)\n\n    pipe.execute()\n    return len(records)\n</code></pre>"},{"location":"api/retrieval/","title":"Retrieval API","text":"<p>The retrieval module provides functions for retrieving features for both training (offline) and inference (online) use cases.</p>"},{"location":"api/retrieval/#functions","title":"Functions","text":""},{"location":"api/retrieval/#offline-retrieval-training","title":"Offline Retrieval (Training)","text":""},{"location":"api/retrieval/#mlforge.retrieval.get_training_data","title":"mlforge.retrieval.get_training_data","text":"<pre><code>get_training_data(\n    features: list[FeatureSpec],\n    entity_df: DataFrame,\n    store: str | Path | Store = \"./feature_store\",\n    entities: list[EntityKeyTransform] | None = None,\n    timestamp: str | None = None,\n) -&gt; pl.DataFrame\n</code></pre> <p>Retrieve features and join to an entity DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[FeatureSpec]</code> <p>Feature specifications. Can be: - \"feature_name\" - uses latest version - (\"feature_name\", \"1.0.0\") - uses specific version</p> required <code>entity_df</code> <code>DataFrame</code> <p>DataFrame with entity keys to join on</p> required <code>store</code> <code>str | Path | Store</code> <p>Path to feature store or Store instance</p> <code>'./feature_store'</code> <code>entities</code> <code>list[EntityKeyTransform] | None</code> <p>Entity key transforms to apply to entity_df before joining</p> <code>None</code> <code>timestamp</code> <code>str | None</code> <p>Column in entity_df to use for point-in-time joins.        If provided, features with timestamps will be asof-joined.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>entity_df with feature columns joined</p> Example <p>from mlforge import get_training_data from transactions.entities import with_user_id</p> <p>transactions = pl.read_parquet(\"data/transactions.parquet\")</p> Source code in <code>src/mlforge/retrieval.py</code> <pre><code>def get_training_data(\n    features: list[FeatureSpec],\n    entity_df: pl.DataFrame,\n    store: str | Path | store_.Store = \"./feature_store\",\n    entities: list[utils.EntityKeyTransform] | None = None,\n    timestamp: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Retrieve features and join to an entity DataFrame.\n\n    Args:\n        features: Feature specifications. Can be:\n            - \"feature_name\" - uses latest version\n            - (\"feature_name\", \"1.0.0\") - uses specific version\n        entity_df: DataFrame with entity keys to join on\n        store: Path to feature store or Store instance\n        entities: Entity key transforms to apply to entity_df before joining\n        timestamp: Column in entity_df to use for point-in-time joins.\n                   If provided, features with timestamps will be asof-joined.\n\n    Returns:\n        entity_df with feature columns joined\n\n    Example:\n        from mlforge import get_training_data\n        from transactions.entities import with_user_id\n\n        transactions = pl.read_parquet(\"data/transactions.parquet\")\n\n        # Point-in-time correct training data with mixed versions\n        training_df = get_training_data(\n            features=[\n                \"user_spend_mean_30d\",              # latest version\n                (\"merchant_features\", \"1.0.0\"),    # pinned version\n            ],\n            entity_df=transactions,\n            entities=[with_user_id],\n            timestamp=\"trans_date_trans_time\",\n        )\n    \"\"\"\n    if isinstance(store, (str, Path)):\n        store = store_.LocalStore(path=store)\n\n    result = _apply_entity_transforms(entity_df, entities)\n\n    for feature_spec in features:\n        # Parse feature specification\n        if isinstance(feature_spec, tuple):\n            feature_name, feature_version = feature_spec\n        else:\n            feature_name = feature_spec\n            feature_version = None  # Use latest\n\n        if not store.exists(feature_name, feature_version):\n            version_str = (\n                f\" version '{feature_version}'\" if feature_version else \"\"\n            )\n            raise ValueError(\n                f\"Feature '{feature_name}'{version_str} not found. Run `mlforge build` first.\"\n            )\n\n        feature_df = store.read(feature_name, feature_version)\n        join_keys = list(set(result.columns) &amp; set(feature_df.columns))\n\n        # Remove timestamp columns from join keys\u2014they're handled separately\n        if timestamp:\n            join_keys = [k for k in join_keys if k != timestamp]\n\n        if not join_keys:\n            raise ValueError(\n                f\"No common columns to join '{feature_name}'. \"\n                f\"entity_df has: {result.columns}, feature has: {feature_df.columns}\"\n            )\n\n        # Determine join strategy\n        feature_timestamp = _get_feature_timestamp(feature_df)\n\n        if timestamp and feature_timestamp:\n            # Point-in-time join\n            result = _asof_join(\n                left=result,\n                right=feature_df,\n                on_keys=join_keys,\n                left_timestamp=timestamp,\n                right_timestamp=feature_timestamp,\n            )\n        else:\n            # Standard join\n            result = result.join(feature_df, on=join_keys, how=\"left\")\n\n    return result\n</code></pre>"},{"location":"api/retrieval/#mlforge.retrieval.get_training_data--point-in-time-correct-training-data-with-mixed-versions","title":"Point-in-time correct training data with mixed versions","text":"<p>training_df = get_training_data(     features=[         \"user_spend_mean_30d\",              # latest version         (\"merchant_features\", \"1.0.0\"),    # pinned version     ],     entity_df=transactions,     entities=[with_user_id],     timestamp=\"trans_date_trans_time\", )</p>"},{"location":"api/retrieval/#online-retrieval-inference","title":"Online Retrieval (Inference)","text":""},{"location":"api/retrieval/#mlforge.retrieval.get_online_features","title":"mlforge.retrieval.get_online_features","text":"<pre><code>get_online_features(\n    features: list[str],\n    entity_df: DataFrame,\n    store: OnlineStore,\n    entities: list[EntityKeyTransform] | None = None,\n) -&gt; pl.DataFrame\n</code></pre> <p>Retrieve features from an online store for inference.</p> <p>Unlike get_training_data(), this function: - Always returns latest values (no point-in-time joins) - Does not support versioning (online stores hold latest only) - Uses direct key lookups instead of DataFrame joins</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[str]</code> <p>List of feature names to retrieve</p> required <code>entity_df</code> <code>DataFrame</code> <p>DataFrame with entity keys (e.g., inference requests)</p> required <code>store</code> <code>OnlineStore</code> <p>Online store instance (e.g., RedisStore)</p> required <code>entities</code> <code>list[EntityKeyTransform] | None</code> <p>Optional entity key transforms to apply before lookup</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>entity_df with feature columns joined (None for missing entities)</p> Example <p>from mlforge import get_online_features, RedisStore from myproject.entities import with_user_id</p> <p>store = RedisStore(host=\"localhost\") request_df = pl.DataFrame({\"user_id\": [\"user_123\", \"user_456\"]})</p> <p>features_df = get_online_features(     features=[\"user_spend\"],     entity_df=request_df,     entities=[with_user_id],     store=store, )</p> Source code in <code>src/mlforge/retrieval.py</code> <pre><code>def get_online_features(\n    features: list[str],\n    entity_df: pl.DataFrame,\n    store: online_.OnlineStore,\n    entities: list[utils.EntityKeyTransform] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Retrieve features from an online store for inference.\n\n    Unlike get_training_data(), this function:\n    - Always returns latest values (no point-in-time joins)\n    - Does not support versioning (online stores hold latest only)\n    - Uses direct key lookups instead of DataFrame joins\n\n    Args:\n        features: List of feature names to retrieve\n        entity_df: DataFrame with entity keys (e.g., inference requests)\n        store: Online store instance (e.g., RedisStore)\n        entities: Optional entity key transforms to apply before lookup\n\n    Returns:\n        entity_df with feature columns joined (None for missing entities)\n\n    Example:\n        from mlforge import get_online_features, RedisStore\n        from myproject.entities import with_user_id\n\n        store = RedisStore(host=\"localhost\")\n        request_df = pl.DataFrame({\"user_id\": [\"user_123\", \"user_456\"]})\n\n        features_df = get_online_features(\n            features=[\"user_spend\"],\n            entity_df=request_df,\n            entities=[with_user_id],\n            store=store,\n        )\n    \"\"\"\n    result = _apply_entity_transforms(entity_df, entities)\n\n    # Retrieve each feature and join to result\n    for feature_name in features:\n        result = _join_online_feature(result, feature_name, store, entities)\n\n    return result\n</code></pre>"},{"location":"api/store/","title":"Store API","text":"<p>The store module defines interfaces and implementations for offline feature storage.</p>"},{"location":"api/store/#abstract-base-classes","title":"Abstract Base Classes","text":""},{"location":"api/store/#mlforge.store.Store","title":"mlforge.store.Store","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for offline feature storage backends.</p> <p>Defines the interface that all storage implementations must provide for persisting and retrieving materialized features with versioning.</p> <p>v0.5.0: Added version parameter to read/write/exists methods and new list_versions/get_latest_version methods.</p> Source code in <code>src/mlforge/store.py</code> <pre><code>class Store(ABC):\n    \"\"\"\n    Abstract base class for offline feature storage backends.\n\n    Defines the interface that all storage implementations must provide\n    for persisting and retrieving materialized features with versioning.\n\n    v0.5.0: Added version parameter to read/write/exists methods and\n    new list_versions/get_latest_version methods.\n    \"\"\"\n\n    @abstractmethod\n    def write(\n        self,\n        feature_name: str,\n        result: results.ResultKind,\n        feature_version: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Persist a materialized feature to storage.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            result: Result kind containing data to write\n            feature_version: Semantic version string (e.g., \"1.0.0\")\n\n        Returns:\n            Metadata dictionary with path, row_count, schema\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def read(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Retrieve a materialized feature from storage.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version to read. If None, reads latest.\n\n        Returns:\n            Feature data as a DataFrame\n\n        Raises:\n            FileNotFoundError: If the feature/version has not been materialized\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def exists(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; bool:\n        \"\"\"\n        Check whether a feature version has been materialized.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version to check. If None, checks any version.\n\n        Returns:\n            True if feature exists in storage, False otherwise\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def list_versions(self, feature_name: str) -&gt; list[str]:\n        \"\"\"\n        List all versions of a feature.\n\n        Args:\n            feature_name: Unique identifier for the feature\n\n        Returns:\n            Sorted list of version strings (oldest to newest)\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_latest_version(self, feature_name: str) -&gt; str | None:\n        \"\"\"\n        Get the latest version of a feature.\n\n        Args:\n            feature_name: Unique identifier for the feature\n\n        Returns:\n            Latest version string, or None if no versions exist\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def path_for(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; Path | str:\n        \"\"\"\n        Get the storage path for a feature version.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, uses latest.\n\n        Returns:\n            Path or str where the feature is or would be stored\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def metadata_path_for(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; Path | str:\n        \"\"\"\n        Get the storage path for a feature version's metadata file.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, uses latest.\n\n        Returns:\n            Path where the feature's .meta.json is or would be stored\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def write_metadata(\n        self,\n        feature_name: str,\n        metadata: manifest.FeatureMetadata,\n    ) -&gt; None:\n        \"\"\"\n        Write feature metadata to storage.\n\n        Uses metadata.version to determine storage path.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            metadata: FeatureMetadata object to persist\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def read_metadata(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; manifest.FeatureMetadata | None:\n        \"\"\"\n        Read feature metadata from storage.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, reads latest.\n\n        Returns:\n            FeatureMetadata if exists, None otherwise\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def list_metadata(self) -&gt; list[manifest.FeatureMetadata]:\n        \"\"\"\n        List metadata for latest version of all features in the store.\n\n        Returns:\n            List of FeatureMetadata for all features (latest versions only)\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists(\n    feature_name: str, feature_version: str | None = None\n) -&gt; bool\n</code></pre> <p>Check whether a feature version has been materialized.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version to check. If None, checks any version.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if feature exists in storage, False otherwise</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef exists(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; bool:\n    \"\"\"\n    Check whether a feature version has been materialized.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version to check. If None, checks any version.\n\n    Returns:\n        True if feature exists in storage, False otherwise\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.get_latest_version","title":"get_latest_version  <code>abstractmethod</code>","text":"<pre><code>get_latest_version(feature_name: str) -&gt; str | None\n</code></pre> <p>Get the latest version of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Latest version string, or None if no versions exist</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef get_latest_version(self, feature_name: str) -&gt; str | None:\n    \"\"\"\n    Get the latest version of a feature.\n\n    Args:\n        feature_name: Unique identifier for the feature\n\n    Returns:\n        Latest version string, or None if no versions exist\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.list_metadata","title":"list_metadata  <code>abstractmethod</code>","text":"<pre><code>list_metadata() -&gt; list[manifest.FeatureMetadata]\n</code></pre> <p>List metadata for latest version of all features in the store.</p> <p>Returns:</p> Type Description <code>list[FeatureMetadata]</code> <p>List of FeatureMetadata for all features (latest versions only)</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef list_metadata(self) -&gt; list[manifest.FeatureMetadata]:\n    \"\"\"\n    List metadata for latest version of all features in the store.\n\n    Returns:\n        List of FeatureMetadata for all features (latest versions only)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.list_versions","title":"list_versions  <code>abstractmethod</code>","text":"<pre><code>list_versions(feature_name: str) -&gt; list[str]\n</code></pre> <p>List all versions of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of version strings (oldest to newest)</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef list_versions(self, feature_name: str) -&gt; list[str]:\n    \"\"\"\n    List all versions of a feature.\n\n    Args:\n        feature_name: Unique identifier for the feature\n\n    Returns:\n        Sorted list of version strings (oldest to newest)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.metadata_path_for","title":"metadata_path_for  <code>abstractmethod</code>","text":"<pre><code>metadata_path_for(\n    feature_name: str, feature_version: str | None = None\n) -&gt; Path | str\n</code></pre> <p>Get the storage path for a feature version's metadata file.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, uses latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path | str</code> <p>Path where the feature's .meta.json is or would be stored</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef metadata_path_for(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; Path | str:\n    \"\"\"\n    Get the storage path for a feature version's metadata file.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, uses latest.\n\n    Returns:\n        Path where the feature's .meta.json is or would be stored\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.path_for","title":"path_for  <code>abstractmethod</code>","text":"<pre><code>path_for(\n    feature_name: str, feature_version: str | None = None\n) -&gt; Path | str\n</code></pre> <p>Get the storage path for a feature version.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, uses latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path | str</code> <p>Path or str where the feature is or would be stored</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef path_for(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; Path | str:\n    \"\"\"\n    Get the storage path for a feature version.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, uses latest.\n\n    Returns:\n        Path or str where the feature is or would be stored\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read(\n    feature_name: str, feature_version: str | None = None\n) -&gt; pl.DataFrame\n</code></pre> <p>Retrieve a materialized feature from storage.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version to read. If None, reads latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Feature data as a DataFrame</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the feature/version has not been materialized</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef read(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Retrieve a materialized feature from storage.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version to read. If None, reads latest.\n\n    Returns:\n        Feature data as a DataFrame\n\n    Raises:\n        FileNotFoundError: If the feature/version has not been materialized\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.read_metadata","title":"read_metadata  <code>abstractmethod</code>","text":"<pre><code>read_metadata(\n    feature_name: str, feature_version: str | None = None\n) -&gt; manifest.FeatureMetadata | None\n</code></pre> <p>Read feature metadata from storage.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, reads latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>FeatureMetadata | None</code> <p>FeatureMetadata if exists, None otherwise</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef read_metadata(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; manifest.FeatureMetadata | None:\n    \"\"\"\n    Read feature metadata from storage.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, reads latest.\n\n    Returns:\n        FeatureMetadata if exists, None otherwise\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(\n    feature_name: str,\n    result: ResultKind,\n    feature_version: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Persist a materialized feature to storage.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>result</code> <code>ResultKind</code> <p>Result kind containing data to write</p> required <code>feature_version</code> <code>str</code> <p>Semantic version string (e.g., \"1.0.0\")</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Metadata dictionary with path, row_count, schema</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef write(\n    self,\n    feature_name: str,\n    result: results.ResultKind,\n    feature_version: str,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Persist a materialized feature to storage.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        result: Result kind containing data to write\n        feature_version: Semantic version string (e.g., \"1.0.0\")\n\n    Returns:\n        Metadata dictionary with path, row_count, schema\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#mlforge.store.Store.write_metadata","title":"write_metadata  <code>abstractmethod</code>","text":"<pre><code>write_metadata(\n    feature_name: str, metadata: FeatureMetadata\n) -&gt; None\n</code></pre> <p>Write feature metadata to storage.</p> <p>Uses metadata.version to determine storage path.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>metadata</code> <code>FeatureMetadata</code> <p>FeatureMetadata object to persist</p> required Source code in <code>src/mlforge/store.py</code> <pre><code>@abstractmethod\ndef write_metadata(\n    self,\n    feature_name: str,\n    metadata: manifest.FeatureMetadata,\n) -&gt; None:\n    \"\"\"\n    Write feature metadata to storage.\n\n    Uses metadata.version to determine storage path.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        metadata: FeatureMetadata object to persist\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/store/#implementations","title":"Implementations","text":""},{"location":"api/store/#local-storage","title":"Local Storage","text":""},{"location":"api/store/#mlforge.store.LocalStore","title":"mlforge.store.LocalStore","text":"<p>               Bases: <code>Store</code></p> <p>Local filesystem storage backend using Parquet format.</p> Stores features in versioned directories <p>feature_store/ \u251c\u2500\u2500 user_spend/ \u2502   \u251c\u2500\u2500 1.0.0/ \u2502   \u2502   \u251c\u2500\u2500 data.parquet \u2502   \u2502   \u2514\u2500\u2500 .meta.json \u2502   \u251c\u2500\u2500 1.1.0/ \u2502   \u2502   \u2514\u2500\u2500 ... \u2502   \u2514\u2500\u2500 _latest.json</p> <p>Attributes:</p> Name Type Description <code>path</code> <p>Root directory for storing feature files</p> Example <p>store = LocalStore(\"./feature_store\") store.write(\"user_age\", result, version=\"1.0.0\") age_df = store.read(\"user_age\")  # reads latest age_df = store.read(\"user_age\", version=\"1.0.0\")  # reads specific</p> Source code in <code>src/mlforge/store.py</code> <pre><code>class LocalStore(Store):\n    \"\"\"\n    Local filesystem storage backend using Parquet format.\n\n    Stores features in versioned directories:\n        feature_store/\n        \u251c\u2500\u2500 user_spend/\n        \u2502   \u251c\u2500\u2500 1.0.0/\n        \u2502   \u2502   \u251c\u2500\u2500 data.parquet\n        \u2502   \u2502   \u2514\u2500\u2500 .meta.json\n        \u2502   \u251c\u2500\u2500 1.1.0/\n        \u2502   \u2502   \u2514\u2500\u2500 ...\n        \u2502   \u2514\u2500\u2500 _latest.json\n\n    Attributes:\n        path: Root directory for storing feature files\n\n    Example:\n        store = LocalStore(\"./feature_store\")\n        store.write(\"user_age\", result, version=\"1.0.0\")\n        age_df = store.read(\"user_age\")  # reads latest\n        age_df = store.read(\"user_age\", version=\"1.0.0\")  # reads specific\n    \"\"\"\n\n    def __init__(self, path: str | Path = \"./feature_store\"):\n        \"\"\"\n        Initialize local storage backend.\n\n        Args:\n            path: Directory path for feature storage. Defaults to \"./feature_store\".\n        \"\"\"\n        self.path = Path(path)\n        self.path.mkdir(parents=True, exist_ok=True)\n\n    @override\n    def path_for(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; Path:\n        \"\"\"\n        Get file path for a feature version.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, uses latest.\n\n        Returns:\n            Path to the feature's parquet file\n        \"\"\"\n        resolved = version.resolve_version(\n            self.path, feature_name, feature_version\n        )\n\n        if resolved is None:\n            # No versions exist yet - return path for hypothetical 1.0.0\n            return version.versioned_data_path(self.path, feature_name, \"1.0.0\")\n\n        return version.versioned_data_path(self.path, feature_name, resolved)\n\n    @override\n    def write(\n        self,\n        feature_name: str,\n        result: results.ResultKind,\n        feature_version: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Write feature data to versioned parquet file.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            result: Engine result containing feature data and metadata\n            feature_version: Semantic version string (e.g., \"1.0.0\")\n\n        Returns:\n            Metadata dictionary with path, row count, and schema\n        \"\"\"\n        path = version.versioned_data_path(\n            self.path, feature_name, feature_version\n        )\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        result.write_parquet(path)\n\n        # Update _latest.json pointer\n        version.write_latest_pointer(self.path, feature_name, feature_version)\n\n        # Create .gitignore in feature directory (if not present)\n        version.write_feature_gitignore(self.path, feature_name)\n\n        return {\n            \"path\": str(path),\n            \"row_count\": result.row_count(),\n            \"schema\": result.schema(),\n        }\n\n    @override\n    def read(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read feature data from versioned parquet file.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version to read. If None, reads latest.\n\n        Returns:\n            Feature data as a DataFrame\n\n        Raises:\n            FileNotFoundError: If the feature/version doesn't exist\n        \"\"\"\n        resolved = version.resolve_version(\n            self.path, feature_name, feature_version\n        )\n\n        if resolved is None:\n            raise FileNotFoundError(\n                f\"Feature '{feature_name}' not found. Run 'mlforge build' first.\"\n            )\n\n        path = version.versioned_data_path(self.path, feature_name, resolved)\n\n        if not path.exists():\n            raise FileNotFoundError(\n                f\"Feature '{feature_name}' version '{resolved}' not found.\"\n            )\n\n        return pl.read_parquet(path)\n\n    @override\n    def exists(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; bool:\n        \"\"\"\n        Check if feature version exists.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version to check. If None, checks any version.\n\n        Returns:\n            True if the feature/version exists, False otherwise\n        \"\"\"\n        if feature_version is None:\n            # Check if any version exists\n            return len(self.list_versions(feature_name)) &gt; 0\n\n        path = version.versioned_data_path(\n            self.path, feature_name, feature_version\n        )\n        return path.exists()\n\n    @override\n    def list_versions(self, feature_name: str) -&gt; list[str]:\n        \"\"\"\n        List all versions of a feature.\n\n        Args:\n            feature_name: Unique identifier for the feature\n\n        Returns:\n            Sorted list of version strings (oldest to newest)\n        \"\"\"\n        return version.list_versions(self.path, feature_name)\n\n    @override\n    def get_latest_version(self, feature_name: str) -&gt; str | None:\n        \"\"\"\n        Get the latest version of a feature.\n\n        Args:\n            feature_name: Unique identifier for the feature\n\n        Returns:\n            Latest version string, or None if no versions exist\n        \"\"\"\n        return version.get_latest_version(self.path, feature_name)\n\n    @override\n    def metadata_path_for(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; Path:\n        \"\"\"\n        Get file path for a feature version's metadata.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, uses latest.\n\n        Returns:\n            Path to the feature's .meta.json file\n        \"\"\"\n        resolved = version.resolve_version(\n            self.path, feature_name, feature_version\n        )\n\n        if resolved is None:\n            # No versions exist yet\n            return version.versioned_metadata_path(\n                self.path, feature_name, \"1.0.0\"\n            )\n\n        return version.versioned_metadata_path(\n            self.path, feature_name, resolved\n        )\n\n    @override\n    def write_metadata(\n        self,\n        feature_name: str,\n        metadata: manifest.FeatureMetadata,\n    ) -&gt; None:\n        \"\"\"\n        Write feature metadata to versioned JSON file.\n\n        Uses metadata.version to determine storage path.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            metadata: FeatureMetadata object to persist\n        \"\"\"\n        path = version.versioned_metadata_path(\n            self.path, feature_name, metadata.version\n        )\n        path.parent.mkdir(parents=True, exist_ok=True)\n        manifest.write_metadata_file(path, metadata)\n\n    @override\n    def read_metadata(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; manifest.FeatureMetadata | None:\n        \"\"\"\n        Read feature metadata from versioned JSON file.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, reads latest.\n\n        Returns:\n            FeatureMetadata if exists and valid, None otherwise\n        \"\"\"\n        resolved = version.resolve_version(\n            self.path, feature_name, feature_version\n        )\n\n        if resolved is None:\n            return None\n\n        path = version.versioned_metadata_path(\n            self.path, feature_name, resolved\n        )\n        return manifest.read_metadata_file(path)\n\n    @override\n    def list_metadata(self) -&gt; list[manifest.FeatureMetadata]:\n        \"\"\"\n        List metadata for latest version of all features.\n\n        Scans for feature directories and reads their latest metadata.\n\n        Returns:\n            List of FeatureMetadata for all features (latest versions only)\n        \"\"\"\n        metadata_list: list[manifest.FeatureMetadata] = []\n\n        # Scan for feature directories (contain version subdirectories)\n        for feature_dir in self.path.iterdir():\n            if not feature_dir.is_dir() or feature_dir.name.startswith(\"_\"):\n                continue\n\n            latest = self.get_latest_version(feature_dir.name)\n            if latest:\n                meta = self.read_metadata(feature_dir.name, latest)\n                if meta:\n                    metadata_list.append(meta)\n\n        return metadata_list\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.__init__","title":"__init__","text":"<pre><code>__init__(path: str | Path = './feature_store')\n</code></pre> <p>Initialize local storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Directory path for feature storage. Defaults to \"./feature_store\".</p> <code>'./feature_store'</code> Source code in <code>src/mlforge/store.py</code> <pre><code>def __init__(self, path: str | Path = \"./feature_store\"):\n    \"\"\"\n    Initialize local storage backend.\n\n    Args:\n        path: Directory path for feature storage. Defaults to \"./feature_store\".\n    \"\"\"\n    self.path = Path(path)\n    self.path.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.exists","title":"exists","text":"<pre><code>exists(\n    feature_name: str, feature_version: str | None = None\n) -&gt; bool\n</code></pre> <p>Check if feature version exists.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version to check. If None, checks any version.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the feature/version exists, False otherwise</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef exists(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; bool:\n    \"\"\"\n    Check if feature version exists.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version to check. If None, checks any version.\n\n    Returns:\n        True if the feature/version exists, False otherwise\n    \"\"\"\n    if feature_version is None:\n        # Check if any version exists\n        return len(self.list_versions(feature_name)) &gt; 0\n\n    path = version.versioned_data_path(\n        self.path, feature_name, feature_version\n    )\n    return path.exists()\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.get_latest_version","title":"get_latest_version","text":"<pre><code>get_latest_version(feature_name: str) -&gt; str | None\n</code></pre> <p>Get the latest version of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Latest version string, or None if no versions exist</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef get_latest_version(self, feature_name: str) -&gt; str | None:\n    \"\"\"\n    Get the latest version of a feature.\n\n    Args:\n        feature_name: Unique identifier for the feature\n\n    Returns:\n        Latest version string, or None if no versions exist\n    \"\"\"\n    return version.get_latest_version(self.path, feature_name)\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.list_metadata","title":"list_metadata","text":"<pre><code>list_metadata() -&gt; list[manifest.FeatureMetadata]\n</code></pre> <p>List metadata for latest version of all features.</p> <p>Scans for feature directories and reads their latest metadata.</p> <p>Returns:</p> Type Description <code>list[FeatureMetadata]</code> <p>List of FeatureMetadata for all features (latest versions only)</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef list_metadata(self) -&gt; list[manifest.FeatureMetadata]:\n    \"\"\"\n    List metadata for latest version of all features.\n\n    Scans for feature directories and reads their latest metadata.\n\n    Returns:\n        List of FeatureMetadata for all features (latest versions only)\n    \"\"\"\n    metadata_list: list[manifest.FeatureMetadata] = []\n\n    # Scan for feature directories (contain version subdirectories)\n    for feature_dir in self.path.iterdir():\n        if not feature_dir.is_dir() or feature_dir.name.startswith(\"_\"):\n            continue\n\n        latest = self.get_latest_version(feature_dir.name)\n        if latest:\n            meta = self.read_metadata(feature_dir.name, latest)\n            if meta:\n                metadata_list.append(meta)\n\n    return metadata_list\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.list_versions","title":"list_versions","text":"<pre><code>list_versions(feature_name: str) -&gt; list[str]\n</code></pre> <p>List all versions of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of version strings (oldest to newest)</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef list_versions(self, feature_name: str) -&gt; list[str]:\n    \"\"\"\n    List all versions of a feature.\n\n    Args:\n        feature_name: Unique identifier for the feature\n\n    Returns:\n        Sorted list of version strings (oldest to newest)\n    \"\"\"\n    return version.list_versions(self.path, feature_name)\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.metadata_path_for","title":"metadata_path_for","text":"<pre><code>metadata_path_for(\n    feature_name: str, feature_version: str | None = None\n) -&gt; Path\n</code></pre> <p>Get file path for a feature version's metadata.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, uses latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the feature's .meta.json file</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef metadata_path_for(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Get file path for a feature version's metadata.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, uses latest.\n\n    Returns:\n        Path to the feature's .meta.json file\n    \"\"\"\n    resolved = version.resolve_version(\n        self.path, feature_name, feature_version\n    )\n\n    if resolved is None:\n        # No versions exist yet\n        return version.versioned_metadata_path(\n            self.path, feature_name, \"1.0.0\"\n        )\n\n    return version.versioned_metadata_path(\n        self.path, feature_name, resolved\n    )\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.path_for","title":"path_for","text":"<pre><code>path_for(\n    feature_name: str, feature_version: str | None = None\n) -&gt; Path\n</code></pre> <p>Get file path for a feature version.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, uses latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the feature's parquet file</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef path_for(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Get file path for a feature version.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, uses latest.\n\n    Returns:\n        Path to the feature's parquet file\n    \"\"\"\n    resolved = version.resolve_version(\n        self.path, feature_name, feature_version\n    )\n\n    if resolved is None:\n        # No versions exist yet - return path for hypothetical 1.0.0\n        return version.versioned_data_path(self.path, feature_name, \"1.0.0\")\n\n    return version.versioned_data_path(self.path, feature_name, resolved)\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.read","title":"read","text":"<pre><code>read(\n    feature_name: str, feature_version: str | None = None\n) -&gt; pl.DataFrame\n</code></pre> <p>Read feature data from versioned parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version to read. If None, reads latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Feature data as a DataFrame</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the feature/version doesn't exist</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef read(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read feature data from versioned parquet file.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version to read. If None, reads latest.\n\n    Returns:\n        Feature data as a DataFrame\n\n    Raises:\n        FileNotFoundError: If the feature/version doesn't exist\n    \"\"\"\n    resolved = version.resolve_version(\n        self.path, feature_name, feature_version\n    )\n\n    if resolved is None:\n        raise FileNotFoundError(\n            f\"Feature '{feature_name}' not found. Run 'mlforge build' first.\"\n        )\n\n    path = version.versioned_data_path(self.path, feature_name, resolved)\n\n    if not path.exists():\n        raise FileNotFoundError(\n            f\"Feature '{feature_name}' version '{resolved}' not found.\"\n        )\n\n    return pl.read_parquet(path)\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(\n    feature_name: str, feature_version: str | None = None\n) -&gt; manifest.FeatureMetadata | None\n</code></pre> <p>Read feature metadata from versioned JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, reads latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>FeatureMetadata | None</code> <p>FeatureMetadata if exists and valid, None otherwise</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef read_metadata(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; manifest.FeatureMetadata | None:\n    \"\"\"\n    Read feature metadata from versioned JSON file.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, reads latest.\n\n    Returns:\n        FeatureMetadata if exists and valid, None otherwise\n    \"\"\"\n    resolved = version.resolve_version(\n        self.path, feature_name, feature_version\n    )\n\n    if resolved is None:\n        return None\n\n    path = version.versioned_metadata_path(\n        self.path, feature_name, resolved\n    )\n    return manifest.read_metadata_file(path)\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.write","title":"write","text":"<pre><code>write(\n    feature_name: str,\n    result: ResultKind,\n    feature_version: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Write feature data to versioned parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>result</code> <code>ResultKind</code> <p>Engine result containing feature data and metadata</p> required <code>feature_version</code> <code>str</code> <p>Semantic version string (e.g., \"1.0.0\")</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Metadata dictionary with path, row count, and schema</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef write(\n    self,\n    feature_name: str,\n    result: results.ResultKind,\n    feature_version: str,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Write feature data to versioned parquet file.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        result: Engine result containing feature data and metadata\n        feature_version: Semantic version string (e.g., \"1.0.0\")\n\n    Returns:\n        Metadata dictionary with path, row count, and schema\n    \"\"\"\n    path = version.versioned_data_path(\n        self.path, feature_name, feature_version\n    )\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    result.write_parquet(path)\n\n    # Update _latest.json pointer\n    version.write_latest_pointer(self.path, feature_name, feature_version)\n\n    # Create .gitignore in feature directory (if not present)\n    version.write_feature_gitignore(self.path, feature_name)\n\n    return {\n        \"path\": str(path),\n        \"row_count\": result.row_count(),\n        \"schema\": result.schema(),\n    }\n</code></pre>"},{"location":"api/store/#mlforge.store.LocalStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(\n    feature_name: str, metadata: FeatureMetadata\n) -&gt; None\n</code></pre> <p>Write feature metadata to versioned JSON file.</p> <p>Uses metadata.version to determine storage path.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>metadata</code> <code>FeatureMetadata</code> <p>FeatureMetadata object to persist</p> required Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef write_metadata(\n    self,\n    feature_name: str,\n    metadata: manifest.FeatureMetadata,\n) -&gt; None:\n    \"\"\"\n    Write feature metadata to versioned JSON file.\n\n    Uses metadata.version to determine storage path.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        metadata: FeatureMetadata object to persist\n    \"\"\"\n    path = version.versioned_metadata_path(\n        self.path, feature_name, metadata.version\n    )\n    path.parent.mkdir(parents=True, exist_ok=True)\n    manifest.write_metadata_file(path, metadata)\n</code></pre>"},{"location":"api/store/#cloud-storage","title":"Cloud Storage","text":""},{"location":"api/store/#mlforge.store.S3Store","title":"mlforge.store.S3Store","text":"<p>               Bases: <code>Store</code></p> <p>Amazon S3 storage backend using Parquet format.</p> Stores features in versioned directories within an S3 bucket <p>s3://bucket/prefix/ \u251c\u2500\u2500 user_spend/ \u2502   \u251c\u2500\u2500 1.0.0/ \u2502   \u2502   \u251c\u2500\u2500 data.parquet \u2502   \u2502   \u2514\u2500\u2500 .meta.json \u2502   \u251c\u2500\u2500 1.1.0/ \u2502   \u2502   \u2514\u2500\u2500 ... \u2502   \u2514\u2500\u2500 _latest.json</p> <p>Uses AWS credentials from environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION).</p> <p>Attributes:</p> Name Type Description <code>bucket</code> <p>S3 bucket name for storing features</p> <code>prefix</code> <p>Optional path prefix within the bucket</p> <code>region</code> <p>AWS region (optional)</p> Example <p>store = S3Store(bucket=\"mlforge-features\", prefix=\"prod/features\") store.write(\"user_age\", result, version=\"1.0.0\") age_df = store.read(\"user_age\")  # reads latest</p> Source code in <code>src/mlforge/store.py</code> <pre><code>class S3Store(Store):\n    \"\"\"\n    Amazon S3 storage backend using Parquet format.\n\n    Stores features in versioned directories within an S3 bucket:\n        s3://bucket/prefix/\n        \u251c\u2500\u2500 user_spend/\n        \u2502   \u251c\u2500\u2500 1.0.0/\n        \u2502   \u2502   \u251c\u2500\u2500 data.parquet\n        \u2502   \u2502   \u2514\u2500\u2500 .meta.json\n        \u2502   \u251c\u2500\u2500 1.1.0/\n        \u2502   \u2502   \u2514\u2500\u2500 ...\n        \u2502   \u2514\u2500\u2500 _latest.json\n\n    Uses AWS credentials from environment variables\n    (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION).\n\n    Attributes:\n        bucket: S3 bucket name for storing features\n        prefix: Optional path prefix within the bucket\n        region: AWS region (optional)\n\n    Example:\n        store = S3Store(bucket=\"mlforge-features\", prefix=\"prod/features\")\n        store.write(\"user_age\", result, version=\"1.0.0\")\n        age_df = store.read(\"user_age\")  # reads latest\n    \"\"\"\n\n    def __init__(\n        self, bucket: str, prefix: str = \"\", region: str | None = None\n    ) -&gt; None:\n        \"\"\"\n        Initialize S3 storage backend.\n\n        Args:\n            bucket: S3 bucket name for feature storage\n            prefix: Path prefix within bucket. Defaults to empty string.\n            region: AWS region. Defaults to None (uses AWS_DEFAULT_REGION).\n\n        Raises:\n            ValueError: If bucket doesn't exist or is not accessible\n        \"\"\"\n        self.bucket = bucket\n        self.prefix = prefix.strip(\"/\")\n        self.region = region\n        self._s3 = s3fs.S3FileSystem()  # Uses AWS env vars automatically\n\n        if not self._s3.exists(self.bucket):\n            raise ValueError(\n                f\"Bucket '{self.bucket}' does not exist or is not accessible. \"\n                f\"Ensure the bucket is created and credentials have appropriate permissions.\"\n            )\n\n    def _base_path(self) -&gt; str:\n        \"\"\"Get base S3 path (bucket/prefix).\"\"\"\n        if self.prefix:\n            return f\"s3://{self.bucket}/{self.prefix}\"\n        return f\"s3://{self.bucket}\"\n\n    def _versioned_data_path(\n        self, feature_name: str, feature_version: str\n    ) -&gt; str:\n        \"\"\"Get S3 path for versioned feature data.\"\"\"\n        return (\n            f\"{self._base_path()}/{feature_name}/{feature_version}/data.parquet\"\n        )\n\n    def _versioned_metadata_path(\n        self, feature_name: str, feature_version: str\n    ) -&gt; str:\n        \"\"\"Get S3 path for versioned feature metadata.\"\"\"\n        return (\n            f\"{self._base_path()}/{feature_name}/{feature_version}/.meta.json\"\n        )\n\n    def _latest_pointer_path(self, feature_name: str) -&gt; str:\n        \"\"\"Get S3 path for _latest.json pointer.\"\"\"\n        return f\"{self._base_path()}/{feature_name}/_latest.json\"\n\n    def _feature_dir_path(self, feature_name: str) -&gt; str:\n        \"\"\"Get S3 path for feature directory.\"\"\"\n        return f\"{self._base_path()}/{feature_name}\"\n\n    def _write_latest_pointer(\n        self, feature_name: str, feature_version: str\n    ) -&gt; None:\n        \"\"\"Write _latest.json pointer to S3.\"\"\"\n        path = self._latest_pointer_path(feature_name)\n        with self._s3.open(path, \"w\") as f:\n            json.dump({\"version\": feature_version}, f, indent=2)\n\n    def _read_latest_pointer(self, feature_name: str) -&gt; str | None:\n        \"\"\"Read _latest.json pointer from S3.\"\"\"\n        path = self._latest_pointer_path(feature_name)\n        if not self._s3.exists(path):\n            return None\n\n        try:\n            with self._s3.open(path, \"r\") as f:\n                data = json.load(f)\n            return data.get(\"version\")\n        except (json.JSONDecodeError, KeyError):\n            return None\n\n    @override\n    def list_versions(self, feature_name: str) -&gt; list[str]:\n        \"\"\"\n        List all versions of a feature in S3.\n\n        Args:\n            feature_name: Unique identifier for the feature\n\n        Returns:\n            Sorted list of version strings (oldest to newest)\n        \"\"\"\n        feature_dir = self._feature_dir_path(feature_name)\n\n        # Remove s3:// prefix for ls\n        feature_dir_key = feature_dir.replace(\"s3://\", \"\")\n\n        try:\n            # List directories in the feature directory\n            items = self._s3.ls(feature_dir_key, detail=False)\n        except FileNotFoundError:\n            return []\n\n        versions = []\n        for item in items:\n            # Extract the directory name (version)\n            name = item.split(\"/\")[-1]\n            if version.is_valid_version(name):\n                versions.append(name)\n\n        return version.sort_versions(versions)\n\n    @override\n    def get_latest_version(self, feature_name: str) -&gt; str | None:\n        \"\"\"\n        Get the latest version of a feature from S3.\n\n        Args:\n            feature_name: Unique identifier for the feature\n\n        Returns:\n            Latest version string, or None if no versions exist\n        \"\"\"\n        return self._read_latest_pointer(feature_name)\n\n    def _resolve_version(\n        self, feature_name: str, feature_version: str | None\n    ) -&gt; str | None:\n        \"\"\"Resolve version to latest if None.\"\"\"\n        if feature_version is not None:\n            return feature_version\n        return self.get_latest_version(feature_name)\n\n    @override\n    def path_for(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Get S3 URI for a feature version.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, uses latest.\n\n        Returns:\n            S3 URI where the feature is or would be stored\n        \"\"\"\n        resolved = self._resolve_version(feature_name, feature_version)\n\n        if resolved is None:\n            # No versions exist yet\n            return self._versioned_data_path(feature_name, \"1.0.0\")\n\n        return self._versioned_data_path(feature_name, resolved)\n\n    @override\n    def write(\n        self,\n        feature_name: str,\n        result: results.ResultKind,\n        feature_version: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Write feature data to versioned S3 parquet file.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            result: Engine result containing feature data and metadata\n            feature_version: Semantic version string (e.g., \"1.0.0\")\n\n        Returns:\n            Metadata dictionary with S3 URI, row count, and schema\n        \"\"\"\n        path = self._versioned_data_path(feature_name, feature_version)\n        result.write_parquet(path)\n\n        # Update _latest.json pointer\n        self._write_latest_pointer(feature_name, feature_version)\n\n        return {\n            \"path\": path,\n            \"row_count\": result.row_count(),\n            \"schema\": result.schema(),\n        }\n\n    @override\n    def read(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read feature data from versioned S3 parquet file.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version to read. If None, reads latest.\n\n        Returns:\n            Feature data as a DataFrame\n\n        Raises:\n            FileNotFoundError: If the feature/version doesn't exist\n        \"\"\"\n        resolved = self._resolve_version(feature_name, feature_version)\n\n        if resolved is None:\n            raise FileNotFoundError(\n                f\"Feature '{feature_name}' not found. Run 'mlforge build' first.\"\n            )\n\n        path = self._versioned_data_path(feature_name, resolved)\n\n        if not self._s3.exists(path):\n            raise FileNotFoundError(\n                f\"Feature '{feature_name}' version '{resolved}' not found.\"\n            )\n\n        return pl.read_parquet(path)\n\n    @override\n    def exists(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; bool:\n        \"\"\"\n        Check if feature version exists in S3.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version to check. If None, checks any version.\n\n        Returns:\n            True if the feature/version exists, False otherwise\n        \"\"\"\n        if feature_version is None:\n            # Check if any version exists\n            return len(self.list_versions(feature_name)) &gt; 0\n\n        path = self._versioned_data_path(feature_name, feature_version)\n        return self._s3.exists(path)\n\n    @override\n    def metadata_path_for(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Get S3 URI for a feature version's metadata.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, uses latest.\n\n        Returns:\n            S3 URI where the feature's metadata is or would be stored\n        \"\"\"\n        resolved = self._resolve_version(feature_name, feature_version)\n\n        if resolved is None:\n            return self._versioned_metadata_path(feature_name, \"1.0.0\")\n\n        return self._versioned_metadata_path(feature_name, resolved)\n\n    @override\n    def write_metadata(\n        self,\n        feature_name: str,\n        metadata: manifest.FeatureMetadata,\n    ) -&gt; None:\n        \"\"\"\n        Write feature metadata to versioned S3 JSON file.\n\n        Uses metadata.version to determine storage path.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            metadata: FeatureMetadata object to persist\n        \"\"\"\n        path = self._versioned_metadata_path(feature_name, metadata.version)\n        with self._s3.open(path, \"w\") as f:\n            json.dump(metadata.to_dict(), f, indent=2)\n\n    @override\n    def read_metadata(\n        self,\n        feature_name: str,\n        feature_version: str | None = None,\n    ) -&gt; manifest.FeatureMetadata | None:\n        \"\"\"\n        Read feature metadata from versioned S3 JSON file.\n\n        Args:\n            feature_name: Unique identifier for the feature\n            feature_version: Specific version. If None, reads latest.\n\n        Returns:\n            FeatureMetadata if exists and valid, None otherwise\n        \"\"\"\n        resolved = self._resolve_version(feature_name, feature_version)\n\n        if resolved is None:\n            return None\n\n        path = self._versioned_metadata_path(feature_name, resolved)\n\n        if not self._s3.exists(path):\n            return None\n\n        try:\n            with self._s3.open(path, \"r\") as f:\n                data = json.load(f)\n        except json.JSONDecodeError as e:\n            logger.warning(f\"Invalid JSON in {path}: {e}\")\n            return None\n\n        try:\n            return manifest.FeatureMetadata.from_dict(data)\n        except KeyError as e:\n            logger.warning(f\"Schema mismatch in {path}: missing key {e}\")\n            return None\n\n    @override\n    def list_metadata(self) -&gt; list[manifest.FeatureMetadata]:\n        \"\"\"\n        List metadata for latest version of all features in S3.\n\n        Scans for feature directories and reads their latest metadata.\n\n        Returns:\n            List of FeatureMetadata for all features (latest versions only)\n        \"\"\"\n        metadata_list: list[manifest.FeatureMetadata] = []\n\n        # List all directories at the base path\n        base_key = self._base_path().replace(\"s3://\", \"\")\n\n        try:\n            items = self._s3.ls(base_key, detail=False)\n        except FileNotFoundError:\n            return []\n\n        for item in items:\n            feature_name = item.split(\"/\")[-1]\n\n            # Skip hidden/metadata directories\n            if feature_name.startswith(\"_\"):\n                continue\n\n            latest = self.get_latest_version(feature_name)\n            if latest:\n                meta = self.read_metadata(feature_name, latest)\n                if meta:\n                    metadata_list.append(meta)\n\n        return metadata_list\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.__init__","title":"__init__","text":"<pre><code>__init__(\n    bucket: str, prefix: str = \"\", region: str | None = None\n) -&gt; None\n</code></pre> <p>Initialize S3 storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name for feature storage</p> required <code>prefix</code> <code>str</code> <p>Path prefix within bucket. Defaults to empty string.</p> <code>''</code> <code>region</code> <code>str | None</code> <p>AWS region. Defaults to None (uses AWS_DEFAULT_REGION).</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bucket doesn't exist or is not accessible</p> Source code in <code>src/mlforge/store.py</code> <pre><code>def __init__(\n    self, bucket: str, prefix: str = \"\", region: str | None = None\n) -&gt; None:\n    \"\"\"\n    Initialize S3 storage backend.\n\n    Args:\n        bucket: S3 bucket name for feature storage\n        prefix: Path prefix within bucket. Defaults to empty string.\n        region: AWS region. Defaults to None (uses AWS_DEFAULT_REGION).\n\n    Raises:\n        ValueError: If bucket doesn't exist or is not accessible\n    \"\"\"\n    self.bucket = bucket\n    self.prefix = prefix.strip(\"/\")\n    self.region = region\n    self._s3 = s3fs.S3FileSystem()  # Uses AWS env vars automatically\n\n    if not self._s3.exists(self.bucket):\n        raise ValueError(\n            f\"Bucket '{self.bucket}' does not exist or is not accessible. \"\n            f\"Ensure the bucket is created and credentials have appropriate permissions.\"\n        )\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.exists","title":"exists","text":"<pre><code>exists(\n    feature_name: str, feature_version: str | None = None\n) -&gt; bool\n</code></pre> <p>Check if feature version exists in S3.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version to check. If None, checks any version.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the feature/version exists, False otherwise</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef exists(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; bool:\n    \"\"\"\n    Check if feature version exists in S3.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version to check. If None, checks any version.\n\n    Returns:\n        True if the feature/version exists, False otherwise\n    \"\"\"\n    if feature_version is None:\n        # Check if any version exists\n        return len(self.list_versions(feature_name)) &gt; 0\n\n    path = self._versioned_data_path(feature_name, feature_version)\n    return self._s3.exists(path)\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.get_latest_version","title":"get_latest_version","text":"<pre><code>get_latest_version(feature_name: str) -&gt; str | None\n</code></pre> <p>Get the latest version of a feature from S3.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Latest version string, or None if no versions exist</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef get_latest_version(self, feature_name: str) -&gt; str | None:\n    \"\"\"\n    Get the latest version of a feature from S3.\n\n    Args:\n        feature_name: Unique identifier for the feature\n\n    Returns:\n        Latest version string, or None if no versions exist\n    \"\"\"\n    return self._read_latest_pointer(feature_name)\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.list_metadata","title":"list_metadata","text":"<pre><code>list_metadata() -&gt; list[manifest.FeatureMetadata]\n</code></pre> <p>List metadata for latest version of all features in S3.</p> <p>Scans for feature directories and reads their latest metadata.</p> <p>Returns:</p> Type Description <code>list[FeatureMetadata]</code> <p>List of FeatureMetadata for all features (latest versions only)</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef list_metadata(self) -&gt; list[manifest.FeatureMetadata]:\n    \"\"\"\n    List metadata for latest version of all features in S3.\n\n    Scans for feature directories and reads their latest metadata.\n\n    Returns:\n        List of FeatureMetadata for all features (latest versions only)\n    \"\"\"\n    metadata_list: list[manifest.FeatureMetadata] = []\n\n    # List all directories at the base path\n    base_key = self._base_path().replace(\"s3://\", \"\")\n\n    try:\n        items = self._s3.ls(base_key, detail=False)\n    except FileNotFoundError:\n        return []\n\n    for item in items:\n        feature_name = item.split(\"/\")[-1]\n\n        # Skip hidden/metadata directories\n        if feature_name.startswith(\"_\"):\n            continue\n\n        latest = self.get_latest_version(feature_name)\n        if latest:\n            meta = self.read_metadata(feature_name, latest)\n            if meta:\n                metadata_list.append(meta)\n\n    return metadata_list\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.list_versions","title":"list_versions","text":"<pre><code>list_versions(feature_name: str) -&gt; list[str]\n</code></pre> <p>List all versions of a feature in S3.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of version strings (oldest to newest)</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef list_versions(self, feature_name: str) -&gt; list[str]:\n    \"\"\"\n    List all versions of a feature in S3.\n\n    Args:\n        feature_name: Unique identifier for the feature\n\n    Returns:\n        Sorted list of version strings (oldest to newest)\n    \"\"\"\n    feature_dir = self._feature_dir_path(feature_name)\n\n    # Remove s3:// prefix for ls\n    feature_dir_key = feature_dir.replace(\"s3://\", \"\")\n\n    try:\n        # List directories in the feature directory\n        items = self._s3.ls(feature_dir_key, detail=False)\n    except FileNotFoundError:\n        return []\n\n    versions = []\n    for item in items:\n        # Extract the directory name (version)\n        name = item.split(\"/\")[-1]\n        if version.is_valid_version(name):\n            versions.append(name)\n\n    return version.sort_versions(versions)\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.metadata_path_for","title":"metadata_path_for","text":"<pre><code>metadata_path_for(\n    feature_name: str, feature_version: str | None = None\n) -&gt; str\n</code></pre> <p>Get S3 URI for a feature version's metadata.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, uses latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>S3 URI where the feature's metadata is or would be stored</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef metadata_path_for(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Get S3 URI for a feature version's metadata.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, uses latest.\n\n    Returns:\n        S3 URI where the feature's metadata is or would be stored\n    \"\"\"\n    resolved = self._resolve_version(feature_name, feature_version)\n\n    if resolved is None:\n        return self._versioned_metadata_path(feature_name, \"1.0.0\")\n\n    return self._versioned_metadata_path(feature_name, resolved)\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.path_for","title":"path_for","text":"<pre><code>path_for(\n    feature_name: str, feature_version: str | None = None\n) -&gt; str\n</code></pre> <p>Get S3 URI for a feature version.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, uses latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>S3 URI where the feature is or would be stored</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef path_for(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Get S3 URI for a feature version.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, uses latest.\n\n    Returns:\n        S3 URI where the feature is or would be stored\n    \"\"\"\n    resolved = self._resolve_version(feature_name, feature_version)\n\n    if resolved is None:\n        # No versions exist yet\n        return self._versioned_data_path(feature_name, \"1.0.0\")\n\n    return self._versioned_data_path(feature_name, resolved)\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.read","title":"read","text":"<pre><code>read(\n    feature_name: str, feature_version: str | None = None\n) -&gt; pl.DataFrame\n</code></pre> <p>Read feature data from versioned S3 parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version to read. If None, reads latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Feature data as a DataFrame</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the feature/version doesn't exist</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef read(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read feature data from versioned S3 parquet file.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version to read. If None, reads latest.\n\n    Returns:\n        Feature data as a DataFrame\n\n    Raises:\n        FileNotFoundError: If the feature/version doesn't exist\n    \"\"\"\n    resolved = self._resolve_version(feature_name, feature_version)\n\n    if resolved is None:\n        raise FileNotFoundError(\n            f\"Feature '{feature_name}' not found. Run 'mlforge build' first.\"\n        )\n\n    path = self._versioned_data_path(feature_name, resolved)\n\n    if not self._s3.exists(path):\n        raise FileNotFoundError(\n            f\"Feature '{feature_name}' version '{resolved}' not found.\"\n        )\n\n    return pl.read_parquet(path)\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(\n    feature_name: str, feature_version: str | None = None\n) -&gt; manifest.FeatureMetadata | None\n</code></pre> <p>Read feature metadata from versioned S3 JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>feature_version</code> <code>str | None</code> <p>Specific version. If None, reads latest.</p> <code>None</code> <p>Returns:</p> Type Description <code>FeatureMetadata | None</code> <p>FeatureMetadata if exists and valid, None otherwise</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef read_metadata(\n    self,\n    feature_name: str,\n    feature_version: str | None = None,\n) -&gt; manifest.FeatureMetadata | None:\n    \"\"\"\n    Read feature metadata from versioned S3 JSON file.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        feature_version: Specific version. If None, reads latest.\n\n    Returns:\n        FeatureMetadata if exists and valid, None otherwise\n    \"\"\"\n    resolved = self._resolve_version(feature_name, feature_version)\n\n    if resolved is None:\n        return None\n\n    path = self._versioned_metadata_path(feature_name, resolved)\n\n    if not self._s3.exists(path):\n        return None\n\n    try:\n        with self._s3.open(path, \"r\") as f:\n            data = json.load(f)\n    except json.JSONDecodeError as e:\n        logger.warning(f\"Invalid JSON in {path}: {e}\")\n        return None\n\n    try:\n        return manifest.FeatureMetadata.from_dict(data)\n    except KeyError as e:\n        logger.warning(f\"Schema mismatch in {path}: missing key {e}\")\n        return None\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.write","title":"write","text":"<pre><code>write(\n    feature_name: str,\n    result: ResultKind,\n    feature_version: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Write feature data to versioned S3 parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>result</code> <code>ResultKind</code> <p>Engine result containing feature data and metadata</p> required <code>feature_version</code> <code>str</code> <p>Semantic version string (e.g., \"1.0.0\")</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Metadata dictionary with S3 URI, row count, and schema</p> Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef write(\n    self,\n    feature_name: str,\n    result: results.ResultKind,\n    feature_version: str,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Write feature data to versioned S3 parquet file.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        result: Engine result containing feature data and metadata\n        feature_version: Semantic version string (e.g., \"1.0.0\")\n\n    Returns:\n        Metadata dictionary with S3 URI, row count, and schema\n    \"\"\"\n    path = self._versioned_data_path(feature_name, feature_version)\n    result.write_parquet(path)\n\n    # Update _latest.json pointer\n    self._write_latest_pointer(feature_name, feature_version)\n\n    return {\n        \"path\": path,\n        \"row_count\": result.row_count(),\n        \"schema\": result.schema(),\n    }\n</code></pre>"},{"location":"api/store/#mlforge.store.S3Store.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(\n    feature_name: str, metadata: FeatureMetadata\n) -&gt; None\n</code></pre> <p>Write feature metadata to versioned S3 JSON file.</p> <p>Uses metadata.version to determine storage path.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Unique identifier for the feature</p> required <code>metadata</code> <code>FeatureMetadata</code> <p>FeatureMetadata object to persist</p> required Source code in <code>src/mlforge/store.py</code> <pre><code>@override\ndef write_metadata(\n    self,\n    feature_name: str,\n    metadata: manifest.FeatureMetadata,\n) -&gt; None:\n    \"\"\"\n    Write feature metadata to versioned S3 JSON file.\n\n    Uses metadata.version to determine storage path.\n\n    Args:\n        feature_name: Unique identifier for the feature\n        metadata: FeatureMetadata object to persist\n    \"\"\"\n    path = self._versioned_metadata_path(feature_name, metadata.version)\n    with self._s3.open(path, \"w\") as f:\n        json.dump(metadata.to_dict(), f, indent=2)\n</code></pre>"},{"location":"api/store/#type-aliases","title":"Type Aliases","text":""},{"location":"api/store/#mlforge.store.OfflineStoreKind","title":"mlforge.store.OfflineStoreKind","text":"<pre><code>OfflineStoreKind = LocalStore | S3Store\n</code></pre>"},{"location":"api/types/","title":"Types API","text":"<p>The types module provides a unified type system for consistent schema handling across different compute engines (Polars, DuckDB).</p>"},{"location":"api/types/#classes","title":"Classes","text":""},{"location":"api/types/#datatype","title":"DataType","text":""},{"location":"api/types/#mlforge.types.DataType","title":"mlforge.types.DataType  <code>dataclass</code>","text":"<p>Immutable, hashable type representation.</p> <p>This is the canonical type used throughout mlforge for type comparisons, schema hashing, and metadata storage.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>TypeKind</code> <p>The fundamental type category</p> <code>nullable</code> <code>bool</code> <p>Whether null values are allowed (default True)</p> <code>timezone</code> <code>str | None</code> <p>Timezone for DATETIME types (e.g., \"UTC\", \"America/New_York\")</p> <code>precision</code> <code>int | None</code> <p>Decimal precision (total digits)</p> <code>scale</code> <code>int | None</code> <p>Decimal scale (digits after decimal point)</p> Example Source code in <code>src/mlforge/types.py</code> <pre><code>@dataclass(frozen=True)\nclass DataType:\n    \"\"\"\n    Immutable, hashable type representation.\n\n    This is the canonical type used throughout mlforge for type comparisons,\n    schema hashing, and metadata storage.\n\n    Attributes:\n        kind: The fundamental type category\n        nullable: Whether null values are allowed (default True)\n        timezone: Timezone for DATETIME types (e.g., \"UTC\", \"America/New_York\")\n        precision: Decimal precision (total digits)\n        scale: Decimal scale (digits after decimal point)\n\n    Example:\n        # Simple types\n        DataType(TypeKind.INT64)\n        DataType(TypeKind.STRING, nullable=False)\n\n        # Datetime with timezone\n        DataType(TypeKind.DATETIME, timezone=\"UTC\")\n\n        # Decimal with precision\n        DataType(TypeKind.DECIMAL, precision=38, scale=10)\n    \"\"\"\n\n    kind: TypeKind\n    nullable: bool = True\n    timezone: str | None = None\n    precision: int | None = None\n    scale: int | None = None\n\n    def to_canonical_string(self) -&gt; str:\n        \"\"\"\n        Convert to canonical string representation.\n\n        This is the format used in metadata files for human readability\n        and consistent hashing across engines.\n\n        Returns:\n            Canonical type string (e.g., \"int64\", \"datetime[UTC]\")\n        \"\"\"\n        base = self.kind.value\n\n        # Add timezone for datetime\n        if self.kind == TypeKind.DATETIME and self.timezone:\n            return f\"{base}[{self.timezone}]\"\n\n        # Add precision/scale for decimal\n        if self.kind == TypeKind.DECIMAL:\n            if self.precision is not None and self.scale is not None:\n                return f\"{base}[{self.precision},{self.scale}]\"\n            elif self.precision is not None:\n                return f\"{base}[{self.precision}]\"\n\n        return base\n\n    def to_json(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Serialize to JSON-compatible dict.\n\n        Returns:\n            Dictionary suitable for JSON serialization\n        \"\"\"\n        result: dict[str, Any] = {\"kind\": self.kind.value}\n\n        if not self.nullable:\n            result[\"nullable\"] = False\n\n        if self.timezone is not None:\n            result[\"timezone\"] = self.timezone\n\n        if self.precision is not None:\n            result[\"precision\"] = self.precision\n\n        if self.scale is not None:\n            result[\"scale\"] = self.scale\n\n        return result\n\n    @classmethod\n    def from_json(cls, data: dict[str, Any]) -&gt; \"DataType\":\n        \"\"\"\n        Deserialize from JSON dict.\n\n        Args:\n            data: Dictionary from JSON deserialization\n\n        Returns:\n            DataType instance\n        \"\"\"\n        return cls(\n            kind=TypeKind(data[\"kind\"]),\n            nullable=data.get(\"nullable\", True),\n            timezone=data.get(\"timezone\"),\n            precision=data.get(\"precision\"),\n            scale=data.get(\"scale\"),\n        )\n\n    @classmethod\n    def from_canonical_string(cls, s: str) -&gt; \"DataType\":\n        \"\"\"\n        Parse canonical string representation.\n\n        Args:\n            s: Canonical type string (e.g., \"int64\", \"datetime[UTC]\")\n\n        Returns:\n            DataType instance\n        \"\"\"\n        # Handle parameterized types like \"datetime[UTC]\" or \"decimal[38,10]\"\n        if \"[\" in s and s.endswith(\"]\"):\n            base, params = s[:-1].split(\"[\", 1)\n            kind = TypeKind(base)\n\n            if kind == TypeKind.DATETIME:\n                return cls(kind=kind, timezone=params)\n            elif kind == TypeKind.DECIMAL:\n                parts = params.split(\",\")\n                precision = int(parts[0])\n                scale = int(parts[1]) if len(parts) &gt; 1 else None\n                return cls(kind=kind, precision=precision, scale=scale)\n\n        return cls(kind=TypeKind(s))\n\n    def is_numeric(self) -&gt; bool:\n        \"\"\"Check if this is a numeric type.\"\"\"\n        return self.kind in {\n            TypeKind.INT8,\n            TypeKind.INT16,\n            TypeKind.INT32,\n            TypeKind.INT64,\n            TypeKind.UINT8,\n            TypeKind.UINT16,\n            TypeKind.UINT32,\n            TypeKind.UINT64,\n            TypeKind.FLOAT32,\n            TypeKind.FLOAT64,\n            TypeKind.DECIMAL,\n        }\n\n    def is_integer(self) -&gt; bool:\n        \"\"\"Check if this is an integer type.\"\"\"\n        return self.kind in {\n            TypeKind.INT8,\n            TypeKind.INT16,\n            TypeKind.INT32,\n            TypeKind.INT64,\n            TypeKind.UINT8,\n            TypeKind.UINT16,\n            TypeKind.UINT32,\n            TypeKind.UINT64,\n        }\n\n    def is_floating(self) -&gt; bool:\n        \"\"\"Check if this is a floating point type.\"\"\"\n        return self.kind in {TypeKind.FLOAT32, TypeKind.FLOAT64}\n\n    def is_temporal(self) -&gt; bool:\n        \"\"\"Check if this is a temporal type.\"\"\"\n        return self.kind in {\n            TypeKind.DATE,\n            TypeKind.DATETIME,\n            TypeKind.TIME,\n            TypeKind.DURATION,\n        }\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType--simple-types","title":"Simple types","text":"<p>DataType(TypeKind.INT64) DataType(TypeKind.STRING, nullable=False)</p>"},{"location":"api/types/#mlforge.types.DataType--datetime-with-timezone","title":"Datetime with timezone","text":"<p>DataType(TypeKind.DATETIME, timezone=\"UTC\")</p>"},{"location":"api/types/#mlforge.types.DataType--decimal-with-precision","title":"Decimal with precision","text":"<p>DataType(TypeKind.DECIMAL, precision=38, scale=10)</p>"},{"location":"api/types/#mlforge.types.DataType.from_canonical_string","title":"from_canonical_string  <code>classmethod</code>","text":"<pre><code>from_canonical_string(s: str) -&gt; DataType\n</code></pre> <p>Parse canonical string representation.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Canonical type string (e.g., \"int64\", \"datetime[UTC]\")</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>DataType instance</p> Source code in <code>src/mlforge/types.py</code> <pre><code>@classmethod\ndef from_canonical_string(cls, s: str) -&gt; \"DataType\":\n    \"\"\"\n    Parse canonical string representation.\n\n    Args:\n        s: Canonical type string (e.g., \"int64\", \"datetime[UTC]\")\n\n    Returns:\n        DataType instance\n    \"\"\"\n    # Handle parameterized types like \"datetime[UTC]\" or \"decimal[38,10]\"\n    if \"[\" in s and s.endswith(\"]\"):\n        base, params = s[:-1].split(\"[\", 1)\n        kind = TypeKind(base)\n\n        if kind == TypeKind.DATETIME:\n            return cls(kind=kind, timezone=params)\n        elif kind == TypeKind.DECIMAL:\n            parts = params.split(\",\")\n            precision = int(parts[0])\n            scale = int(parts[1]) if len(parts) &gt; 1 else None\n            return cls(kind=kind, precision=precision, scale=scale)\n\n    return cls(kind=TypeKind(s))\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(data: dict[str, Any]) -&gt; DataType\n</code></pre> <p>Deserialize from JSON dict.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary from JSON deserialization</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>DataType instance</p> Source code in <code>src/mlforge/types.py</code> <pre><code>@classmethod\ndef from_json(cls, data: dict[str, Any]) -&gt; \"DataType\":\n    \"\"\"\n    Deserialize from JSON dict.\n\n    Args:\n        data: Dictionary from JSON deserialization\n\n    Returns:\n        DataType instance\n    \"\"\"\n    return cls(\n        kind=TypeKind(data[\"kind\"]),\n        nullable=data.get(\"nullable\", True),\n        timezone=data.get(\"timezone\"),\n        precision=data.get(\"precision\"),\n        scale=data.get(\"scale\"),\n    )\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType.is_floating","title":"is_floating","text":"<pre><code>is_floating() -&gt; bool\n</code></pre> <p>Check if this is a floating point type.</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def is_floating(self) -&gt; bool:\n    \"\"\"Check if this is a floating point type.\"\"\"\n    return self.kind in {TypeKind.FLOAT32, TypeKind.FLOAT64}\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType.is_integer","title":"is_integer","text":"<pre><code>is_integer() -&gt; bool\n</code></pre> <p>Check if this is an integer type.</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def is_integer(self) -&gt; bool:\n    \"\"\"Check if this is an integer type.\"\"\"\n    return self.kind in {\n        TypeKind.INT8,\n        TypeKind.INT16,\n        TypeKind.INT32,\n        TypeKind.INT64,\n        TypeKind.UINT8,\n        TypeKind.UINT16,\n        TypeKind.UINT32,\n        TypeKind.UINT64,\n    }\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType.is_numeric","title":"is_numeric","text":"<pre><code>is_numeric() -&gt; bool\n</code></pre> <p>Check if this is a numeric type.</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def is_numeric(self) -&gt; bool:\n    \"\"\"Check if this is a numeric type.\"\"\"\n    return self.kind in {\n        TypeKind.INT8,\n        TypeKind.INT16,\n        TypeKind.INT32,\n        TypeKind.INT64,\n        TypeKind.UINT8,\n        TypeKind.UINT16,\n        TypeKind.UINT32,\n        TypeKind.UINT64,\n        TypeKind.FLOAT32,\n        TypeKind.FLOAT64,\n        TypeKind.DECIMAL,\n    }\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType.is_temporal","title":"is_temporal","text":"<pre><code>is_temporal() -&gt; bool\n</code></pre> <p>Check if this is a temporal type.</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def is_temporal(self) -&gt; bool:\n    \"\"\"Check if this is a temporal type.\"\"\"\n    return self.kind in {\n        TypeKind.DATE,\n        TypeKind.DATETIME,\n        TypeKind.TIME,\n        TypeKind.DURATION,\n    }\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType.to_canonical_string","title":"to_canonical_string","text":"<pre><code>to_canonical_string() -&gt; str\n</code></pre> <p>Convert to canonical string representation.</p> <p>This is the format used in metadata files for human readability and consistent hashing across engines.</p> <p>Returns:</p> Type Description <code>str</code> <p>Canonical type string (e.g., \"int64\", \"datetime[UTC]\")</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def to_canonical_string(self) -&gt; str:\n    \"\"\"\n    Convert to canonical string representation.\n\n    This is the format used in metadata files for human readability\n    and consistent hashing across engines.\n\n    Returns:\n        Canonical type string (e.g., \"int64\", \"datetime[UTC]\")\n    \"\"\"\n    base = self.kind.value\n\n    # Add timezone for datetime\n    if self.kind == TypeKind.DATETIME and self.timezone:\n        return f\"{base}[{self.timezone}]\"\n\n    # Add precision/scale for decimal\n    if self.kind == TypeKind.DECIMAL:\n        if self.precision is not None and self.scale is not None:\n            return f\"{base}[{self.precision},{self.scale}]\"\n        elif self.precision is not None:\n            return f\"{base}[{self.precision}]\"\n\n    return base\n</code></pre>"},{"location":"api/types/#mlforge.types.DataType.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; dict[str, Any]\n</code></pre> <p>Serialize to JSON-compatible dict.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary suitable for JSON serialization</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def to_json(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize to JSON-compatible dict.\n\n    Returns:\n        Dictionary suitable for JSON serialization\n    \"\"\"\n    result: dict[str, Any] = {\"kind\": self.kind.value}\n\n    if not self.nullable:\n        result[\"nullable\"] = False\n\n    if self.timezone is not None:\n        result[\"timezone\"] = self.timezone\n\n    if self.precision is not None:\n        result[\"precision\"] = self.precision\n\n    if self.scale is not None:\n        result[\"scale\"] = self.scale\n\n    return result\n</code></pre>"},{"location":"api/types/#typekind","title":"TypeKind","text":""},{"location":"api/types/#mlforge.types.TypeKind","title":"mlforge.types.TypeKind","text":"<p>               Bases: <code>Enum</code></p> <p>Canonical type kinds for mlforge.</p> <p>Type names are Arrow-compatible for industry standard interoperability.</p> Source code in <code>src/mlforge/types.py</code> <pre><code>class TypeKind(Enum):\n    \"\"\"\n    Canonical type kinds for mlforge.\n\n    Type names are Arrow-compatible for industry standard interoperability.\n    \"\"\"\n\n    # Integer types (signed)\n    INT8 = \"int8\"\n    INT16 = \"int16\"\n    INT32 = \"int32\"\n    INT64 = \"int64\"\n\n    # Integer types (unsigned)\n    UINT8 = \"uint8\"\n    UINT16 = \"uint16\"\n    UINT32 = \"uint32\"\n    UINT64 = \"uint64\"\n\n    # Floating point types\n    FLOAT32 = \"float32\"\n    FLOAT64 = \"float64\"\n\n    # String type\n    STRING = \"string\"\n\n    # Boolean type\n    BOOLEAN = \"boolean\"\n\n    # Temporal types\n    DATE = \"date\"\n    DATETIME = \"datetime\"\n    TIME = \"time\"\n    DURATION = \"duration\"\n\n    # Complex types (for future expansion)\n    DECIMAL = \"decimal\"\n    LIST = \"list\"\n    STRUCT = \"struct\"\n\n    # Fallback for unknown types\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"api/types/#functions","title":"Functions","text":""},{"location":"api/types/#from_polars","title":"from_polars","text":""},{"location":"api/types/#mlforge.types.from_polars","title":"mlforge.types.from_polars","text":"<pre><code>from_polars(dtype: Any) -&gt; DataType\n</code></pre> <p>Convert Polars dtype to canonical DataType.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>Any</code> <p>Polars DataType object (e.g., pl.Int64, pl.Utf8)</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>Canonical DataType representation</p> Example <p>from_polars(pl.Int64)  # DataType(TypeKind.INT64) from_polars(pl.Datetime(\"us\", \"UTC\"))  # DataType(TypeKind.DATETIME, timezone=\"UTC\")</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def from_polars(dtype: Any) -&gt; DataType:\n    \"\"\"\n    Convert Polars dtype to canonical DataType.\n\n    Args:\n        dtype: Polars DataType object (e.g., pl.Int64, pl.Utf8)\n\n    Returns:\n        Canonical DataType representation\n\n    Example:\n        from_polars(pl.Int64)  # DataType(TypeKind.INT64)\n        from_polars(pl.Datetime(\"us\", \"UTC\"))  # DataType(TypeKind.DATETIME, timezone=\"UTC\")\n    \"\"\"\n    # Get the dtype class name\n    dtype_str = str(dtype)\n\n    # Handle Datetime with timezone\n    if dtype_str.startswith(\"Datetime\"):\n        # Extract timezone from Datetime(time_unit='us', time_zone='UTC')\n        # or Datetime (no params)\n        try:\n            import polars as pl\n\n            if hasattr(dtype, \"time_zone\"):\n                tz = dtype.time_zone\n            elif isinstance(dtype, type) and issubclass(dtype, pl.Datetime):\n                tz = None\n            else:\n                tz = None\n            return DataType(TypeKind.DATETIME, timezone=tz)\n        except (AttributeError, ImportError):\n            return DataType(TypeKind.DATETIME)\n\n    # Handle simple type names\n    # Extract base type name (e.g., \"Int64\" from \"Int64\")\n    base_name = dtype_str.split(\"(\")[0].strip()\n\n    kind = _POLARS_TO_KIND.get(base_name, TypeKind.UNKNOWN)\n    return DataType(kind)\n</code></pre>"},{"location":"api/types/#from_duckdb","title":"from_duckdb","text":""},{"location":"api/types/#mlforge.types.from_duckdb","title":"mlforge.types.from_duckdb","text":"<pre><code>from_duckdb(dtype_str: str) -&gt; DataType\n</code></pre> <p>Convert DuckDB type string to canonical DataType.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_str</code> <code>str</code> <p>DuckDB type string (e.g., \"BIGINT\", \"VARCHAR\")</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>Canonical DataType representation</p> Example <p>from_duckdb(\"BIGINT\")  # DataType(TypeKind.INT64) from_duckdb(\"DOUBLE\")  # DataType(TypeKind.FLOAT64)</p> Source code in <code>src/mlforge/types.py</code> <pre><code>def from_duckdb(dtype_str: str) -&gt; DataType:\n    \"\"\"\n    Convert DuckDB type string to canonical DataType.\n\n    Args:\n        dtype_str: DuckDB type string (e.g., \"BIGINT\", \"VARCHAR\")\n\n    Returns:\n        Canonical DataType representation\n\n    Example:\n        from_duckdb(\"BIGINT\")  # DataType(TypeKind.INT64)\n        from_duckdb(\"DOUBLE\")  # DataType(TypeKind.FLOAT64)\n    \"\"\"\n    # Normalize to uppercase\n    dtype_upper = dtype_str.upper().strip()\n\n    # Handle parameterized types like DECIMAL(10,2) or VARCHAR(255)\n    base_type = dtype_upper.split(\"(\")[0].strip()\n\n    # Handle TIMESTAMP WITH TIME ZONE\n    if \"TIMESTAMP\" in dtype_upper and \"TIME ZONE\" in dtype_upper:\n        # Extract timezone if present (DuckDB doesn't expose it directly)\n        return DataType(TypeKind.DATETIME, timezone=\"UTC\")\n\n    # Handle DECIMAL with precision/scale\n    if base_type in (\"DECIMAL\", \"NUMERIC\") and \"(\" in dtype_upper:\n        params = dtype_upper.split(\"(\")[1].rstrip(\")\")\n        parts = params.split(\",\")\n        precision = int(parts[0].strip())\n        scale = int(parts[1].strip()) if len(parts) &gt; 1 else 0\n        return DataType(TypeKind.DECIMAL, precision=precision, scale=scale)\n\n    kind = _DUCKDB_TO_KIND.get(base_type, TypeKind.UNKNOWN)\n    return DataType(kind)\n</code></pre>"},{"location":"api/utils/","title":"Utils API","text":"<p>The utils module provides utilities for entity key generation and transformation.</p>"},{"location":"api/utils/#functions","title":"Functions","text":""},{"location":"api/utils/#mlforge.utils.surrogate_key","title":"mlforge.utils.surrogate_key","text":"<pre><code>surrogate_key(*columns: str) -&gt; pl.Expr\n</code></pre> <p>Generate a surrogate key by hashing column values.</p> <p>Concatenates column values with null handling, then produces a hash. Useful for creating stable identifiers from natural keys.</p> <p>Parameters:</p> Name Type Description Default <code>*columns</code> <code>str</code> <p>Column names to include in the hash</p> <code>()</code> <p>Returns:</p> Type Description <code>Expr</code> <p>Polars expression that produces a string hash</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no columns are provided</p> Example <p>df.with_columns(     surrogate_key(\"first_name\", \"last_name\", \"dob\").alias(\"user_id\") )</p> Source code in <code>src/mlforge/utils.py</code> <pre><code>def surrogate_key(*columns: str) -&gt; pl.Expr:\n    \"\"\"\n    Generate a surrogate key by hashing column values.\n\n    Concatenates column values with null handling, then produces a hash.\n    Useful for creating stable identifiers from natural keys.\n\n    Args:\n        *columns: Column names to include in the hash\n\n    Returns:\n        Polars expression that produces a string hash\n\n    Raises:\n        ValueError: If no columns are provided\n\n    Example:\n        df.with_columns(\n            surrogate_key(\"first_name\", \"last_name\", \"dob\").alias(\"user_id\")\n        )\n    \"\"\"\n    if not columns:\n        raise ValueError(\"surrogate_key requires at least one column\")\n\n    concat_expr = pl.concat_str(\n        [pl.col(c).cast(pl.Utf8).fill_null(\"__NULL__\") for c in columns],\n        separator=\"||\",\n    )\n\n    return concat_expr.hash().cast(pl.Utf8)\n</code></pre>"},{"location":"api/utils/#mlforge.utils.entity_key","title":"mlforge.utils.entity_key","text":"<pre><code>entity_key(*columns: str, alias: str) -&gt; EntityKeyTransform\n</code></pre> <p>Create a reusable entity key transformation function.</p> <p>Returns a function that adds a surrogate key column to a DataFrame by hashing the specified source columns. Useful for defining entity relationships and passing to get_training_data().</p> <p>Parameters:</p> Name Type Description Default <code>*columns</code> <code>str</code> <p>Source column names to hash</p> <code>()</code> <code>alias</code> <code>str</code> <p>Name for the generated surrogate key column</p> required <p>Returns:</p> Type Description <code>EntityKeyTransform</code> <p>Transform function compatible with df.pipe() and get_training_data()</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no columns provided or alias is empty</p> Example Source code in <code>src/mlforge/utils.py</code> <pre><code>def entity_key(*columns: str, alias: str) -&gt; EntityKeyTransform:\n    \"\"\"\n    Create a reusable entity key transformation function.\n\n    Returns a function that adds a surrogate key column to a DataFrame\n    by hashing the specified source columns. Useful for defining entity\n    relationships and passing to get_training_data().\n\n    Args:\n        *columns: Source column names to hash\n        alias: Name for the generated surrogate key column\n\n    Returns:\n        Transform function compatible with df.pipe() and get_training_data()\n\n    Raises:\n        ValueError: If no columns provided or alias is empty\n\n    Example:\n        # Define reusable transform\n        with_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n        # Apply to DataFrame\n        users_df = df.pipe(with_user_id)\n\n        # Use in feature retrieval\n        training_data = get_training_data(\n            features=[\"user_age\"],\n            entity_df=transactions,\n            entities=[with_user_id]\n        )\n    \"\"\"\n    if not columns:\n        raise ValueError(\"entity_key requires at least one column\")\n\n    if not alias:\n        raise ValueError(\"entity_key requires an alias\")\n\n    def transform(df: pl.DataFrame) -&gt; pl.DataFrame:\n        return df.with_columns(surrogate_key(*columns).alias(alias))\n\n    transform._entity_key_columns = columns  # type: ignore[attr-defined]\n    transform._entity_key_alias = alias  # type: ignore[attr-defined]\n\n    return transform  # type: ignore[return-value]\n</code></pre>"},{"location":"api/utils/#mlforge.utils.entity_key--define-reusable-transform","title":"Define reusable transform","text":"<p>with_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")</p>"},{"location":"api/utils/#mlforge.utils.entity_key--apply-to-dataframe","title":"Apply to DataFrame","text":"<p>users_df = df.pipe(with_user_id)</p>"},{"location":"api/utils/#mlforge.utils.entity_key--use-in-feature-retrieval","title":"Use in feature retrieval","text":"<p>training_data = get_training_data(     features=[\"user_age\"],     entity_df=transactions,     entities=[with_user_id] )</p>"},{"location":"api/utils/#protocols","title":"Protocols","text":""},{"location":"api/utils/#mlforge.utils.EntityKeyTransform","title":"mlforge.utils.EntityKeyTransform","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for entity key transformation functions.</p> <p>Defines the interface for functions created by entity_key() that add surrogate keys to DataFrames. Includes metadata attributes for column tracking.</p> <p>Attributes:</p> Name Type Description <code>_entity_key_columns</code> <code>tuple[str, ...]</code> <p>Source columns used to generate the key</p> <code>_entity_key_alias</code> <code>str</code> <p>Name of the generated key column</p> Source code in <code>src/mlforge/utils.py</code> <pre><code>class EntityKeyTransform(Protocol):\n    \"\"\"\n    Protocol for entity key transformation functions.\n\n    Defines the interface for functions created by entity_key() that\n    add surrogate keys to DataFrames. Includes metadata attributes\n    for column tracking.\n\n    Attributes:\n        _entity_key_columns: Source columns used to generate the key\n        _entity_key_alias: Name of the generated key column\n    \"\"\"\n\n    _entity_key_columns: tuple[str, ...]\n    _entity_key_alias: str\n\n    def __call__(self, df: pl.DataFrame) -&gt; pl.DataFrame: ...\n</code></pre>"},{"location":"api/validation/","title":"Validation","text":""},{"location":"api/validation/#mlforge.validation","title":"mlforge.validation","text":"<p>Validation runner for feature data quality checks.</p> <p>This module provides the infrastructure for running validators against DataFrames and collecting results. It is used by the build process to validate feature function outputs before metrics are applied.</p> Example <p>from mlforge.validation import validate_dataframe from mlforge.validators import not_null, greater_than</p> <p>validators = {     \"amount\": [not_null(), greater_than(0)],     \"user_id\": [not_null()], }</p> <p>results = validate_dataframe(df, validators) if not results.passed:     for failure in results.failures:         print(f\"{failure.column}: {failure.result.message}\")</p>"},{"location":"api/validation/#mlforge.validation.ColumnValidationResult","title":"ColumnValidationResult  <code>dataclass</code>","text":"<p>Result of running a single validator on a single column.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>str</code> <p>Name of the column that was validated</p> <code>validator_name</code> <code>str</code> <p>Name of the validator that was run</p> <code>result</code> <code>ValidationResult</code> <p>The ValidationResult from the validator</p> Source code in <code>src/mlforge/validation.py</code> <pre><code>@dataclass\nclass ColumnValidationResult:\n    \"\"\"\n    Result of running a single validator on a single column.\n\n    Attributes:\n        column: Name of the column that was validated\n        validator_name: Name of the validator that was run\n        result: The ValidationResult from the validator\n    \"\"\"\n\n    column: str\n    validator_name: str\n    result: validators_.ValidationResult\n</code></pre>"},{"location":"api/validation/#mlforge.validation.FeatureValidationResult","title":"FeatureValidationResult  <code>dataclass</code>","text":"<p>Aggregated validation results for a feature.</p> <p>Attributes:</p> Name Type Description <code>feature_name</code> <code>str</code> <p>Name of the feature that was validated</p> <code>column_results</code> <code>list[ColumnValidationResult]</code> <p>List of individual column validation results</p> Source code in <code>src/mlforge/validation.py</code> <pre><code>@dataclass\nclass FeatureValidationResult:\n    \"\"\"\n    Aggregated validation results for a feature.\n\n    Attributes:\n        feature_name: Name of the feature that was validated\n        column_results: List of individual column validation results\n    \"\"\"\n\n    feature_name: str\n    column_results: list[ColumnValidationResult] = field(default_factory=list)\n\n    @property\n    def passed(self) -&gt; bool:\n        \"\"\"Return True if all validations passed.\"\"\"\n        return all(r.result.passed for r in self.column_results)\n\n    @property\n    def failures(self) -&gt; list[ColumnValidationResult]:\n        \"\"\"Return only the validation results that failed.\"\"\"\n        return [r for r in self.column_results if not r.result.passed]\n\n    @property\n    def failure_count(self) -&gt; int:\n        \"\"\"Return the number of failed validations.\"\"\"\n        return len(self.failures)\n</code></pre>"},{"location":"api/validation/#mlforge.validation.FeatureValidationResult.failure_count","title":"failure_count  <code>property</code>","text":"<pre><code>failure_count: int\n</code></pre> <p>Return the number of failed validations.</p>"},{"location":"api/validation/#mlforge.validation.FeatureValidationResult.failures","title":"failures  <code>property</code>","text":"<pre><code>failures: list[ColumnValidationResult]\n</code></pre> <p>Return only the validation results that failed.</p>"},{"location":"api/validation/#mlforge.validation.FeatureValidationResult.passed","title":"passed  <code>property</code>","text":"<pre><code>passed: bool\n</code></pre> <p>Return True if all validations passed.</p>"},{"location":"api/validation/#mlforge.validation.validate_dataframe","title":"validate_dataframe","text":"<pre><code>validate_dataframe(\n    df: DataFrame, validators: dict[str, list[Validator]]\n) -&gt; list[ColumnValidationResult]\n</code></pre> <p>Run validators against DataFrame columns.</p> <p>Validators are only run on columns that exist in the DataFrame. Missing columns are silently skipped (dbt-style behavior).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to validate</p> required <code>validators</code> <code>dict[str, list[Validator]]</code> <p>Mapping of column names to lists of validators</p> required <p>Returns:</p> Type Description <code>list[ColumnValidationResult]</code> <p>List of ColumnValidationResult for each validator run</p> Example <p>results = validate_dataframe(     df,     {\"amount\": [not_null(), greater_than(0)]} ) for r in results:     if not r.result.passed:         print(f\"{r.column} failed {r.validator_name}: {r.result.message}\")</p> Source code in <code>src/mlforge/validation.py</code> <pre><code>def validate_dataframe(\n    df: pl.DataFrame,\n    validators: dict[str, list[validators_.Validator]],\n) -&gt; list[ColumnValidationResult]:\n    \"\"\"\n    Run validators against DataFrame columns.\n\n    Validators are only run on columns that exist in the DataFrame.\n    Missing columns are silently skipped (dbt-style behavior).\n\n    Args:\n        df: DataFrame to validate\n        validators: Mapping of column names to lists of validators\n\n    Returns:\n        List of ColumnValidationResult for each validator run\n\n    Example:\n        results = validate_dataframe(\n            df,\n            {\"amount\": [not_null(), greater_than(0)]}\n        )\n        for r in results:\n            if not r.result.passed:\n                print(f\"{r.column} failed {r.validator_name}: {r.result.message}\")\n    \"\"\"\n    results: list[ColumnValidationResult] = []\n\n    for column, column_validators in validators.items():\n        if column not in df.columns:\n            logger.debug(f\"Skipping validation for missing column: {column}\")\n            continue\n\n        series = df.get_column(column)\n        for validator in column_validators:\n            logger.debug(f\"Running {validator.name} on column {column}\")\n            result = validator(series)\n            results.append(\n                ColumnValidationResult(\n                    column=column,\n                    validator_name=validator.name,\n                    result=result,\n                )\n            )\n\n    return results\n</code></pre>"},{"location":"api/validation/#mlforge.validation.validate_feature","title":"validate_feature","text":"<pre><code>validate_feature(\n    feature_name: str,\n    df: DataFrame,\n    validators: dict[str, list[Validator]],\n) -&gt; FeatureValidationResult\n</code></pre> <p>Validate a feature's DataFrame and return aggregated results.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature being validated</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame output from feature function</p> required <code>validators</code> <code>dict[str, list[Validator]]</code> <p>Mapping of column names to lists of validators</p> required <p>Returns:</p> Type Description <code>FeatureValidationResult</code> <p>FeatureValidationResult with all validation outcomes</p> Example <p>result = validate_feature(     \"user_spend\",     df,     {\"amount\": [not_null()]} ) if not result.passed:     print(f\"Feature {result.feature_name} failed validation\")</p> Source code in <code>src/mlforge/validation.py</code> <pre><code>def validate_feature(\n    feature_name: str,\n    df: pl.DataFrame,\n    validators: dict[str, list[validators_.Validator]],\n) -&gt; FeatureValidationResult:\n    \"\"\"\n    Validate a feature's DataFrame and return aggregated results.\n\n    Args:\n        feature_name: Name of the feature being validated\n        df: DataFrame output from feature function\n        validators: Mapping of column names to lists of validators\n\n    Returns:\n        FeatureValidationResult with all validation outcomes\n\n    Example:\n        result = validate_feature(\n            \"user_spend\",\n            df,\n            {\"amount\": [not_null()]}\n        )\n        if not result.passed:\n            print(f\"Feature {result.feature_name} failed validation\")\n    \"\"\"\n    column_results = validate_dataframe(df, validators)\n    return FeatureValidationResult(\n        feature_name=feature_name,\n        column_results=column_results,\n    )\n</code></pre>"},{"location":"api/validators/","title":"Validators","text":""},{"location":"api/validators/#mlforge.validators","title":"mlforge.validators","text":"<p>Validators for feature data quality checks.</p> <p>Validators are functions that check column values and return ValidationResult indicating whether the check passed or failed. They run on the output of feature functions before metrics are applied.</p> Example <p>@feature(     source=\"data/transactions.parquet\",     keys=[\"user_id\"],     validators={         \"amount\": [not_null(), greater_than(0)],         \"user_id\": [not_null()],     }, ) def user_transactions(df): ...</p>"},{"location":"api/validators/#mlforge.validators.ValidationResult","title":"ValidationResult  <code>dataclass</code>","text":"<p>Result of running a validator on a column.</p> <p>Attributes:</p> Name Type Description <code>passed</code> <code>bool</code> <p>Whether the validation check passed</p> <code>message</code> <code>str | None</code> <p>Human-readable description of failure (None if passed)</p> <code>failed_count</code> <code>int | None</code> <p>Number of rows that failed validation (None if passed)</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>@dataclass\nclass ValidationResult:\n    \"\"\"\n    Result of running a validator on a column.\n\n    Attributes:\n        passed: Whether the validation check passed\n        message: Human-readable description of failure (None if passed)\n        failed_count: Number of rows that failed validation (None if passed)\n    \"\"\"\n\n    passed: bool\n    message: str | None = None\n    failed_count: int | None = None\n</code></pre>"},{"location":"api/validators/#mlforge.validators.Validator","title":"Validator  <code>dataclass</code>","text":"<p>Container for a validator function with metadata.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Display name for the validator (used in error messages)</p> <code>fn</code> <code>ValidatorFunc</code> <p>The validation function that checks a Series</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>@dataclass\nclass Validator:\n    \"\"\"\n    Container for a validator function with metadata.\n\n    Attributes:\n        name: Display name for the validator (used in error messages)\n        fn: The validation function that checks a Series\n    \"\"\"\n\n    name: str\n    fn: ValidatorFunc\n\n    def __call__(self, series: pl.Series) -&gt; ValidationResult:\n        \"\"\"Execute the validator on a Series.\"\"\"\n        return self.fn(series)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.Validator.__call__","title":"__call__","text":"<pre><code>__call__(series: Series) -&gt; ValidationResult\n</code></pre> <p>Execute the validator on a Series.</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def __call__(self, series: pl.Series) -&gt; ValidationResult:\n    \"\"\"Execute the validator on a Series.\"\"\"\n    return self.fn(series)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.greater_than","title":"greater_than","text":"<pre><code>greater_than(value: float) -&gt; Validator\n</code></pre> <p>Validate that all values are strictly greater than a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The threshold value (exclusive lower bound)</p> required <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any values are &lt;= threshold</p> Example <p>validators={\"amount\": [greater_than(0)]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def greater_than(value: float) -&gt; Validator:\n    \"\"\"\n    Validate that all values are strictly greater than a threshold.\n\n    Args:\n        value: The threshold value (exclusive lower bound)\n\n    Returns:\n        Validator that fails if any values are &lt;= threshold\n\n    Example:\n        validators={\"amount\": [greater_than(0)]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        failed_count = int((series &lt;= value).sum())\n        if failed_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{failed_count:,} values &lt;= {value}\",\n                failed_count=failed_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"greater_than({value})\", fn=validate)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.greater_than_or_equal","title":"greater_than_or_equal","text":"<pre><code>greater_than_or_equal(value: float) -&gt; Validator\n</code></pre> <p>Validate that all values are greater than or equal to a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The threshold value (inclusive lower bound)</p> required <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any values are &lt; threshold</p> Example <p>validators={\"quantity\": [greater_than_or_equal(0)]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def greater_than_or_equal(value: float) -&gt; Validator:\n    \"\"\"\n    Validate that all values are greater than or equal to a threshold.\n\n    Args:\n        value: The threshold value (inclusive lower bound)\n\n    Returns:\n        Validator that fails if any values are &lt; threshold\n\n    Example:\n        validators={\"quantity\": [greater_than_or_equal(0)]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        failed_count = int((series &lt; value).sum())\n        if failed_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{failed_count:,} values &lt; {value}\",\n                failed_count=failed_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"greater_than_or_equal({value})\", fn=validate)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.in_range","title":"in_range","text":"<pre><code>in_range(\n    min_value: float,\n    max_value: float,\n    inclusive: bool = True,\n) -&gt; Validator\n</code></pre> <p>Validate that all values fall within a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>Lower bound of the range</p> required <code>max_value</code> <code>float</code> <p>Upper bound of the range</p> required <code>inclusive</code> <code>bool</code> <p>If True, bounds are inclusive. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any values are outside the range</p> Example <p>validators={     \"age\": [in_range(0, 120)],     \"score\": [in_range(0, 1, inclusive=True)], }</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def in_range(\n    min_value: float,\n    max_value: float,\n    inclusive: bool = True,\n) -&gt; Validator:\n    \"\"\"\n    Validate that all values fall within a specified range.\n\n    Args:\n        min_value: Lower bound of the range\n        max_value: Upper bound of the range\n        inclusive: If True, bounds are inclusive. Defaults to True.\n\n    Returns:\n        Validator that fails if any values are outside the range\n\n    Example:\n        validators={\n            \"age\": [in_range(0, 120)],\n            \"score\": [in_range(0, 1, inclusive=True)],\n        }\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        if inclusive:\n            outside = (series &lt; min_value) | (series &gt; max_value)\n        else:\n            outside = (series &lt;= min_value) | (series &gt;= max_value)\n\n        failed_count = int(outside.sum())\n        if failed_count &gt; 0:\n            bound_type = \"inclusive\" if inclusive else \"exclusive\"\n            return ValidationResult(\n                passed=False,\n                message=f\"{failed_count:,} values outside [{min_value}, {max_value}] ({bound_type})\",\n                failed_count=failed_count,\n            )\n        return ValidationResult(passed=True)\n\n    bound_str = \"[]\" if inclusive else \"()\"\n    return Validator(\n        name=f\"in_range{bound_str[0]}{min_value}, {max_value}{bound_str[1]}\",\n        fn=validate,\n    )\n</code></pre>"},{"location":"api/validators/#mlforge.validators.is_in","title":"is_in","text":"<pre><code>is_in(allowed_values: list) -&gt; Validator\n</code></pre> <p>Validate that all values are in a set of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_values</code> <code>list</code> <p>List of valid values</p> required <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any values are not in the allowed set</p> Example <p>validators={\"status\": [is_in([\"pending\", \"approved\", \"rejected\"])]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def is_in(allowed_values: list) -&gt; Validator:\n    \"\"\"\n    Validate that all values are in a set of allowed values.\n\n    Args:\n        allowed_values: List of valid values\n\n    Returns:\n        Validator that fails if any values are not in the allowed set\n\n    Example:\n        validators={\"status\": [is_in([\"pending\", \"approved\", \"rejected\"])]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        not_in_set = ~series.is_in(allowed_values)\n        failed_count = int(not_in_set.sum())\n        if failed_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{failed_count:,} values not in allowed set\",\n                failed_count=failed_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"is_in({allowed_values})\", fn=validate)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.less_than","title":"less_than","text":"<pre><code>less_than(value: float) -&gt; Validator\n</code></pre> <p>Validate that all values are strictly less than a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The threshold value (exclusive upper bound)</p> required <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any values are &gt;= threshold</p> Example <p>validators={\"discount_rate\": [less_than(1.0)]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def less_than(value: float) -&gt; Validator:\n    \"\"\"\n    Validate that all values are strictly less than a threshold.\n\n    Args:\n        value: The threshold value (exclusive upper bound)\n\n    Returns:\n        Validator that fails if any values are &gt;= threshold\n\n    Example:\n        validators={\"discount_rate\": [less_than(1.0)]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        failed_count = int((series &gt;= value).sum())\n        if failed_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{failed_count:,} values &gt;= {value}\",\n                failed_count=failed_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"less_than({value})\", fn=validate)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.less_than_or_equal","title":"less_than_or_equal","text":"<pre><code>less_than_or_equal(value: float) -&gt; Validator\n</code></pre> <p>Validate that all values are less than or equal to a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The threshold value (inclusive upper bound)</p> required <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any values are &gt; threshold</p> Example <p>validators={\"percentage\": [less_than_or_equal(100)]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def less_than_or_equal(value: float) -&gt; Validator:\n    \"\"\"\n    Validate that all values are less than or equal to a threshold.\n\n    Args:\n        value: The threshold value (inclusive upper bound)\n\n    Returns:\n        Validator that fails if any values are &gt; threshold\n\n    Example:\n        validators={\"percentage\": [less_than_or_equal(100)]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        failed_count = int((series &gt; value).sum())\n        if failed_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{failed_count:,} values &gt; {value}\",\n                failed_count=failed_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"less_than_or_equal({value})\", fn=validate)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.matches_regex","title":"matches_regex","text":"<pre><code>matches_regex(pattern: str) -&gt; Validator\n</code></pre> <p>Validate that all string values match a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression pattern to match</p> required <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any values don't match the pattern</p> Example <p>validators={\"email\": [matches_regex(r\"^[\\w.-]+@[\\w.-]+.\\w+$\")]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def matches_regex(pattern: str) -&gt; Validator:\n    \"\"\"\n    Validate that all string values match a regex pattern.\n\n    Args:\n        pattern: Regular expression pattern to match\n\n    Returns:\n        Validator that fails if any values don't match the pattern\n\n    Example:\n        validators={\"email\": [matches_regex(r\"^[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+$\")]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        non_matching = ~series.str.contains(pattern)\n        failed_count = int(non_matching.sum())\n        if failed_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{failed_count:,} values don't match pattern '{pattern}'\",\n                failed_count=failed_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"matches_regex('{pattern}')\", fn=validate)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.not_null","title":"not_null","text":"<pre><code>not_null() -&gt; Validator\n</code></pre> <p>Validate that a column contains no null values.</p> <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any null values are found</p> Example <p>validators={\"user_id\": [not_null()]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def not_null() -&gt; Validator:\n    \"\"\"\n    Validate that a column contains no null values.\n\n    Returns:\n        Validator that fails if any null values are found\n\n    Example:\n        validators={\"user_id\": [not_null()]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        null_count = series.null_count()\n        if null_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{null_count:,} null values found\",\n                failed_count=null_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=\"not_null\", fn=validate)\n</code></pre>"},{"location":"api/validators/#mlforge.validators.unique","title":"unique","text":"<pre><code>unique() -&gt; Validator\n</code></pre> <p>Validate that all values in a column are unique (no duplicates).</p> <p>Returns:</p> Type Description <code>Validator</code> <p>Validator that fails if any duplicate values are found</p> Example <p>validators={\"transaction_id\": [unique()]}</p> Source code in <code>src/mlforge/validators.py</code> <pre><code>def unique() -&gt; Validator:\n    \"\"\"\n    Validate that all values in a column are unique (no duplicates).\n\n    Returns:\n        Validator that fails if any duplicate values are found\n\n    Example:\n        validators={\"transaction_id\": [unique()]}\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        total_count = series.len()\n        unique_count = series.n_unique()\n        duplicate_count = total_count - unique_count\n        if duplicate_count &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{duplicate_count:,} duplicate values found\",\n                failed_count=duplicate_count,\n            )\n        return ValidationResult(passed=True)\n\n    return Validator(name=\"unique\", fn=validate)\n</code></pre>"},{"location":"api/version/","title":"Version API","text":"<p>The version module provides types and functions for semantic versioning, change detection, and Git integration.</p>"},{"location":"api/version/#enums","title":"Enums","text":""},{"location":"api/version/#mlforge.version.ChangeType","title":"mlforge.version.ChangeType","text":"<p>               Bases: <code>Enum</code></p> <p>Type of change detected between feature versions.</p> <p>Used to determine semantic version bumps: - INITIAL: First build \u2192 1.0.0 - MAJOR: Breaking change \u2192 X+1.0.0 - MINOR: Additive change \u2192 X.Y+1.0 - PATCH: Data refresh \u2192 X.Y.Z+1</p> Source code in <code>src/mlforge/version.py</code> <pre><code>class ChangeType(Enum):\n    \"\"\"\n    Type of change detected between feature versions.\n\n    Used to determine semantic version bumps:\n    - INITIAL: First build \u2192 1.0.0\n    - MAJOR: Breaking change \u2192 X+1.0.0\n    - MINOR: Additive change \u2192 X.Y+1.0\n    - PATCH: Data refresh \u2192 X.Y.Z+1\n    \"\"\"\n\n    INITIAL = \"initial\"\n    MAJOR = \"major\"\n    MINOR = \"minor\"\n    PATCH = \"patch\"\n\n    def is_breaking(self) -&gt; bool:\n        \"\"\"Check if this change type represents a breaking change.\"\"\"\n        return self == ChangeType.MAJOR\n</code></pre>"},{"location":"api/version/#mlforge.version.ChangeType.is_breaking","title":"is_breaking","text":"<pre><code>is_breaking() -&gt; bool\n</code></pre> <p>Check if this change type represents a breaking change.</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def is_breaking(self) -&gt; bool:\n    \"\"\"Check if this change type represents a breaking change.\"\"\"\n    return self == ChangeType.MAJOR\n</code></pre>"},{"location":"api/version/#data-classes","title":"Data Classes","text":""},{"location":"api/version/#mlforge.version.ChangeSummary","title":"mlforge.version.ChangeSummary  <code>dataclass</code>","text":"<p>Summary of changes that triggered a version bump.</p> <p>Stored in FeatureMetadata.change_summary for auditability.</p> <p>Attributes:</p> Name Type Description <code>change_type</code> <code>ChangeType</code> <p>Type of version bump applied</p> <code>reason</code> <code>str</code> <p>Human-readable reason code</p> <code>details</code> <code>list[str]</code> <p>List of specific changes (e.g., column names added/removed)</p> Source code in <code>src/mlforge/version.py</code> <pre><code>@dataclass\nclass ChangeSummary:\n    \"\"\"\n    Summary of changes that triggered a version bump.\n\n    Stored in FeatureMetadata.change_summary for auditability.\n\n    Attributes:\n        change_type: Type of version bump applied\n        reason: Human-readable reason code\n        details: List of specific changes (e.g., column names added/removed)\n    \"\"\"\n\n    change_type: ChangeType\n    reason: str\n    details: list[str] = field(default_factory=list)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        return {\n            \"bump_type\": self.change_type.value,\n            \"reason\": self.reason,\n            \"details\": self.details,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -&gt; ChangeSummary:\n        \"\"\"Create from dictionary.\"\"\"\n        return cls(\n            change_type=ChangeType(data[\"bump_type\"]),\n            reason=data[\"reason\"],\n            details=data.get(\"details\", []),\n        )\n</code></pre>"},{"location":"api/version/#mlforge.version.ChangeSummary.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; ChangeSummary\n</code></pre> <p>Create from dictionary.</p> Source code in <code>src/mlforge/version.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; ChangeSummary:\n    \"\"\"Create from dictionary.\"\"\"\n    return cls(\n        change_type=ChangeType(data[\"bump_type\"]),\n        reason=data[\"reason\"],\n        details=data.get(\"details\", []),\n    )\n</code></pre>"},{"location":"api/version/#mlforge.version.ChangeSummary.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for JSON serialization.</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n    return {\n        \"bump_type\": self.change_type.value,\n        \"reason\": self.reason,\n        \"details\": self.details,\n    }\n</code></pre>"},{"location":"api/version/#version-parsing-and-bumping","title":"Version Parsing and Bumping","text":""},{"location":"api/version/#mlforge.version.parse_version","title":"mlforge.version.parse_version","text":"<pre><code>parse_version(version_str: str) -&gt; tuple[int, int, int]\n</code></pre> <p>Parse semantic version string to tuple.</p> <p>Parameters:</p> Name Type Description Default <code>version_str</code> <code>str</code> <p>Version string like \"1.2.3\"</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>Tuple of (major, minor, patch)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If version string is invalid</p> Example <p>parse_version(\"1.2.3\") (1, 2, 3)</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def parse_version(version_str: str) -&gt; tuple[int, int, int]:\n    \"\"\"\n    Parse semantic version string to tuple.\n\n    Args:\n        version_str: Version string like \"1.2.3\"\n\n    Returns:\n        Tuple of (major, minor, patch)\n\n    Raises:\n        ValueError: If version string is invalid\n\n    Example:\n        &gt;&gt;&gt; parse_version(\"1.2.3\")\n        (1, 2, 3)\n    \"\"\"\n    match = _VERSION_PATTERN.match(version_str)\n    if not match:\n        raise ValueError(\n            f\"Invalid version format: '{version_str}'. Expected 'X.Y.Z' (e.g., '1.0.0')\"\n        )\n    return int(match.group(1)), int(match.group(2)), int(match.group(3))\n</code></pre>"},{"location":"api/version/#mlforge.version.format_version","title":"mlforge.version.format_version","text":"<pre><code>format_version(major: int, minor: int, patch: int) -&gt; str\n</code></pre> <p>Format version tuple to string.</p> <p>Parameters:</p> Name Type Description Default <code>major</code> <code>int</code> <p>Major version number</p> required <code>minor</code> <code>int</code> <p>Minor version number</p> required <code>patch</code> <code>int</code> <p>Patch version number</p> required <p>Returns:</p> Type Description <code>str</code> <p>Version string like \"1.2.3\"</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def format_version(major: int, minor: int, patch: int) -&gt; str:\n    \"\"\"\n    Format version tuple to string.\n\n    Args:\n        major: Major version number\n        minor: Minor version number\n        patch: Patch version number\n\n    Returns:\n        Version string like \"1.2.3\"\n    \"\"\"\n    return f\"{major}.{minor}.{patch}\"\n</code></pre>"},{"location":"api/version/#mlforge.version.bump_version","title":"mlforge.version.bump_version","text":"<pre><code>bump_version(current: str, change_type: ChangeType) -&gt; str\n</code></pre> <p>Increment version by change type.</p> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>str</code> <p>Current version string (e.g., \"1.2.3\")</p> required <code>change_type</code> <code>ChangeType</code> <p>Type of version increment</p> required <p>Returns:</p> Type Description <code>str</code> <p>New version string</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If change_type is INITIAL (use \"1.0.0\" directly)</p> Example <p>bump_version(\"1.2.3\", ChangeType.MINOR) \"1.3.0\" bump_version(\"1.2.3\", ChangeType.MAJOR) \"2.0.0\"</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def bump_version(current: str, change_type: ChangeType) -&gt; str:\n    \"\"\"\n    Increment version by change type.\n\n    Args:\n        current: Current version string (e.g., \"1.2.3\")\n        change_type: Type of version increment\n\n    Returns:\n        New version string\n\n    Raises:\n        ValueError: If change_type is INITIAL (use \"1.0.0\" directly)\n\n    Example:\n        &gt;&gt;&gt; bump_version(\"1.2.3\", ChangeType.MINOR)\n        \"1.3.0\"\n        &gt;&gt;&gt; bump_version(\"1.2.3\", ChangeType.MAJOR)\n        \"2.0.0\"\n    \"\"\"\n    if change_type == ChangeType.INITIAL:\n        raise ValueError(\n            \"Cannot bump INITIAL change type. Use '1.0.0' directly.\"\n        )\n\n    major, minor, patch = parse_version(current)\n\n    match change_type:\n        case ChangeType.MAJOR:\n            return format_version(major + 1, 0, 0)\n        case ChangeType.MINOR:\n            return format_version(major, minor + 1, 0)\n        case ChangeType.PATCH:\n            return format_version(major, minor, patch + 1)\n        case _:\n            raise ValueError(f\"Unexpected change type: {change_type}\")\n</code></pre>"},{"location":"api/version/#mlforge.version.sort_versions","title":"mlforge.version.sort_versions","text":"<pre><code>sort_versions(versions: list[str]) -&gt; list[str]\n</code></pre> <p>Sort version strings semantically.</p> <p>Parameters:</p> Name Type Description Default <code>versions</code> <code>list[str]</code> <p>List of version strings</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list (oldest to newest)</p> Example <p>sort_versions([\"1.10.0\", \"1.2.0\", \"2.0.0\"]) [\"1.2.0\", \"1.10.0\", \"2.0.0\"]</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def sort_versions(versions: list[str]) -&gt; list[str]:\n    \"\"\"\n    Sort version strings semantically.\n\n    Args:\n        versions: List of version strings\n\n    Returns:\n        Sorted list (oldest to newest)\n\n    Example:\n        &gt;&gt;&gt; sort_versions([\"1.10.0\", \"1.2.0\", \"2.0.0\"])\n        [\"1.2.0\", \"1.10.0\", \"2.0.0\"]\n    \"\"\"\n    return sorted(versions, key=lambda v: parse_version(v))\n</code></pre>"},{"location":"api/version/#mlforge.version.is_valid_version","title":"mlforge.version.is_valid_version","text":"<pre><code>is_valid_version(version_str: str) -&gt; bool\n</code></pre> <p>Check if a string is a valid semantic version.</p> <p>Parameters:</p> Name Type Description Default <code>version_str</code> <code>str</code> <p>String to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid version format, False otherwise</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def is_valid_version(version_str: str) -&gt; bool:\n    \"\"\"\n    Check if a string is a valid semantic version.\n\n    Args:\n        version_str: String to validate\n\n    Returns:\n        True if valid version format, False otherwise\n    \"\"\"\n    return _VERSION_PATTERN.match(version_str) is not None\n</code></pre>"},{"location":"api/version/#path-construction","title":"Path Construction","text":""},{"location":"api/version/#mlforge.version.versioned_data_path","title":"mlforge.version.versioned_data_path","text":"<pre><code>versioned_data_path(\n    store_root: Path, feature_name: str, version: str\n) -&gt; Path\n</code></pre> <p>Get path to versioned feature data file.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>version</code> <code>str</code> <p>Semantic version string (e.g., \"1.0.0\")</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to data.parquet file</p> Example <p>versioned_data_path(Path(\"./store\"), \"user_spend\", \"1.0.0\") Path(\"./store/user_spend/1.0.0/data.parquet\")</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def versioned_data_path(\n    store_root: Path, feature_name: str, version: str\n) -&gt; Path:\n    \"\"\"\n    Get path to versioned feature data file.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n        version: Semantic version string (e.g., \"1.0.0\")\n\n    Returns:\n        Path to data.parquet file\n\n    Example:\n        &gt;&gt;&gt; versioned_data_path(Path(\"./store\"), \"user_spend\", \"1.0.0\")\n        Path(\"./store/user_spend/1.0.0/data.parquet\")\n    \"\"\"\n    return store_root / feature_name / version / \"data.parquet\"\n</code></pre>"},{"location":"api/version/#mlforge.version.versioned_metadata_path","title":"mlforge.version.versioned_metadata_path","text":"<pre><code>versioned_metadata_path(\n    store_root: Path, feature_name: str, version: str\n) -&gt; Path\n</code></pre> <p>Get path to versioned feature metadata file.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>version</code> <code>str</code> <p>Semantic version string</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to .meta.json file</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def versioned_metadata_path(\n    store_root: Path, feature_name: str, version: str\n) -&gt; Path:\n    \"\"\"\n    Get path to versioned feature metadata file.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n        version: Semantic version string\n\n    Returns:\n        Path to .meta.json file\n    \"\"\"\n    return store_root / feature_name / version / \".meta.json\"\n</code></pre>"},{"location":"api/version/#mlforge.version.latest_pointer_path","title":"mlforge.version.latest_pointer_path","text":"<pre><code>latest_pointer_path(\n    store_root: Path, feature_name: str\n) -&gt; Path\n</code></pre> <p>Get path to _latest.json pointer file.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to _latest.json file within feature directory</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def latest_pointer_path(store_root: Path, feature_name: str) -&gt; Path:\n    \"\"\"\n    Get path to _latest.json pointer file.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n\n    Returns:\n        Path to _latest.json file within feature directory\n    \"\"\"\n    return store_root / feature_name / \"_latest.json\"\n</code></pre>"},{"location":"api/version/#mlforge.version.feature_versions_dir","title":"mlforge.version.feature_versions_dir","text":"<pre><code>feature_versions_dir(\n    store_root: Path, feature_name: str\n) -&gt; Path\n</code></pre> <p>Get path to feature's version directory (parent of all versions).</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to feature directory containing version subdirectories</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def feature_versions_dir(store_root: Path, feature_name: str) -&gt; Path:\n    \"\"\"\n    Get path to feature's version directory (parent of all versions).\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n\n    Returns:\n        Path to feature directory containing version subdirectories\n    \"\"\"\n    return store_root / feature_name\n</code></pre>"},{"location":"api/version/#hash-computation","title":"Hash Computation","text":""},{"location":"api/version/#mlforge.version.compute_schema_hash","title":"mlforge.version.compute_schema_hash","text":"<pre><code>compute_schema_hash(columns: list[ColumnMetadata]) -&gt; str\n</code></pre> <p>Compute hash of column names and dtypes for schema change detection.</p> <p>Captures structural schema changes (columns added/removed, dtype changes).</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[ColumnMetadata]</code> <p>List of ColumnMetadata from feature result</p> required <p>Returns:</p> Type Description <code>str</code> <p>Hex string hash (first 12 characters of SHA256)</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def compute_schema_hash(columns: list[ColumnMetadata]) -&gt; str:\n    \"\"\"\n    Compute hash of column names and dtypes for schema change detection.\n\n    Captures structural schema changes (columns added/removed, dtype changes).\n\n    Args:\n        columns: List of ColumnMetadata from feature result\n\n    Returns:\n        Hex string hash (first 12 characters of SHA256)\n    \"\"\"\n    # Sort columns by name for consistent hashing\n    sorted_cols = sorted(columns, key=lambda c: c.name)\n\n    # Include only name and dtype (not input/agg/window which are config)\n    schema_data = [(c.name, c.dtype) for c in sorted_cols]\n\n    serialized = json.dumps(schema_data, sort_keys=True)\n    return hashlib.sha256(serialized.encode()).hexdigest()[:12]\n</code></pre>"},{"location":"api/version/#mlforge.version.compute_config_hash","title":"mlforge.version.compute_config_hash","text":"<pre><code>compute_config_hash(\n    keys: list[str],\n    timestamp: str | None,\n    interval: str | None,\n    metrics_config: list[dict[str, Any]] | None,\n) -&gt; str\n</code></pre> <p>Compute hash of feature configuration for config change detection.</p> <p>Captures configuration changes that affect computation (keys, timing, metrics).</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>Entity key columns</p> required <code>timestamp</code> <code>str | None</code> <p>Timestamp column name</p> required <code>interval</code> <code>str | None</code> <p>Rolling interval string</p> required <code>metrics_config</code> <code>list[dict[str, Any]] | None</code> <p>Serialized metrics configuration</p> required <p>Returns:</p> Type Description <code>str</code> <p>Hex string hash (first 12 characters of SHA256)</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def compute_config_hash(\n    keys: list[str],\n    timestamp: str | None,\n    interval: str | None,\n    metrics_config: list[dict[str, Any]] | None,\n) -&gt; str:\n    \"\"\"\n    Compute hash of feature configuration for config change detection.\n\n    Captures configuration changes that affect computation (keys, timing, metrics).\n\n    Args:\n        keys: Entity key columns\n        timestamp: Timestamp column name\n        interval: Rolling interval string\n        metrics_config: Serialized metrics configuration\n\n    Returns:\n        Hex string hash (first 12 characters of SHA256)\n    \"\"\"\n    config_data = {\n        \"keys\": sorted(keys),\n        \"timestamp\": timestamp,\n        \"interval\": interval,\n        \"metrics\": metrics_config or [],\n    }\n\n    serialized = json.dumps(config_data, sort_keys=True)\n    return hashlib.sha256(serialized.encode()).hexdigest()[:12]\n</code></pre>"},{"location":"api/version/#mlforge.version.compute_content_hash","title":"mlforge.version.compute_content_hash","text":"<pre><code>compute_content_hash(path: Path) -&gt; str\n</code></pre> <p>Compute hash of parquet file content for data change detection.</p> <p>Uses file-based hashing for efficiency with large files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to parquet file</p> required <p>Returns:</p> Type Description <code>str</code> <p>Hex string hash (first 12 characters of SHA256)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path doesn't exist</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def compute_content_hash(path: Path) -&gt; str:\n    \"\"\"\n    Compute hash of parquet file content for data change detection.\n\n    Uses file-based hashing for efficiency with large files.\n\n    Args:\n        path: Path to parquet file\n\n    Returns:\n        Hex string hash (first 12 characters of SHA256)\n\n    Raises:\n        FileNotFoundError: If path doesn't exist\n    \"\"\"\n    hasher = hashlib.sha256()\n\n    with open(path, \"rb\") as f:\n        # Read in chunks for memory efficiency\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            hasher.update(chunk)\n\n    return hasher.hexdigest()[:12]\n</code></pre>"},{"location":"api/version/#mlforge.version.compute_source_hash","title":"mlforge.version.compute_source_hash","text":"<pre><code>compute_source_hash(path: Path | str) -&gt; str\n</code></pre> <p>Compute hash of source data file for reproducibility verification.</p> <p>Uses file-based hashing for efficiency with large files. This hash is stored in metadata and used by <code>mlforge sync</code> to verify that teammates have the same source data before rebuilding.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to source data file (parquet, csv, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Hex string hash (first 12 characters of SHA256)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path doesn't exist</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def compute_source_hash(path: Path | str) -&gt; str:\n    \"\"\"\n    Compute hash of source data file for reproducibility verification.\n\n    Uses file-based hashing for efficiency with large files. This hash\n    is stored in metadata and used by `mlforge sync` to verify that\n    teammates have the same source data before rebuilding.\n\n    Args:\n        path: Path to source data file (parquet, csv, etc.)\n\n    Returns:\n        Hex string hash (first 12 characters of SHA256)\n\n    Raises:\n        FileNotFoundError: If path doesn't exist\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n\n    hasher = hashlib.sha256()\n\n    with open(path, \"rb\") as f:\n        # Read in chunks for memory efficiency\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            hasher.update(chunk)\n\n    return hasher.hexdigest()[:12]\n</code></pre>"},{"location":"api/version/#change-detection","title":"Change Detection","text":""},{"location":"api/version/#mlforge.version.detect_change_type","title":"mlforge.version.detect_change_type","text":"<pre><code>detect_change_type(\n    previous_columns: list[str] | None,\n    current_columns: list[str],\n    previous_schema_hash: str | None,\n    current_schema_hash: str,\n    previous_config_hash: str | None,\n    current_config_hash: str,\n) -&gt; ChangeType\n</code></pre> <p>Determine version bump type based on schema and config changes.</p> <p>Change detection logic (from roadmap): - No previous version \u2192 INITIAL (1.0.0) - Columns removed \u2192 MAJOR (breaking) - Dtype changed \u2192 MAJOR (breaking, detected via schema_hash) - Columns added \u2192 MINOR (additive) - Config changed \u2192 MINOR - Same schema/config \u2192 PATCH (data refresh)</p> <p>Parameters:</p> Name Type Description Default <code>previous_columns</code> <code>list[str] | None</code> <p>Column names from previous version (None if first build)</p> required <code>current_columns</code> <code>list[str]</code> <p>Column names from current build</p> required <code>previous_schema_hash</code> <code>str | None</code> <p>Schema hash from previous version</p> required <code>current_schema_hash</code> <code>str</code> <p>Schema hash from current build</p> required <code>previous_config_hash</code> <code>str | None</code> <p>Config hash from previous version</p> required <code>current_config_hash</code> <code>str</code> <p>Config hash from current build</p> required <p>Returns:</p> Type Description <code>ChangeType</code> <p>ChangeType indicating required version bump</p> Example <p>detect_change_type(None, [\"a\", \"b\"], None, \"abc123\", None, \"def456\") ChangeType.INITIAL detect_change_type( ...     [\"a\", \"b\", \"c\"], [\"a\", \"b\"], \"abc\", \"def\", \"123\", \"123\" ... ) ChangeType.MAJOR  # Column removed</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def detect_change_type(\n    previous_columns: list[str] | None,\n    current_columns: list[str],\n    previous_schema_hash: str | None,\n    current_schema_hash: str,\n    previous_config_hash: str | None,\n    current_config_hash: str,\n) -&gt; ChangeType:\n    \"\"\"\n    Determine version bump type based on schema and config changes.\n\n    Change detection logic (from roadmap):\n    - No previous version \u2192 INITIAL (1.0.0)\n    - Columns removed \u2192 MAJOR (breaking)\n    - Dtype changed \u2192 MAJOR (breaking, detected via schema_hash)\n    - Columns added \u2192 MINOR (additive)\n    - Config changed \u2192 MINOR\n    - Same schema/config \u2192 PATCH (data refresh)\n\n    Args:\n        previous_columns: Column names from previous version (None if first build)\n        current_columns: Column names from current build\n        previous_schema_hash: Schema hash from previous version\n        current_schema_hash: Schema hash from current build\n        previous_config_hash: Config hash from previous version\n        current_config_hash: Config hash from current build\n\n    Returns:\n        ChangeType indicating required version bump\n\n    Example:\n        &gt;&gt;&gt; detect_change_type(None, [\"a\", \"b\"], None, \"abc123\", None, \"def456\")\n        ChangeType.INITIAL\n        &gt;&gt;&gt; detect_change_type(\n        ...     [\"a\", \"b\", \"c\"], [\"a\", \"b\"], \"abc\", \"def\", \"123\", \"123\"\n        ... )\n        ChangeType.MAJOR  # Column removed\n    \"\"\"\n    # First build - no previous version\n    if previous_columns is None or previous_schema_hash is None:\n        return ChangeType.INITIAL\n\n    previous_set = set(previous_columns)\n    current_set = set(current_columns)\n\n    # Check for removed columns (breaking change)\n    removed_columns = previous_set - current_set\n    if removed_columns:\n        return ChangeType.MAJOR\n\n    # Check for schema hash change (dtype changes are breaking)\n    # Note: We already checked for removed columns, so if schema_hash differs\n    # and no columns removed, it must be dtype change or column addition\n    if previous_schema_hash != current_schema_hash:\n        # Added columns (additive change)\n        added_columns = current_set - previous_set\n        if added_columns:\n            return ChangeType.MINOR\n        # Same columns but different hash = dtype changed (breaking)\n        return ChangeType.MAJOR\n\n    # Check for config changes (interval, metrics, etc.)\n    if previous_config_hash != current_config_hash:\n        return ChangeType.MINOR\n\n    # Same schema and config = data refresh only\n    return ChangeType.PATCH\n</code></pre>"},{"location":"api/version/#mlforge.version.build_change_summary","title":"mlforge.version.build_change_summary","text":"<pre><code>build_change_summary(\n    change_type: ChangeType,\n    previous_columns: list[str] | None,\n    current_columns: list[str],\n) -&gt; ChangeSummary\n</code></pre> <p>Build structured change summary for metadata.</p> <p>Parameters:</p> Name Type Description Default <code>change_type</code> <code>ChangeType</code> <p>Detected change type</p> required <code>previous_columns</code> <code>list[str] | None</code> <p>Previous version columns</p> required <code>current_columns</code> <code>list[str]</code> <p>Current version columns</p> required <p>Returns:</p> Type Description <code>ChangeSummary</code> <p>ChangeSummary with bump_type, reason, and details</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def build_change_summary(\n    change_type: ChangeType,\n    previous_columns: list[str] | None,\n    current_columns: list[str],\n) -&gt; ChangeSummary:\n    \"\"\"\n    Build structured change summary for metadata.\n\n    Args:\n        change_type: Detected change type\n        previous_columns: Previous version columns\n        current_columns: Current version columns\n\n    Returns:\n        ChangeSummary with bump_type, reason, and details\n    \"\"\"\n    if change_type == ChangeType.INITIAL:\n        return ChangeSummary(\n            change_type=ChangeType.INITIAL,\n            reason=\"first_build\",\n            details=[],\n        )\n\n    previous_set = set(previous_columns or [])\n    current_set = set(current_columns)\n\n    removed = sorted(previous_set - current_set)\n    added = sorted(current_set - previous_set)\n\n    if removed:\n        return ChangeSummary(\n            change_type=ChangeType.MAJOR,\n            reason=\"columns_removed\",\n            details=removed,\n        )\n\n    if added:\n        return ChangeSummary(\n            change_type=ChangeType.MINOR,\n            reason=\"columns_added\",\n            details=added,\n        )\n\n    if change_type == ChangeType.MINOR:\n        return ChangeSummary(\n            change_type=ChangeType.MINOR,\n            reason=\"config_changed\",\n            details=[],\n        )\n\n    if change_type == ChangeType.MAJOR:\n        return ChangeSummary(\n            change_type=ChangeType.MAJOR,\n            reason=\"dtype_changed\",\n            details=[],\n        )\n\n    return ChangeSummary(\n        change_type=ChangeType.PATCH,\n        reason=\"data_refresh\",\n        details=[],\n    )\n</code></pre>"},{"location":"api/version/#version-discovery","title":"Version Discovery","text":""},{"location":"api/version/#mlforge.version.list_versions","title":"mlforge.version.list_versions","text":"<pre><code>list_versions(\n    store_root: Path, feature_name: str\n) -&gt; list[str]\n</code></pre> <p>List all versions of a feature, sorted semantically.</p> <p>Scans the feature directory for version subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of version strings (oldest to newest), empty if none</p> Example <p>list_versions(Path(\"./store\"), \"user_spend\") [\"1.0.0\", \"1.0.1\", \"1.1.0\"]</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def list_versions(store_root: Path, feature_name: str) -&gt; list[str]:\n    \"\"\"\n    List all versions of a feature, sorted semantically.\n\n    Scans the feature directory for version subdirectories.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n\n    Returns:\n        Sorted list of version strings (oldest to newest), empty if none\n\n    Example:\n        &gt;&gt;&gt; list_versions(Path(\"./store\"), \"user_spend\")\n        [\"1.0.0\", \"1.0.1\", \"1.1.0\"]\n    \"\"\"\n    feature_dir = feature_versions_dir(store_root, feature_name)\n\n    if not feature_dir.exists():\n        return []\n\n    versions = []\n    for path in feature_dir.iterdir():\n        if path.is_dir() and is_valid_version(path.name):\n            versions.append(path.name)\n\n    return sort_versions(versions)\n</code></pre>"},{"location":"api/version/#mlforge.version.get_latest_version","title":"mlforge.version.get_latest_version","text":"<pre><code>get_latest_version(\n    store_root: Path, feature_name: str\n) -&gt; str | None\n</code></pre> <p>Get latest version from _latest.json pointer.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Latest version string, or None if no versions exist</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def get_latest_version(store_root: Path, feature_name: str) -&gt; str | None:\n    \"\"\"\n    Get latest version from _latest.json pointer.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n\n    Returns:\n        Latest version string, or None if no versions exist\n    \"\"\"\n    pointer_path = latest_pointer_path(store_root, feature_name)\n\n    if not pointer_path.exists():\n        return None\n\n    with open(pointer_path) as f:\n        data = json.load(f)\n\n    return data.get(\"version\")\n</code></pre>"},{"location":"api/version/#mlforge.version.write_latest_pointer","title":"mlforge.version.write_latest_pointer","text":"<pre><code>write_latest_pointer(\n    store_root: Path, feature_name: str, version: str\n) -&gt; None\n</code></pre> <p>Write _latest.json pointer file.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>version</code> <code>str</code> <p>Version to mark as latest</p> required Source code in <code>src/mlforge/version.py</code> <pre><code>def write_latest_pointer(\n    store_root: Path, feature_name: str, version: str\n) -&gt; None:\n    \"\"\"\n    Write _latest.json pointer file.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n        version: Version to mark as latest\n    \"\"\"\n    pointer_path = latest_pointer_path(store_root, feature_name)\n    pointer_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(pointer_path, \"w\") as f:\n        json.dump({\"version\": version}, f, indent=2)\n</code></pre>"},{"location":"api/version/#mlforge.version.resolve_version","title":"mlforge.version.resolve_version","text":"<pre><code>resolve_version(\n    store_root: Path, feature_name: str, version: str | None\n) -&gt; str | None\n</code></pre> <p>Resolve version string to actual version.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <code>version</code> <code>str | None</code> <p>Explicit version or None for latest</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Resolved version string, or None if feature doesn't exist</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def resolve_version(\n    store_root: Path,\n    feature_name: str,\n    version: str | None,\n) -&gt; str | None:\n    \"\"\"\n    Resolve version string to actual version.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n        version: Explicit version or None for latest\n\n    Returns:\n        Resolved version string, or None if feature doesn't exist\n    \"\"\"\n    if version is not None:\n        return version\n    return get_latest_version(store_root, feature_name)\n</code></pre>"},{"location":"api/version/#git-integration","title":"Git Integration","text":""},{"location":"api/version/#mlforge.version.write_feature_gitignore","title":"mlforge.version.write_feature_gitignore","text":"<pre><code>write_feature_gitignore(\n    store_root: Path, feature_name: str\n) -&gt; bool\n</code></pre> <p>Write .gitignore to feature directory if not already present.</p> <p>Creates a .gitignore file that ignores data.parquet files in version subdirectories. This allows committing metadata (.meta.json, _latest.json) while excluding large data files that can be rebuilt from source.</p> <p>Parameters:</p> Name Type Description Default <code>store_root</code> <code>Path</code> <p>Root path of the feature store</p> required <code>feature_name</code> <code>str</code> <p>Name of the feature</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if .gitignore was created, False if it already existed</p> Source code in <code>src/mlforge/version.py</code> <pre><code>def write_feature_gitignore(store_root: Path, feature_name: str) -&gt; bool:\n    \"\"\"\n    Write .gitignore to feature directory if not already present.\n\n    Creates a .gitignore file that ignores data.parquet files in version\n    subdirectories. This allows committing metadata (.meta.json, _latest.json)\n    while excluding large data files that can be rebuilt from source.\n\n    Args:\n        store_root: Root path of the feature store\n        feature_name: Name of the feature\n\n    Returns:\n        True if .gitignore was created, False if it already existed\n    \"\"\"\n    feature_dir = feature_versions_dir(store_root, feature_name)\n    gitignore_path = feature_dir / \".gitignore\"\n\n    if gitignore_path.exists():\n        return False\n\n    feature_dir.mkdir(parents=True, exist_ok=True)\n    gitignore_path.write_text(_GITIGNORE_CONTENT)\n    return True\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>mlforge requires Python 3.13 or later.</p>"},{"location":"getting-started/installation/#install-via-pip","title":"Install via pip","text":"<pre><code>pip install mlforge-sdk\n</code></pre>"},{"location":"getting-started/installation/#install-via-uv","title":"Install via uv","text":"<pre><code>uv add mlforge-sdk\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>mlforge has the following core dependencies:</p> <ul> <li>polars (&gt;=1.35.2) - DataFrame library for data transformations</li> <li>pyarrow (&gt;=22.0.0) - Parquet file support</li> <li>pydantic (&gt;=2.12.4) - Data validation</li> <li>cyclopts (&gt;=4.2.1) - CLI framework</li> <li>loguru (&gt;=0.7.3) - Logging</li> </ul> <p>These will be installed automatically when you install mlforge.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that mlforge is installed correctly:</p> <pre><code>mlforge --help\n</code></pre> <p>You should see the CLI help output:</p> <pre><code>Usage: mlforge [OPTIONS] COMMAND\n\nA simple feature store SDK\n\nCommands:\n  build  Materialize features to offline storage\n  list   Display all registered features in a table\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to mlforge or run it from source:</p> <pre><code># Clone the repository\ngit clone https://github.com/chonalchendo/mlforge.git\ncd mlforge\n\n# Install with uv (recommended)\nuv sync\n\n# Or install with pip\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Continue to the Quickstart Guide to build your first feature.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This guide will walk you through creating your first feature store in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#setup","title":"Setup","text":"<p>First, install mlforge:</p> pipuv <pre><code>pip install mlforge-sdk\n</code></pre> <pre><code>uv add mlforge-sdk\n</code></pre>"},{"location":"getting-started/quickstart/#1-prepare-your-data","title":"1. Prepare Your Data","text":"<p>Create a simple dataset. For this example, we'll use transaction data:</p> <pre><code>import polars as pl\nfrom pathlib import Path\n\n# Create sample transaction data\ntransactions = pl.DataFrame({\n    \"user_id\": [\"u1\", \"u1\", \"u2\", \"u2\", \"u3\"],\n    \"amount\": [10.0, 25.0, 50.0, 15.0, 100.0],\n    \"transaction_time\": [\n        \"2024-01-01 10:00:00\",\n        \"2024-01-02 11:00:00\",\n        \"2024-01-01 12:00:00\",\n        \"2024-01-03 09:00:00\",\n        \"2024-01-01 15:00:00\",\n    ]\n})\n\n# Save to parquet\nPath(\"data\").mkdir(exist_ok=True)\ntransactions.write_parquet(\"data/transactions.parquet\")\n</code></pre>"},{"location":"getting-started/quickstart/#2-define-features","title":"2. Define Features","text":"<p>Create a file called <code>features.py</code>:</p> <pre><code>import mlforge as mlf\nimport polars as pl\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    tags=[\"spending\"],\n    description=\"Total spend per user\"\n)\ndef user_total_spend(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.group_by(\"user_id\").agg(\n        pl.col(\"amount\").sum().alias(\"total_spend\")\n    )\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    tags=[\"spending\"],\n    description=\"Average transaction amount per user\"\n)\ndef user_avg_transaction(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.group_by(\"user_id\").agg(\n        pl.col(\"amount\").mean().alias(\"avg_transaction\")\n    )\n</code></pre>"},{"location":"getting-started/quickstart/#3-create-feature-definitions","title":"3. Create Feature Definitions","text":"<p>Create <code>definitions.py</code>:</p> <pre><code>import mlforge as mlf\nimport features\n\ndefs = mlf.Definitions(\n    name=\"quickstart\",\n    features=[features],\n    offline_store=mlf.LocalStore(\"./feature_store\")\n)\n</code></pre>"},{"location":"getting-started/quickstart/#4-build-features","title":"4. Build Features","text":"<p>Materialize your features using the CLI:</p> <pre><code>mlforge build\n</code></pre> <p>You should see output like:</p> <pre><code>Materializing user_total_spend\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 total_spend \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 35.0        \u2502\n\u2502 u2      \u2502 65.0        \u2502\n\u2502 u3      \u2502 100.0       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nMaterializing user_avg_transaction\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 avg_transaction \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 17.5            \u2502\n\u2502 u2      \u2502 32.5            \u2502\n\u2502 u3      \u2502 100.0           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nBuilt 2 features\n</code></pre>"},{"location":"getting-started/quickstart/#5-retrieve-features","title":"5. Retrieve Features","text":"<p>Use features in your training pipeline:</p> <pre><code>import mlforge as mlf\nimport polars as pl\n\n# Load entity data (e.g., labels for training)\nentities = pl.DataFrame({\n    \"user_id\": [\"u1\", \"u2\", \"u3\"],\n    \"label\": [0, 1, 0]\n})\n\n# Get training data with features\ntraining_data = mlf.get_training_data(\n    features=[\"user_total_spend\", \"user_avg_transaction\"],\n    entity_df=entities\n)\n\nprint(training_data)\n</code></pre> <p>Output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 label \u2502 total_spend \u2502 avg_transaction \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 0     \u2502 35.0        \u2502 17.5            \u2502\n\u2502 u2      \u2502 1     \u2502 65.0        \u2502 32.5            \u2502\n\u2502 u3      \u2502 0     \u2502 100.0       \u2502 100.0           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/quickstart/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>You defined features using the <code>@mlf.feature</code> decorator</li> <li>Registered them with a <code>mlf.Definitions</code> object</li> <li>Materialized them to local Parquet storage with <code>mlforge build</code></li> <li>Retrieved them for training using <code>mlf.get_training_data()</code></li> </ol>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about defining features</li> <li>Explore entity keys for complex joins</li> <li>Understand point-in-time correctness</li> <li>Browse the CLI reference</li> </ul>"},{"location":"user-guide/building-features/","title":"Building Features","text":"<p>Once you've defined features, the next step is to build them and persist to storage. This guide covers building features using both the CLI and the Python API.</p>"},{"location":"user-guide/building-features/#using-the-cli","title":"Using the CLI","text":"<p>The recommended way to build features is via the <code>mlforge build</code> command.</p>"},{"location":"user-guide/building-features/#basic-usage","title":"Basic Usage","text":"<pre><code>mlforge build\n</code></pre> <p>This will:</p> <ol> <li>Load your <code>Definitions</code> object from <code>definitions.py</code></li> <li>Materialize all registered features</li> <li>Write them to your configured offline store</li> <li>Display a preview of each feature</li> </ol>"},{"location":"user-guide/building-features/#build-specific-features","title":"Build Specific Features","text":"<p>Build only selected features by name:</p> <pre><code>mlforge build --features user_total_spend,user_avg_spend\n</code></pre> <p>Or build features by tag:</p> <pre><code>mlforge build --tags user_metrics,demographics\n</code></pre> <p>Mutually exclusive filters</p> <p>The <code>--features</code> and <code>--tags</code> options cannot be used together. Choose one filtering approach per build command.</p>"},{"location":"user-guide/building-features/#force-rebuild","title":"Force Rebuild","text":"<p>By default, mlforge skips features that already exist. Use <code>--force</code> to rebuild:</p> <pre><code>mlforge build --force\n</code></pre>"},{"location":"user-guide/building-features/#disable-preview","title":"Disable Preview","text":"<p>Turn off the data preview:</p> <pre><code>mlforge build --no-preview\n</code></pre>"},{"location":"user-guide/building-features/#control-preview-size","title":"Control Preview Size","text":"<p>Adjust the number of rows shown:</p> <pre><code>mlforge build --preview-rows 10\n</code></pre>"},{"location":"user-guide/building-features/#verbose-logging","title":"Verbose Logging","text":"<p>Enable debug logging:</p> <pre><code>mlforge build --verbose\n</code></pre>"},{"location":"user-guide/building-features/#versioning","title":"Versioning","text":"<p>Starting in v0.5.0, mlforge automatically versions your features using semantic versioning.</p>"},{"location":"user-guide/building-features/#automatic-version-detection","title":"Automatic Version Detection","text":"<p>When you run <code>mlforge build</code>, mlforge:</p> <ol> <li>Checks if the feature already exists</li> <li>Compares the new build against the latest version</li> <li>Detects what changed (schema, config, or data)</li> <li>Automatically bumps the version according to semantic versioning rules</li> </ol> <p>Version bump logic:</p> <ul> <li>MAJOR (2.0.0): Breaking changes<ul> <li>Columns removed</li> <li>Data types changed</li> </ul> </li> <li>MINOR (1.1.0): Additive changes<ul> <li>Columns added</li> <li>Configuration changed (interval, metrics, etc.)</li> </ul> </li> <li>PATCH (1.0.1): Non-breaking changes<ul> <li>Data refresh (same schema and config)</li> </ul> </li> </ul>"},{"location":"user-guide/building-features/#override-automatic-versioning","title":"Override Automatic Versioning","text":"<p>Specify an explicit version using the <code>--version</code> flag:</p> <pre><code>mlforge build --version 2.0.0\n</code></pre> <p>Or in Python:</p> <pre><code>defs.build(version=\"2.0.0\")\n</code></pre> <p>Warning</p> <p>Overriding the version skips change detection. Only use this when you need precise control over versioning.</p>"},{"location":"user-guide/building-features/#version-examples","title":"Version Examples","text":"<p>First build - Creates version 1.0.0:</p> <pre><code>mlforge build --features user_spend\n# \u2192 Created user_spend v1.0.0\n</code></pre> <p>Data refresh - Same schema/config, new data \u2192 PATCH bump:</p> <pre><code>mlforge build --features user_spend --force\n# \u2192 Created user_spend v1.0.1\n</code></pre> <p>Add column - Additive change \u2192 MINOR bump:</p> <pre><code># Add a new column to the feature output\n@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef user_spend(df):\n    return df.select([\"user_id\", \"total_spend\", \"avg_spend\"])  # Added avg_spend\n\ndefs.build()\n# \u2192 Created user_spend v1.1.0\n</code></pre> <p>Remove column - Breaking change \u2192 MAJOR bump:</p> <pre><code># Remove a column from feature output\n@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef user_spend(df):\n    return df.select([\"user_id\"])  # Removed total_spend\n\ndefs.build()\n# \u2192 Created user_spend v2.0.0\n</code></pre>"},{"location":"user-guide/building-features/#team-collaboration-via-git","title":"Team Collaboration via Git","text":"<p>mlforge enables teams to collaborate on feature definitions via Git while keeping large data files out of version control.</p>"},{"location":"user-guide/building-features/#how-it-works","title":"How It Works","text":"<ol> <li>Metadata is committed: <code>.meta.json</code> and <code>_latest.json</code> files</li> <li>Data is ignored: <code>data.parquet</code> files are excluded via auto-generated <code>.gitignore</code></li> <li>Teammates rebuild locally: Run <code>mlforge sync</code> to recreate data from metadata</li> </ol>"},{"location":"user-guide/building-features/#workflow","title":"Workflow","text":"<p>Developer 1 - Creates a new feature:</p> <pre><code># Define and build feature\nmlforge build --features user_spend\n\n# Commit metadata to Git\ngit add feature_store/user_spend/\ngit commit -m \"feat: add user_spend feature v1.0.0\"\ngit push\n</code></pre> <p>Developer 2 - Pulls changes and rebuilds data:</p> <pre><code># Pull latest changes\ngit pull\n\n# Rebuild features from metadata\nmlforge sync\n\n# user_spend v1.0.0 is now available locally\n</code></pre>"},{"location":"user-guide/building-features/#git-ignore","title":"Git Ignore","text":"<p>mlforge automatically creates <code>.gitignore</code> files in each feature directory:</p> <pre><code># Auto-generated by mlforge\n# Data files are rebuilt from source; commit .meta.json and _latest.json only\n*/data.parquet\n</code></pre> <p>This ensures: - Metadata files are committed (<code>.meta.json</code>, <code>_latest.json</code>) - Data files are ignored (<code>data.parquet</code>) - Feature definitions can be shared via Git - Data can be rebuilt from source using <code>mlforge sync</code></p>"},{"location":"user-guide/building-features/#sync-command","title":"Sync Command","text":"<p>The <code>mlforge sync</code> command rebuilds features from metadata:</p> <pre><code># Preview what would be synced\nmlforge sync --dry-run\n\n# Sync all features with missing data\nmlforge sync\n\n# Sync specific features\nmlforge sync --features user_spend,merchant_spend\n\n# Force sync even if source data changed\nmlforge sync --force\n</code></pre> <p>See the CLI Reference for complete sync command documentation.</p> <p>LocalStore Only</p> <p>The sync command only works with <code>LocalStore</code>. Cloud stores (S3Store) already share data between teammates, so syncing is not needed.</p>"},{"location":"user-guide/building-features/#using-the-python-api","title":"Using the Python API","text":"<p>You can also build features programmatically:</p> <pre><code>import mlforge as mlf\nimport features\n\ndefs = mlf.Definitions(\n    name=\"my-project\",\n    features=[features],\n    offline_store=mlf.LocalStore(\"./feature_store\")\n)\n\n# Build all features\ndefs.build()\n</code></pre>"},{"location":"user-guide/building-features/#build-specific-features_1","title":"Build Specific Features","text":"<p>By feature name:</p> <pre><code>defs.build(feature_names=[\"user_total_spend\", \"user_avg_spend\"])\n</code></pre> <p>By tag:</p> <pre><code>defs.build(tag_names=[\"user_metrics\", \"demographics\"])\n</code></pre> <p>Mutually exclusive parameters</p> <p>The <code>feature_names</code> and <code>tag_names</code> parameters cannot be used together.</p>"},{"location":"user-guide/building-features/#force-rebuild_1","title":"Force Rebuild","text":"<pre><code>defs.build(force=True)\n</code></pre>"},{"location":"user-guide/building-features/#disable-preview_1","title":"Disable Preview","text":"<pre><code>defs.build(preview=False)\n</code></pre>"},{"location":"user-guide/building-features/#custom-preview-size","title":"Custom Preview Size","text":"<pre><code>defs.build(preview_rows=10)\n</code></pre>"},{"location":"user-guide/building-features/#get-output-paths","title":"Get Output Paths","text":"<p>The <code>build()</code> method returns a dictionary mapping feature names to their file paths:</p> <pre><code>from pathlib import Path\n\npaths = defs.build()\n\nfor feature_name, path in paths.items():\n    print(f\"{feature_name}: {path}\")\n\n# Output:\n# user_total_spend: feature_store/user_total_spend.parquet\n# user_avg_spend: feature_store/user_avg_spend.parquet\n</code></pre>"},{"location":"user-guide/building-features/#storage-backend","title":"Storage Backend","text":"<p>Features are stored in the configured offline store. mlforge supports both local and cloud storage backends.</p>"},{"location":"user-guide/building-features/#localstore","title":"LocalStore","text":"<p>Stores features as individual Parquet files on the local filesystem:</p> <pre><code>import mlforge as mlf\n\nstore = mlf.LocalStore(path=\"./feature_store\")\n</code></pre> <p>Each feature is saved as <code>feature_store/&lt;feature_name&gt;.parquet</code>.</p>"},{"location":"user-guide/building-features/#s3store","title":"S3Store","text":"<p>Stores features in Amazon S3 for production deployments:</p> <pre><code>import mlforge as mlf\n\nstore = mlf.S3Store(\n    bucket=\"mlforge-features\",\n    prefix=\"prod/features\"\n)\n</code></pre> <p>Features are stored at <code>s3://mlforge-features/prod/features/&lt;feature_name&gt;.parquet</code>.</p> <p>AWS Credentials</p> <p>S3Store uses standard AWS credential resolution (environment variables, <code>~/.aws/credentials</code>, or IAM roles).</p> <p>See the Storage Backends guide for detailed configuration and IAM policy examples.</p>"},{"location":"user-guide/building-features/#listing-features","title":"Listing Features","text":"<p>View all registered features:</p> <pre><code>mlforge list\n</code></pre> <p>Filter by tags:</p> <pre><code>mlforge list --tags user_metrics\n</code></pre> <p>Output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name             \u2502 Keys         \u2502 Source                   \u2502 Tags         \u2502 Description         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 user_total_spend \u2502 [user_id]    \u2502 data/transactions.parquet\u2502 user_metrics \u2502 Total spend by user \u2502\n\u2502 user_avg_spend   \u2502 [user_id]    \u2502 data/transactions.parquet\u2502 user_metrics \u2502 Avg spend by user   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Or in Python:</p> <pre><code># List all features\nfeatures = defs.list_features()\n\nfor feature in features:\n    print(f\"{feature.name}: {feature.description}\")\n\n# List features by tag\nuser_features = defs.list_features(tags=[\"user_metrics\"])\n</code></pre>"},{"location":"user-guide/building-features/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/building-features/#featurematerializationerror","title":"FeatureMaterializationError","text":"<p>Raised when a feature function fails or returns invalid data:</p> <pre><code>from mlforge.errors import FeatureMaterializationError\n\ntry:\n    defs.build()\nexcept FeatureMaterializationError as e:\n    print(f\"Failed to materialize feature: {e}\")\n</code></pre> <p>Common causes:</p> <ol> <li> <p>Feature function returns None <pre><code>@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef broken_feature(df):\n    df.group_by(\"user_id\").agg(...)\n    # Missing return statement!\n</code></pre></p> </li> <li> <p>Feature function returns wrong type <pre><code>@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef broken_feature(df):\n    return df.to_dict()  # Should return DataFrame\n</code></pre></p> </li> <li> <p>Missing key columns in output <pre><code>@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef broken_feature(df):\n    return df.select(\"amount\")  # user_id is missing!\n</code></pre></p> </li> </ol>"},{"location":"user-guide/building-features/#source-file-errors","title":"Source File Errors","text":"<p>If the source file doesn't exist or has an unsupported format:</p> <pre><code>@mlf.feature(keys=[\"user_id\"], source=\"data/missing.parquet\")\ndef my_feature(df): ...\n\n# Raises: FileNotFoundError\ndefs.build()\n</code></pre> <p>Supported formats: <code>.parquet</code>, <code>.csv</code></p>"},{"location":"user-guide/building-features/#workflow-example","title":"Workflow Example","text":"<p>A typical development workflow:</p> <pre><code># 1. Define features\ncat &gt; features.py &lt;&lt; 'EOF'\nimport mlforge as mlf\nimport polars as pl\n\n@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef user_age(df):\n    return df.select([\"user_id\", \"age\"])\nEOF\n\n# 2. Create definitions\ncat &gt; definitions.py &lt;&lt; 'EOF'\nimport mlforge as mlf\nimport features\n\ndefs = mlf.Definitions(\n    name=\"user-features\",\n    features=[features],\n    offline_store=mlf.LocalStore(\"./feature_store\")\n)\nEOF\n\n# 3. Build features\nmlforge build\n\n# 4. Verify\nmlforge list\n\n# 5. Rebuild specific features if needed\nmlforge build --features user_age --force\n</code></pre>"},{"location":"user-guide/building-features/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/building-features/#1-use-parquet-for-sources","title":"1. Use Parquet for Sources","text":"<p>Parquet is significantly faster than CSV for large datasets:</p> <pre><code># Convert CSV to Parquet once\nimport polars as pl\n\ndf = pl.read_csv(\"data/large_file.csv\")\ndf.write_parquet(\"data/large_file.parquet\")\n\n# Then use Parquet in features\n@mlf.feature(keys=[\"id\"], source=\"data/large_file.parquet\")\ndef my_feature(df): ...\n</code></pre>"},{"location":"user-guide/building-features/#2-filter-early","title":"2. Filter Early","text":"<p>If you don't need all source data, filter it early in your feature function:</p> <pre><code>@mlf.feature(keys=[\"user_id\"], source=\"data/all_events.parquet\")\ndef recent_user_activity(df):\n    return (\n        df\n        .filter(pl.col(\"event_date\") &gt;= \"2024-01-01\")  # Filter early\n        .group_by(\"user_id\")\n        .agg(pl.col(\"event_id\").count())\n    )\n</code></pre>"},{"location":"user-guide/building-features/#3-build-features-incrementally","title":"3. Build Features Incrementally","text":"<p>During development, build one feature at a time:</p> <pre><code>mlforge build --features new_feature\n</code></pre> <p>Once it works, build all features together.</p>"},{"location":"user-guide/building-features/#next-steps","title":"Next Steps","text":"<ul> <li>Retrieving Features - Use features in training pipelines</li> <li>Entity Keys - Work with surrogate keys</li> <li>Point-in-Time Correctness - Temporal feature joins</li> </ul>"},{"location":"user-guide/defining-features/","title":"Defining Features","text":"<p>Features in mlforge are defined using the <code>@mlf.feature</code> decorator, which transforms a Python function into a feature that can be built and retrieved.</p>"},{"location":"user-guide/defining-features/#the-feature-decorator","title":"The @feature Decorator","text":"<p>The decorator requires two parameters and accepts several optional ones:</p> <pre><code>import mlforge as mlf\nimport polars as pl\n\n@mlf.feature(\n    keys=[\"user_id\"],                    # Required: entity keys\n    source=\"data/transactions.parquet\",  # Required: source data path\n    tags=[\"user_metrics\"],               # Optional: feature grouping tags\n    timestamp=\"event_time\",              # Optional: for temporal features\n    description=\"User statistics\",       # Optional: human-readable description\n    interval=\"1d\",                       # Optional: for rolling aggregations\n    metrics=[]                           # Optional: metric specifications\n)\ndef user_stats(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.group_by(\"user_id\").agg(\n        pl.col(\"amount\").mean().alias(\"avg_spend\")\n    )\n</code></pre>"},{"location":"user-guide/defining-features/#required-parameters","title":"Required Parameters","text":""},{"location":"user-guide/defining-features/#keys","title":"keys","text":"<p>List of column names that uniquely identify entities. These columns will be used to join features to your entity DataFrame.</p> <pre><code>@mlf.feature(\n    keys=[\"user_id\"],  # Single key\n    source=\"data/users.parquet\"\n)\ndef user_age(df): ...\n\n@mlf.feature(\n    keys=[\"user_id\", \"merchant_id\"],  # Composite key\n    source=\"data/interactions.parquet\"\n)\ndef user_merchant_interaction(df): ...\n</code></pre>"},{"location":"user-guide/defining-features/#source","title":"source","text":"<p>Path to the source data file. Supports Parquet and CSV formats.</p> <pre><code>@mlf.feature(\n    keys=[\"product_id\"],\n    source=\"data/products.parquet\"  # Parquet\n)\ndef product_features(df): ...\n\n@mlf.feature(\n    keys=[\"customer_id\"],\n    source=\"data/customers.csv\"  # CSV\n)\ndef customer_features(df): ...\n</code></pre> <p>The path can be relative or absolute. Relative paths are resolved from your working directory.</p>"},{"location":"user-guide/defining-features/#optional-parameters","title":"Optional Parameters","text":""},{"location":"user-guide/defining-features/#tags","title":"tags","text":"<p>List of tags to group related features together. Tags enable selective building and listing of features.</p> <pre><code>@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    tags=[\"user_metrics\", \"revenue\"]\n)\ndef user_lifetime_value(df): ...\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/demographics.parquet\",\n    tags=[\"demographics\"]\n)\ndef user_age_group(df): ...\n</code></pre> <p>Build only features with specific tags:</p> <pre><code>mlforge build --tags user_metrics\nmlforge build --tags user_metrics,demographics\n</code></pre> <p>List features by tag:</p> <pre><code>mlforge list --tags revenue\n</code></pre> <p>Organizing features</p> <p>Use tags to organize features by domain (e.g., \"user\", \"product\"), category (e.g., \"demographics\", \"behavior\"), or team ownership (e.g., \"data-science\", \"ml-ops\").</p>"},{"location":"user-guide/defining-features/#timestamp","title":"timestamp","text":"<p>Column name for temporal features. When specified, enables point-in-time correct joins during retrieval.</p> <pre><code>@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/events.parquet\",\n    timestamp=\"event_timestamp\"  # Enables point-in-time joins\n)\ndef user_rolling_stats(df): ...\n</code></pre> <p>Point-in-time correctness</p> <p>Always specify a timestamp for features computed from time-series data. This ensures <code>mlf.get_training_data()</code> performs asof joins, preventing data leakage.</p> <p>See Point-in-Time Correctness for details.</p>"},{"location":"user-guide/defining-features/#description","title":"description","text":"<p>Human-readable description displayed by <code>mlforge list</code>.</p> <pre><code>@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    description=\"Total lifetime spend by user\"\n)\ndef user_lifetime_spend(df): ...\n</code></pre>"},{"location":"user-guide/defining-features/#interval","title":"interval","text":"<p>Time interval for rolling aggregations. Accepts either a string (e.g., <code>\"1d\"</code>, <code>\"6h\"</code>) or a Python <code>timedelta</code> object.</p> <pre><code>from datetime import timedelta\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"transaction_time\",\n    interval=\"1d\"  # String format\n)\ndef daily_features(df): ...\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"transaction_time\",\n    interval=timedelta(hours=6)  # timedelta object\n)\ndef hourly_features(df): ...\n</code></pre> <p>Supported string formats: <code>\"Ns\"</code> (seconds), <code>\"Nm\"</code> (minutes), <code>\"Nh\"</code> (hours), <code>\"Nd\"</code> (days), <code>\"Nw\"</code> (weeks).</p> <p>Using timedelta</p> <p>Using Python's <code>timedelta</code> can make intervals more readable and easier to compute: <pre><code>interval=timedelta(days=7)  # More explicit than \"7d\"\ninterval=timedelta(hours=6)  # Clearer than \"6h\"\ninterval=timedelta(weeks=4)  # Converts to \"28d\" automatically\n</code></pre></p>"},{"location":"user-guide/defining-features/#metrics","title":"metrics","text":"<p>List of metric specifications for computing features. Currently supports <code>mlf.Rolling</code> for time-windowed aggregations.</p> <pre><code>import mlforge as mlf\nfrom datetime import timedelta\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"transaction_time\",\n    interval=\"1d\",\n    metrics=[\n        mlf.Rolling(\n            windows=[\"7d\", \"30d\"],  # Can use strings\n            aggregations={\"amount\": [\"sum\", \"mean\", \"count\"]}\n        )\n    ]\n)\ndef user_transaction_metrics(df): ...\n\n# Or with timedelta for better readability\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"transaction_time\",\n    interval=timedelta(days=1),\n    metrics=[\n        mlf.Rolling(\n            windows=[timedelta(days=7), timedelta(days=30)],  # timedelta objects\n            aggregations={\"amount\": [\"sum\", \"mean\", \"count\"]}\n        )\n    ]\n)\ndef user_transaction_metrics(df): ...\n</code></pre> <p>The <code>mlf.Rolling</code> metric computes aggregations over sliding time windows. Output column names follow the pattern: <code>{tag}__{column}__{aggregation}__{window}__{interval}</code>.</p>"},{"location":"user-guide/defining-features/#feature-functions","title":"Feature Functions","text":"<p>The decorated function must:</p> <ol> <li>Accept a Polars DataFrame as input</li> <li>Return a Polars DataFrame</li> <li>Include the key columns in the output</li> </ol>"},{"location":"user-guide/defining-features/#basic-example","title":"Basic Example","text":"<pre><code>@mlf.feature(\n    keys=[\"product_id\"],\n    source=\"data/sales.parquet\"\n)\ndef product_total_sales(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.group_by(\"product_id\").agg(\n        pl.col(\"quantity\").sum().alias(\"total_sales\")\n    )\n</code></pre>"},{"location":"user-guide/defining-features/#aggregation-example","title":"Aggregation Example","text":"<pre><code>@mlf.feature(\n    keys=[\"customer_id\"],\n    source=\"data/orders.parquet\",\n    description=\"Customer order statistics\"\n)\ndef customer_order_stats(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.group_by(\"customer_id\").agg([\n        pl.col(\"order_id\").count().alias(\"order_count\"),\n        pl.col(\"total_amount\").sum().alias(\"lifetime_value\"),\n        pl.col(\"total_amount\").mean().alias(\"avg_order_value\"),\n        pl.col(\"order_date\").max().alias(\"last_order_date\")\n    ])\n</code></pre>"},{"location":"user-guide/defining-features/#time-based-rolling-features","title":"Time-Based Rolling Features","text":"<pre><code>@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/activity.parquet\",\n    timestamp=\"feature_timestamp\",\n    description=\"User activity over 7-day windows\"\n)\ndef user_7d_activity(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df\n        .sort(\"event_time\")\n        .group_by_dynamic(\n            \"event_time\",\n            every=\"1d\",\n            period=\"7d\",\n            by=\"user_id\"\n        )\n        .agg([\n            pl.col(\"event_id\").count().alias(\"event_count_7d\"),\n            pl.col(\"session_id\").n_unique().alias(\"unique_sessions_7d\")\n        ])\n        .rename({\"event_time\": \"feature_timestamp\"})\n    )\n</code></pre> <p>Timestamp column naming</p> <p>For temporal features, rename your time column to <code>feature_timestamp</code> in the output. This convention ensures correct asof joins during retrieval.</p>"},{"location":"user-guide/defining-features/#multiple-features-per-module","title":"Multiple Features per Module","text":"<p>Organize related features in a single module:</p> <pre><code># user_features.py\nimport mlforge as mlf\nimport polars as pl\n\nSOURCE = \"data/users.parquet\"\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=SOURCE,\n    tags=[\"demographics\"]\n)\ndef user_age(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.select([\"user_id\", \"age\"])\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=SOURCE,\n    tags=[\"demographics\"]\n)\ndef user_tenure_days(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.with_columns(\n        (pl.col(\"created_at\").dt.date() - pl.lit(\"2020-01-01\").str.to_date())\n        .dt.total_days()\n        .alias(\"tenure_days\")\n    ).select([\"user_id\", \"tenure_days\"])\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=SOURCE,\n    tags=[\"subscription\"]\n)\ndef user_is_premium(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.select([\"user_id\", \"is_premium\"])\n</code></pre> <p>Then register the entire module:</p> <pre><code># definitions.py\nimport mlforge as mlf\nimport user_features\n\ndefs = mlf.Definitions(\n    name=\"my-project\",\n    features=[user_features],  # Auto-discovers all features\n    offline_store=mlf.LocalStore(\"./feature_store\")\n)\n</code></pre>"},{"location":"user-guide/defining-features/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/defining-features/#1-keep-features-pure","title":"1. Keep Features Pure","text":"<p>Feature functions should be deterministic and stateless:</p> <pre><code># Good - pure transformation\n@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef user_age_group(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.with_columns(\n        pl.when(pl.col(\"age\") &lt; 25).then(pl.lit(\"young\"))\n        .when(pl.col(\"age\") &lt; 65).then(pl.lit(\"adult\"))\n        .otherwise(pl.lit(\"senior\"))\n        .alias(\"age_group\")\n    )\n\n# Bad - depends on external state\ncurrent_year = 2024  # External dependency\n\n@mlf.feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef user_is_adult(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.with_columns(\n        (current_year - pl.col(\"birth_year\") &gt;= 18).alias(\"is_adult\")\n    )\n</code></pre>"},{"location":"user-guide/defining-features/#2-use-descriptive-names","title":"2. Use Descriptive Names","text":"<p>Name features after what they represent, not how they're computed:</p> <pre><code># Good\n@mlf.feature(keys=[\"user_id\"], source=\"data/transactions.parquet\")\ndef user_total_spend(df): ...\n\n# Bad\n@mlf.feature(keys=[\"user_id\"], source=\"data/transactions.parquet\")\ndef sum_amount_by_user(df): ...\n</code></pre>"},{"location":"user-guide/defining-features/#3-include-key-columns","title":"3. Include Key Columns","text":"<p>Always ensure key columns are in the output:</p> <pre><code># Good\n@mlf.feature(keys=[\"user_id\"], source=\"data/events.parquet\")\ndef user_event_count(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.group_by(\"user_id\").agg(\n        pl.col(\"event_id\").count().alias(\"event_count\")\n    )  # user_id is preserved by group_by\n\n# Bad - missing key\n@mlf.feature(keys=[\"user_id\"], source=\"data/events.parquet\")\ndef user_event_count(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.select(\n        pl.col(\"event_id\").count().alias(\"event_count\")\n    )  # user_id is lost!\n</code></pre>"},{"location":"user-guide/defining-features/#4-add-descriptions-for-complex-features","title":"4. Add Descriptions for Complex Features","text":"<pre><code>@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"feature_timestamp\",\n    description=\"30-day rolling average of transaction amounts\"\n)\ndef user_spend_mean_30d(df): ...\n</code></pre>"},{"location":"user-guide/defining-features/#next-steps","title":"Next Steps","text":"<ul> <li>Building Features - Materialize features to storage</li> <li>Entity Keys - Work with surrogate keys and composite identifiers</li> <li>Point-in-Time Correctness - Learn about temporal joins</li> </ul>"},{"location":"user-guide/entity-keys/","title":"Entity Keys","text":"<p>Entity keys are identifiers that uniquely reference entities in your feature store. mlforge provides utilities for working with both natural keys and surrogate keys.</p>"},{"location":"user-guide/entity-keys/#surrogate-keys","title":"Surrogate Keys","text":"<p>A surrogate key is a generated identifier created by hashing natural key columns. Use <code>surrogate_key()</code> to create them.</p>"},{"location":"user-guide/entity-keys/#basic-usage","title":"Basic Usage","text":"<pre><code>from mlforge import surrogate_key\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"first\": [\"Alice\", \"Bob\"],\n    \"last\": [\"Smith\", \"Jones\"],\n    \"dob\": [\"1990-01-01\", \"1985-05-15\"]\n})\n\n# Generate surrogate key\nresult = df.with_columns(\n    surrogate_key(\"first\", \"last\", \"dob\").alias(\"user_id\")\n)\n\nprint(result)\n</code></pre> <p>Output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 first \u2502 last  \u2502 dob        \u2502 user_id              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice \u2502 Smith \u2502 1990-01-01 \u2502 12345678901234567890 \u2502\n\u2502 Bob   \u2502 Jones \u2502 1985-05-15 \u2502 98765432109876543210 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/entity-keys/#how-it-works","title":"How It Works","text":"<p><code>surrogate_key()</code> concatenates column values with a separator, handles nulls, and hashes the result:</p> <pre><code>surrogate_key(\"col1\", \"col2\", \"col3\")\n\n# Equivalent to:\n# concat_str([col1, col2, col3], separator=\"||\")\n#   .fill_null(\"__NULL__\")\n#   .hash()\n#   .cast(String)\n</code></pre>"},{"location":"user-guide/entity-keys/#null-handling","title":"Null Handling","text":"<p>Null values are replaced with <code>\"__NULL__\"</code> before hashing:</p> <pre><code>df = pl.DataFrame({\n    \"first\": [\"Alice\", None],\n    \"last\": [\"Smith\", \"Jones\"]\n})\n\nresult = df.with_columns(\n    surrogate_key(\"first\", \"last\").alias(\"user_id\")\n)\n\n# Both rows get unique IDs despite the null\n</code></pre>"},{"location":"user-guide/entity-keys/#multiple-columns","title":"Multiple Columns","text":"<p>Use as many columns as needed:</p> <pre><code># Simple key\nuser_id = surrogate_key(\"email\")\n\n# Composite key\ntransaction_id = surrogate_key(\"user_id\", \"merchant_id\", \"timestamp\")\n\n# Complex key\nsession_id = surrogate_key(\"device_id\", \"ip_address\", \"user_agent\", \"timestamp\")\n</code></pre>"},{"location":"user-guide/entity-keys/#entity-key-transforms","title":"Entity Key Transforms","text":"<p>The <code>entity_key()</code> function creates reusable transformations that add surrogate keys to DataFrames.</p>"},{"location":"user-guide/entity-keys/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from mlforge import entity_key\nimport polars as pl\n\n# Define a reusable transform\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# Apply to DataFrame\ndf = pl.DataFrame({\n    \"first\": [\"Alice\", \"Bob\"],\n    \"last\": [\"Smith\", \"Jones\"],\n    \"dob\": [\"1990-01-01\", \"1985-05-15\"]\n})\n\nresult = df.pipe(with_user_id)\n\nprint(result)\n</code></pre> <p>Output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 first \u2502 last  \u2502 dob        \u2502 user_id              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice \u2502 Smith \u2502 1990-01-01 \u2502 12345678901234567890 \u2502\n\u2502 Bob   \u2502 Jones \u2502 1985-05-15 \u2502 98765432109876543210 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/entity-keys/#using-in-features","title":"Using in Features","text":"<p>Entity transforms are commonly used in feature definitions:</p> <pre><code>from mlforge import feature, entity_key\nimport polars as pl\n\n# Define entity transforms\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\nwith_merchant_id = entity_key(\"merchant\", alias=\"merchant_id\")\n\n@feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    description=\"Total spend by user\"\n)\ndef user_total_spend(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df\n        .pipe(with_user_id)  # Add user_id\n        .group_by(\"user_id\")\n        .agg(pl.col(\"amount\").sum().alias(\"total_spend\"))\n    )\n\n@feature(\n    keys=[\"user_id\", \"merchant_id\"],\n    source=\"data/transactions.parquet\",\n    description=\"Spend by user and merchant\"\n)\ndef user_merchant_spend(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df\n        .pipe(with_user_id)        # Add user_id\n        .pipe(with_merchant_id)    # Add merchant_id\n        .group_by([\"user_id\", \"merchant_id\"])\n        .agg(pl.col(\"amount\").sum().alias(\"total_spend\"))\n    )\n</code></pre>"},{"location":"user-guide/entity-keys/#using-in-retrieval","title":"Using in Retrieval","text":"<p>Pass entity transforms to <code>get_training_data()</code> when your entity DataFrame lacks required keys:</p> <pre><code>from mlforge import get_training_data, entity_key\nimport polars as pl\n\n# Entity data without user_id\nentities = pl.DataFrame({\n    \"first\": [\"Alice\", \"Bob\"],\n    \"last\": [\"Smith\", \"Jones\"],\n    \"dob\": [\"1990-01-01\", \"1985-05-15\"],\n    \"label\": [0, 1]\n})\n\n# Define transform\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# Retrieve features\ntraining_data = get_training_data(\n    features=[\"user_total_spend\"],\n    entity_df=entities,\n    entities=[with_user_id]  # Adds user_id before joining\n)\n\nprint(training_data)\n</code></pre> <p>Output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 first \u2502 last  \u2502 dob        \u2502 user_id              \u2502 total_spend \u2502 label \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice \u2502 Smith \u2502 1990-01-01 \u2502 12345678901234567890 \u2502 150.0       \u2502 0     \u2502\n\u2502 Bob   \u2502 Jones \u2502 1985-05-15 \u2502 98765432109876543210 \u2502 250.0       \u2502 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/entity-keys/#multiple-transforms","title":"Multiple Transforms","text":"<p>Apply multiple transforms at once:</p> <pre><code>from mlforge import entity_key, get_training_data\n\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\nwith_account_id = entity_key(\"cc_num\", alias=\"account_id\")\n\ntraining_data = get_training_data(\n    features=[\"user_spend\", \"account_spend\"],\n    entity_df=raw_data,\n    entities=[with_user_id, with_account_id]  # Both transforms applied\n)\n</code></pre>"},{"location":"user-guide/entity-keys/#complete-example","title":"Complete Example","text":"<p>Here's a real-world example using the transactions dataset:</p> <pre><code># entities.py\nfrom mlforge import entity_key\n\n# Define all entity transforms\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\nwith_merchant_id = entity_key(\"merchant\", alias=\"merchant_id\")\nwith_account_id = entity_key(\"cc_num\", alias=\"account_id\")\n</code></pre> <pre><code># features.py\nfrom mlforge import feature\nimport polars as pl\nfrom entities import with_user_id, with_merchant_id, with_account_id\n\nSOURCE = \"data/transactions.parquet\"\n\n@feature(keys=[\"user_id\"], source=SOURCE)\ndef user_total_spend(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df.pipe(with_user_id)\n        .group_by(\"user_id\")\n        .agg(pl.col(\"amt\").sum().alias(\"total_spend\"))\n    )\n\n@feature(keys=[\"merchant_id\"], source=SOURCE)\ndef merchant_total_revenue(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df.pipe(with_merchant_id)\n        .group_by(\"merchant_id\")\n        .agg(pl.col(\"amt\").sum().alias(\"total_revenue\"))\n    )\n\n@feature(keys=[\"account_id\"], source=SOURCE)\ndef account_total_spend(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df.pipe(with_account_id)\n        .group_by(\"account_id\")\n        .agg(pl.col(\"amt\").sum().alias(\"total_spend\"))\n    )\n</code></pre> <pre><code># train.py\nfrom mlforge import get_training_data\nimport polars as pl\nfrom entities import with_user_id, with_merchant_id\n\n# Load raw transaction data\ntransactions = pl.read_parquet(\"data/transactions.parquet\")\n\n# Get features with automatic key generation\ntraining_data = get_training_data(\n    features=[\"user_total_spend\", \"merchant_total_revenue\"],\n    entity_df=transactions,\n    entities=[with_user_id, with_merchant_id]\n)\n</code></pre>"},{"location":"user-guide/entity-keys/#when-to-use-surrogate-keys","title":"When to Use Surrogate Keys","text":""},{"location":"user-guide/entity-keys/#use-surrogate-keys-when","title":"Use surrogate keys when:","text":"<ol> <li> <p>Natural keys are composite <pre><code># Instead of: keys=[\"first\", \"last\", \"dob\"]\n# Use: keys=[\"user_id\"]\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n</code></pre></p> </li> <li> <p>Natural keys are PII <pre><code># Hash email addresses\nwith_user_id = entity_key(\"email\", alias=\"user_id\")\n</code></pre></p> </li> <li> <p>You need stable identifiers across datasets <pre><code># Same user_id in different sources\nusers = raw_users.pipe(with_user_id)\ntransactions = raw_transactions.pipe(with_user_id)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/entity-keys/#dont-use-surrogate-keys-when","title":"Don't use surrogate keys when:","text":"<ol> <li> <p>You already have a natural ID column <pre><code># If your data has user_id, just use it\n@feature(keys=[\"user_id\"], source=\"data/users.parquet\")\ndef user_age(df): ...\n</code></pre></p> </li> <li> <p>Keys are simple and non-sensitive <pre><code># product_id, order_id, etc.\n@feature(keys=[\"product_id\"], source=\"data/products.parquet\")\ndef product_price(df): ...\n</code></pre></p> </li> </ol>"},{"location":"user-guide/entity-keys/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/entity-keys/#1-define-transforms-once","title":"1. Define Transforms Once","text":"<p>Create a dedicated module for entity transforms:</p> <pre><code># entities.py\nfrom mlforge import entity_key\n\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\nwith_merchant_id = entity_key(\"merchant\", alias=\"merchant_id\")\n</code></pre> <p>Import and reuse across features:</p> <pre><code># features.py\nfrom entities import with_user_id, with_merchant_id\n</code></pre>"},{"location":"user-guide/entity-keys/#2-use-consistent-aliases","title":"2. Use Consistent Aliases","text":"<p>Use the same alias name across your project:</p> <pre><code># Good - consistent naming\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# Bad - inconsistent\nwith_uid = entity_key(\"first\", \"last\", \"dob\", alias=\"uid\")\n</code></pre>"},{"location":"user-guide/entity-keys/#3-document-natural-keys","title":"3. Document Natural Keys","text":"<p>Add comments explaining what natural keys compose each surrogate:</p> <pre><code># user_id: hash(first, last, dob)\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# merchant_id: hash(merchant)\nwith_merchant_id = entity_key(\"merchant\", alias=\"merchant_id\")\n</code></pre>"},{"location":"user-guide/entity-keys/#4-validate-key-columns-exist","title":"4. Validate Key Columns Exist","text":"<p>Check that required columns are present before applying transforms:</p> <pre><code>from mlforge import entity_key\n\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# This will raise a clear error if columns are missing\ndf = raw_data.pipe(with_user_id)\n</code></pre>"},{"location":"user-guide/entity-keys/#next-steps","title":"Next Steps","text":"<ul> <li>Point-in-Time Correctness - Temporal feature joins</li> <li>Retrieving Features - Using entity transforms in retrieval</li> <li>API Reference - Utils - Complete API documentation</li> </ul>"},{"location":"user-guide/feature-metadata/","title":"Feature Metadata","text":"<p>mlforge automatically captures and stores metadata for every feature you build. This metadata provides visibility into your feature store, making it easier to understand what features exist, when they were last updated, and what columns they contain.</p>"},{"location":"user-guide/feature-metadata/#what-is-captured","title":"What is Captured","text":"<p>When you build a feature with <code>mlforge build</code>, the following metadata is automatically captured:</p>"},{"location":"user-guide/feature-metadata/#feature-configuration","title":"Feature Configuration","text":"<ul> <li>Name: Feature identifier</li> <li>Entity: Primary entity key</li> <li>Keys: All entity key columns</li> <li>Timestamp: Temporal column (if applicable)</li> <li>Interval: Time interval for rolling aggregations (if applicable)</li> <li>Tags: Feature grouping tags</li> <li>Description: Human-readable description from the <code>@feature</code> decorator</li> </ul>"},{"location":"user-guide/feature-metadata/#versioning-v050","title":"Versioning (v0.5.0+)","text":"<ul> <li>Version: Semantic version string (e.g., \"1.0.0\", \"1.2.3\")</li> <li>Created At: ISO 8601 timestamp of when this version was first created</li> <li>Source Hash: Hash of source data file for reproducibility verification</li> <li>Schema Hash: Hash of column names and types for schema change detection</li> <li>Config Hash: Hash of feature configuration for config change detection</li> <li>Content Hash: Hash of materialized data for content change detection</li> <li>Change Summary: Structured information about what changed:<ul> <li><code>bump_type</code>: Type of version bump (initial/major/minor/patch)</li> <li><code>reason</code>: Why the version was bumped (e.g., \"columns_added\", \"data_refresh\")</li> <li><code>details</code>: Specific changes (e.g., list of added/removed columns)</li> </ul> </li> </ul>"},{"location":"user-guide/feature-metadata/#storage-details","title":"Storage Details","text":"<ul> <li>Path: Location of the materialized feature file (versioned: <code>feature_store/feature_name/version/data.parquet</code>)</li> <li>Source: Path to the source data file</li> <li>Row Count: Number of rows in the materialized feature</li> <li>Last Updated: ISO 8601 timestamp of when this version was last built</li> </ul>"},{"location":"user-guide/feature-metadata/#column-information","title":"Column Information","text":"<p>mlforge separates base columns from generated feature columns:</p>"},{"location":"user-guide/feature-metadata/#base-columns","title":"Base Columns","text":"<p>Base columns are from your feature function output (before metrics are applied):</p> <ul> <li>Name: Column name</li> <li>Type: Polars data type (e.g., <code>Utf8</code>, <code>Float64</code>, <code>Date</code>)</li> <li>Validators: Data quality validators applied to this column</li> </ul> <p>Example base columns: - Entity keys (e.g., <code>user_id</code>, <code>merchant_id</code>) - Timestamp columns (e.g., <code>transaction_date</code>) - Input columns for metrics (e.g., <code>amount</code>, <code>quantity</code>)</p>"},{"location":"user-guide/feature-metadata/#feature-columns","title":"Feature Columns","text":"<p>Feature columns are generated by metrics (e.g., rolling aggregations):</p> <ul> <li>Name: Column name</li> <li>Type: Polars data type</li> <li>Input: Source column from base columns</li> <li>Aggregation: Type of aggregation (sum, count, mean, etc.)</li> <li>Window: Time window for rolling aggregation</li> </ul>"},{"location":"user-guide/feature-metadata/#where-metadata-is-stored","title":"Where Metadata is Stored","text":"<p>Metadata is stored in <code>.meta.json</code> files within each feature's version directory:</p> <pre><code>feature_store/\n\u251c\u2500\u2500 user_spend/\n\u2502   \u251c\u2500\u2500 1.0.0/\n\u2502   \u2502   \u251c\u2500\u2500 data.parquet\n\u2502   \u2502   \u2514\u2500\u2500 .meta.json       # Metadata for v1.0.0\n\u2502   \u251c\u2500\u2500 1.0.1/\n\u2502   \u2502   \u251c\u2500\u2500 data.parquet\n\u2502   \u2502   \u2514\u2500\u2500 .meta.json       # Metadata for v1.0.1\n\u2502   \u251c\u2500\u2500 _latest.json         # Pointer: {\"version\": \"1.0.1\"}\n\u2502   \u2514\u2500\u2500 .gitignore\n\u251c\u2500\u2500 merchant_spend/\n\u2502   \u251c\u2500\u2500 1.0.0/\n\u2502   \u2502   \u251c\u2500\u2500 data.parquet\n\u2502   \u2502   \u2514\u2500\u2500 .meta.json\n\u2502   \u251c\u2500\u2500 _latest.json\n\u2502   \u2514\u2500\u2500 .gitignore\n\u2514\u2500\u2500 account_spend/\n    \u251c\u2500\u2500 2.0.0/\n        \u251c\u2500\u2500 data.parquet\n        \u2514\u2500\u2500 .meta.json\n    \u251c\u2500\u2500 _latest.json\n    \u2514\u2500\u2500 .gitignore\n</code></pre> <p>Each <code>.meta.json</code> file contains the complete metadata for one specific version of a feature in JSON format.</p>"},{"location":"user-guide/feature-metadata/#viewing-metadata","title":"Viewing Metadata","text":""},{"location":"user-guide/feature-metadata/#using-the-cli","title":"Using the CLI","text":""},{"location":"user-guide/feature-metadata/#inspect-a-specific-feature","title":"Inspect a Specific Feature","text":"<p>Use the <code>inspect</code> command to view detailed metadata for a feature:</p> <pre><code>mlforge inspect user_spend\n</code></pre> <p>This displays:</p> <ul> <li>Feature configuration</li> <li>Storage details</li> <li>Column information in a formatted table</li> <li>Tags and description</li> </ul>"},{"location":"user-guide/feature-metadata/#view-all-features","title":"View All Features","text":"<p>Use the <code>manifest</code> command to see a summary of all features:</p> <pre><code>mlforge manifest\n</code></pre> <p>This shows a table with key metrics for each feature:</p> <ul> <li>Feature name</li> <li>Entity</li> <li>Row count</li> <li>Column count</li> <li>Last updated timestamp</li> </ul>"},{"location":"user-guide/feature-metadata/#programmatically","title":"Programmatically","text":"<p>You can also read metadata in your Python code:</p> <pre><code>from mlforge import LocalStore\n\nstore = LocalStore(\"./feature_store\")\n\n# Read metadata for a specific feature\nmetadata = store.read_metadata(\"user_spend\")\n\nif metadata:\n    print(f\"Feature: {metadata.name}\")\n    print(f\"Rows: {metadata.row_count:,}\")\n    print(f\"Last updated: {metadata.last_updated}\")\n\n    # Inspect base columns\n    print(\"\\nBase Columns:\")\n    for col in metadata.columns:\n        print(f\"  {col.name}: {col.dtype}\")\n        if col.validators:\n            for validator in col.validators:\n                print(f\"    - {validator}\")\n\n    # Inspect feature columns\n    print(\"\\nGenerated Features:\")\n    for col in metadata.features:\n        print(f\"  {col.name}: {col.agg}({col.input}) over {col.window}\")\n</code></pre>"},{"location":"user-guide/feature-metadata/#example-metadata-structure","title":"Example Metadata Structure","text":"<p>Here's an example of how metadata is structured for a feature with validators and rolling metrics:</p> <pre><code>from mlforge import feature\nfrom mlforge.metrics import Rolling\nfrom mlforge.validators import not_null, greater_than_or_equal\n\n@feature(\n    keys=[\"merchant_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"transaction_date\",\n    interval=\"1d\",\n    metrics=[\n        Rolling(\n            windows=[\"7d\", \"30d\"],\n            aggregations={\"amt\": [\"sum\", \"count\", \"mean\"]}\n        )\n    ],\n    validators={\n        \"amt\": [not_null(), greater_than_or_equal(0)]\n    }\n)\ndef merchant_spend(df):\n    return df.select([\"merchant_id\", \"transaction_date\", \"amt\"])\n</code></pre> <p>After building, the <code>.meta.json</code> file will contain:</p> <pre><code>{\n  \"name\": \"merchant_spend\",\n  \"version\": \"1.0.0\",\n  \"path\": \"feature_store/merchant_spend/1.0.0/data.parquet\",\n  \"entity\": \"merchant_id\",\n  \"keys\": [\"merchant_id\"],\n  \"source\": \"data/transactions.parquet\",\n  \"row_count\": 10000,\n  \"created_at\": \"2024-01-16T08:30:00Z\",\n  \"last_updated\": \"2024-01-16T08:30:00Z\",\n  \"timestamp\": \"transaction_date\",\n  \"interval\": \"1d\",\n  \"source_hash\": \"abc123def456\",\n  \"schema_hash\": \"789abc012def\",\n  \"config_hash\": \"456def789abc\",\n  \"content_hash\": \"012def456abc\",\n  \"change_summary\": {\n    \"bump_type\": \"initial\",\n    \"reason\": \"first_build\",\n    \"details\": []\n  },\n  \"columns\": [\n    {\n      \"name\": \"merchant_id\",\n      \"dtype\": \"String\"\n    },\n    {\n      \"name\": \"transaction_date\",\n      \"dtype\": \"Datetime(time_unit='us', time_zone=None)\"\n    },\n    {\n      \"name\": \"amt\",\n      \"dtype\": \"Float64\",\n      \"validators\": [\n        {\"validator\": \"not_null\"},\n        {\"validator\": \"greater_than_or_equal\", \"value\": 0}\n      ]\n    }\n  ],\n  \"features\": [\n    {\n      \"name\": \"merchant_spend__amt__sum__1d__7d\",\n      \"dtype\": \"Float64\",\n      \"input\": \"amt\",\n      \"agg\": \"sum\",\n      \"window\": \"7d\"\n    },\n    {\n      \"name\": \"merchant_spend__amt__count__1d__7d\",\n      \"dtype\": \"UInt32\",\n      \"input\": \"amt\",\n      \"agg\": \"count\",\n      \"window\": \"7d\"\n    },\n    {\n      \"name\": \"merchant_spend__amt__mean__1d__7d\",\n      \"dtype\": \"Float64\",\n      \"input\": \"amt\",\n      \"agg\": \"mean\",\n      \"window\": \"7d\"\n    }\n  ]\n}\n</code></pre> <p>Notice how: - Base columns (keys, timestamp, input) are in the <code>columns</code> array with validator info - Generated features (rolling metrics) are in the <code>features</code> array with aggregation details - Validators include both the name and parameters (e.g., <code>value: 0</code>)</p>"},{"location":"user-guide/feature-metadata/#consolidated-manifest","title":"Consolidated Manifest","text":"<p>You can generate a consolidated <code>manifest.json</code> file that contains metadata for all features:</p> <pre><code>mlforge manifest --regenerate\n</code></pre> <p>This creates a single JSON file with all feature metadata, useful for:</p> <ul> <li>Documentation generation</li> <li>Feature catalog UIs</li> <li>Integration with other tools</li> <li>Version control tracking</li> </ul> <p>The manifest file structure:</p> <pre><code>{\n  \"version\": \"1.0\",\n  \"generated_at\": \"2024-01-16T08:30:00Z\",\n  \"features\": {\n    \"user_spend\": { ... },\n    \"merchant_spend\": { ... },\n    \"account_spend\": { ... }\n  }\n}\n</code></pre>"},{"location":"user-guide/feature-metadata/#use-cases","title":"Use Cases","text":""},{"location":"user-guide/feature-metadata/#feature-discovery","title":"Feature Discovery","text":"<p>Quickly understand what features exist in your store:</p> <pre><code>mlforge manifest\n</code></pre>"},{"location":"user-guide/feature-metadata/#debugging","title":"Debugging","text":"<p>Check when a feature was last built and how many rows it has:</p> <pre><code>mlforge inspect user_spend\n</code></pre>"},{"location":"user-guide/feature-metadata/#monitoring","title":"Monitoring","text":"<p>Track feature freshness by comparing <code>last_updated</code> timestamps:</p> <pre><code>from mlforge import LocalStore\nfrom datetime import datetime, timezone, timedelta\n\nstore = LocalStore(\"./feature_store\")\n\n# Find stale features (not updated in last 24 hours)\ncutoff = datetime.now(timezone.utc) - timedelta(hours=24)\n\nfor meta in store.list_metadata():\n    last_updated = datetime.fromisoformat(meta.last_updated.replace('Z', '+00:00'))\n    if last_updated &lt; cutoff:\n        print(f\"Stale feature: {meta.name} (last updated {meta.last_updated})\")\n</code></pre>"},{"location":"user-guide/feature-metadata/#documentation","title":"Documentation","text":"<p>Generate feature catalogs from metadata:</p> <pre><code>from mlforge import LocalStore\n\nstore = LocalStore(\"./feature_store\")\n\nprint(\"# Feature Catalog\\n\")\n\nfor meta in store.list_metadata():\n    print(f\"## {meta.name}\")\n    if meta.description:\n        print(f\"\\n{meta.description}\\n\")\n    print(f\"- **Entity**: {meta.entity}\")\n    print(f\"- **Rows**: {meta.row_count:,}\")\n    print(f\"- **Base Columns**: {len(meta.columns)}\")\n    print(f\"- **Features**: {len(meta.features)}\")\n    if meta.tags:\n        print(f\"- **Tags**: {', '.join(meta.tags)}\")\n    print()\n</code></pre>"},{"location":"user-guide/feature-metadata/#metadata-schema","title":"Metadata Schema","text":"<p>See the Manifest API Reference for detailed JSON schema documentation.</p>"},{"location":"user-guide/feature-metadata/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference - <code>inspect</code> and <code>manifest</code> commands</li> <li>Manifest API - Programmatic access to metadata</li> <li>Building Features - How to build features that generate metadata</li> </ul>"},{"location":"user-guide/online-stores/","title":"Online Stores","text":"<p>Online stores provide low-latency feature serving for real-time inference. mlforge supports Redis as an online store backend.</p>"},{"location":"user-guide/online-stores/#overview","title":"Overview","text":"Store Use Case Latency Setup LocalStore Development, batch ~10ms None S3Store Production batch ~100ms AWS credentials RedisStore Real-time inference ~1ms Redis server"},{"location":"user-guide/online-stores/#redis-store","title":"Redis Store","text":""},{"location":"user-guide/online-stores/#installation","title":"Installation","text":"<p>Install mlforge with Redis support:</p> pipuv <pre><code>pip install mlforge-sdk[redis]\n</code></pre> <pre><code>uv add mlforge-sdk[redis]\n</code></pre>"},{"location":"user-guide/online-stores/#starting-redis","title":"Starting Redis","text":"DockerDocker ComposeLocal Install <pre><code>docker run -d --name redis -p 6379:6379 redis:7-alpine\n</code></pre> <p><pre><code># docker-compose.yml\nservices:\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n</code></pre> <pre><code>docker-compose up -d\n</code></pre></p> <pre><code># macOS\nbrew install redis\nredis-server\n\n# Ubuntu\nsudo apt install redis-server\nsudo systemctl start redis\n</code></pre>"},{"location":"user-guide/online-stores/#configuration","title":"Configuration","text":"<p>Configure Redis in your definitions file:</p> <pre><code>import mlforge as mlf\nfrom mlforge.online import RedisStore\n\ndefs = mlf.Definitions(\n    name=\"my-project\",\n    features=[user_spend, merchant_risk],\n    offline_store=mlf.LocalStore(\"./feature_store\"),\n    online_store=RedisStore(\n        host=\"localhost\",\n        port=6379,\n        db=0,\n        password=None,  # Set for authenticated Redis\n        prefix=\"mlforge\",  # Key prefix\n        ttl=None,  # Optional TTL in seconds\n    ),\n)\n</code></pre>"},{"location":"user-guide/online-stores/#building-to-online-store","title":"Building to Online Store","text":"<p>Build features to Redis using the <code>--online</code> flag:</p> <pre><code>mlforge build --online\n</code></pre> <p>This extracts the latest value per entity and writes to Redis.</p>"},{"location":"user-guide/online-stores/#key-format","title":"Key Format","text":"<p>Features are stored with keys in the format:</p> <pre><code>{prefix}:{feature_name}:{entity_hash}\n</code></pre> <p>Example: <pre><code>mlforge:user_spend:a1b2c3d4e5f6g7h8\n</code></pre></p>"},{"location":"user-guide/online-stores/#reading-features","title":"Reading Features","text":""},{"location":"user-guide/online-stores/#low-level-api","title":"Low-level API","text":"<pre><code>from mlforge.online import RedisStore\n\nstore = RedisStore(host=\"localhost\")\n\n# Single entity\nvalue = store.read(\n    feature_name=\"user_spend\",\n    entity_keys={\"user_id\": \"12345\"}\n)\n# Returns: {\"amt__sum__7d\": 150.0, \"amt__count__7d\": 5}\n\n# Batch read\nvalues = store.read_batch(\n    feature_name=\"user_spend\",\n    entity_keys=[\n        {\"user_id\": \"12345\"},\n        {\"user_id\": \"67890\"},\n    ]\n)\n# Returns: [{\"amt__sum__7d\": 150.0, ...}, {\"amt__sum__7d\": 200.0, ...}]\n</code></pre>"},{"location":"user-guide/online-stores/#high-level-api-recommended","title":"High-level API (Recommended)","text":"<pre><code>import mlforge as mlf\nfrom mlforge.online import RedisStore\nimport polars as pl\n\nstore = RedisStore(host=\"localhost\")\nwith_user_id = mlf.entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# Inference request\nrequest_df = pl.DataFrame({\n    \"request_id\": [\"req_001\", \"req_002\"],\n    \"first\": [\"John\", \"Jane\"],\n    \"last\": [\"Doe\", \"Smith\"],\n    \"dob\": [\"1990-01-15\", \"1985-06-20\"],\n})\n\n# Retrieve features (applies entity transform, joins results)\nfeatures_df = mlf.get_online_features(\n    features=[\"user_spend\"],\n    entity_df=request_df,\n    store=store,\n    entities=[with_user_id],\n)\n</code></pre>"},{"location":"user-guide/online-stores/#connection-options","title":"Connection Options","text":""},{"location":"user-guide/online-stores/#authentication","title":"Authentication","text":"<pre><code>store = RedisStore(\n    host=\"redis.example.com\",\n    port=6379,\n    password=\"your-redis-password\",\n)\n</code></pre>"},{"location":"user-guide/online-stores/#database-selection","title":"Database Selection","text":"<p>Redis supports multiple databases (0-15 by default):</p> <pre><code>store = RedisStore(\n    host=\"localhost\",\n    db=1,  # Use database 1 instead of 0\n)\n</code></pre>"},{"location":"user-guide/online-stores/#ttl-time-to-live","title":"TTL (Time-to-Live)","text":"<p>Set automatic expiration for features:</p> <pre><code>store = RedisStore(\n    host=\"localhost\",\n    ttl=86400,  # Expire after 24 hours\n)\n</code></pre>"},{"location":"user-guide/online-stores/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/online-stores/#1-use-separate-databases-for-environments","title":"1. Use Separate Databases for Environments","text":"<pre><code># Development\ndev_store = RedisStore(host=\"localhost\", db=0)\n\n# Staging\nstaging_store = RedisStore(host=\"localhost\", db=1)\n\n# Production\nprod_store = RedisStore(host=\"redis.prod.internal\", db=0)\n</code></pre>"},{"location":"user-guide/online-stores/#2-set-ttl-for-stale-data-prevention","title":"2. Set TTL for Stale Data Prevention","text":"<pre><code>store = RedisStore(\n    host=\"localhost\",\n    ttl=7 * 24 * 3600,  # 7 days\n)\n</code></pre>"},{"location":"user-guide/online-stores/#3-use-connection-pooling-in-production","title":"3. Use Connection Pooling in Production","text":"<p>RedisStore uses connection pooling by default. For high-throughput scenarios, tune your Redis server's <code>maxclients</code> setting.</p>"},{"location":"user-guide/online-stores/#4-monitor-redis-memory","title":"4. Monitor Redis Memory","text":"<pre><code>redis-cli info memory\n</code></pre>"},{"location":"user-guide/online-stores/#5-verify-connection-before-use","title":"5. Verify Connection Before Use","text":"<pre><code>store = RedisStore(host=\"localhost\")\n\nif not store.ping():\n    raise RuntimeError(\"Cannot connect to Redis\")\n</code></pre>"},{"location":"user-guide/online-stores/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/online-stores/#connection-refused","title":"Connection Refused","text":"<pre><code>redis.exceptions.ConnectionError: Error connecting to localhost:6379\n</code></pre> <p>Solution: Ensure Redis is running: <pre><code>docker ps | grep redis\n# or\nredis-cli ping\n</code></pre></p>"},{"location":"user-guide/online-stores/#authentication-error","title":"Authentication Error","text":"<pre><code>redis.exceptions.AuthenticationError: Authentication required\n</code></pre> <p>Solution: Provide password: <pre><code>store = RedisStore(host=\"localhost\", password=\"your-password\")\n</code></pre></p>"},{"location":"user-guide/online-stores/#memory-issues","title":"Memory Issues","text":"<p>If Redis runs out of memory:</p> <ol> <li>Set TTL to expire old features</li> <li>Use Redis persistence (<code>RDB</code> or <code>AOF</code>)</li> <li>Scale Redis (Redis Cluster for large deployments)</li> </ol>"},{"location":"user-guide/online-stores/#next-steps","title":"Next Steps","text":"<ul> <li>Retrieving Features - Use <code>get_online_features()</code></li> <li>Building Features - Build to online store</li> <li>API Reference - RedisStore API documentation</li> </ul>"},{"location":"user-guide/point-in-time/","title":"Point-in-Time Correctness","text":"<p>Point-in-time (PIT) correctness ensures that features used for training or prediction reflect only the data available at a specific moment in time. This prevents data leakage, where future information accidentally influences model training.</p>"},{"location":"user-guide/point-in-time/#the-problem-data-leakage","title":"The Problem: Data Leakage","text":"<p>Consider a fraud detection model trained on historical transactions:</p> <pre><code># WRONG - causes data leakage\ntransactions = pl.read_parquet(\"data/transactions.parquet\")\nuser_stats = pl.read_parquet(\"features/user_total_spend.parquet\")\n\n# Standard join uses ALL user data\ntraining_data = transactions.join(user_stats, on=\"user_id\")\n</code></pre> <p>Problem: For a transaction on 2024-01-05, the joined <code>user_total_spend</code> includes transactions from 2024-01-06, 2024-01-07, etc. The model learns from future data it wouldn't have during real-time inference.</p>"},{"location":"user-guide/point-in-time/#the-solution-asof-joins","title":"The Solution: Asof Joins","text":"<p>An asof join matches each entity to the most recent feature value available at or before the entity's timestamp:</p> <pre><code># CORRECT - point-in-time correct\nfrom mlforge import get_training_data\n\ntraining_data = get_training_data(\n    features=[\"user_spend_mean_30d\"],\n    entity_df=transactions,\n    timestamp=\"transaction_time\"  # Enables asof join\n)\n</code></pre> <p>Now for a transaction on 2024-01-05, <code>user_spend_mean_30d</code> reflects only data up to 2024-01-05.</p>"},{"location":"user-guide/point-in-time/#temporal-features","title":"Temporal Features","text":"<p>To enable PIT correctness, features must include a timestamp column.</p>"},{"location":"user-guide/point-in-time/#defining-temporal-features","title":"Defining Temporal Features","text":"<p>Use the <code>timestamp</code> parameter in <code>@feature</code>:</p> <pre><code>from mlforge import feature\nimport polars as pl\n\n@feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"feature_timestamp\",  # Enables PIT joins\n    description=\"User mean spend over 30-day windows\"\n)\ndef user_spend_mean_30d(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df\n        .with_columns(\n            pl.col(\"trans_date_trans_time\")\n            .str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n            .alias(\"trans_dt\")\n        )\n        .sort(\"trans_dt\")\n        .group_by_dynamic(\n            \"trans_dt\",\n            every=\"1d\",      # Daily aggregation\n            period=\"30d\",    # 30-day rolling window\n            by=\"user_id\"\n        )\n        .agg(pl.col(\"amt\").mean().alias(\"user_spend_mean_30d\"))\n        .rename({\"trans_dt\": \"feature_timestamp\"})  # IMPORTANT\n    )\n</code></pre> <p>Key points:</p> <ol> <li>Declare <code>timestamp=\"feature_timestamp\"</code> in the decorator</li> <li>Output DataFrame must have a <code>feature_timestamp</code> column</li> <li>Values in <code>feature_timestamp</code> indicate when each feature value became available</li> </ol>"},{"location":"user-guide/point-in-time/#timestamp-convention","title":"Timestamp Convention","text":"<p>mlforge uses <code>feature_timestamp</code> as the standard column name. Always rename your time column:</p> <pre><code>.rename({\"event_time\": \"feature_timestamp\"})\n</code></pre> <p>This ensures <code>get_training_data()</code> correctly identifies the timestamp for asof joins.</p>"},{"location":"user-guide/point-in-time/#using-point-in-time-joins","title":"Using Point-in-Time Joins","text":""},{"location":"user-guide/point-in-time/#step-1-materialize-temporal-features","title":"Step 1: Materialize Temporal Features","text":"<p>Build features with timestamps:</p> <pre><code>mlforge build definitions.py\n</code></pre>"},{"location":"user-guide/point-in-time/#step-2-prepare-entity-data","title":"Step 2: Prepare Entity Data","text":"<p>Ensure your entity DataFrame has a timestamp column:</p> <pre><code>import polars as pl\n\ntransactions = pl.read_parquet(\"data/transactions.parquet\")\n\n# Convert to datetime if needed\ntransactions = transactions.with_columns(\n    pl.col(\"trans_date_trans_time\")\n    .str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n    .alias(\"transaction_time\")\n)\n</code></pre>"},{"location":"user-guide/point-in-time/#step-3-retrieve-with-point-in-time-correctness","title":"Step 3: Retrieve with Point-in-Time Correctness","text":"<p>Pass the <code>timestamp</code> parameter:</p> <pre><code>from mlforge import get_training_data\n\ntraining_data = get_training_data(\n    features=[\"user_spend_mean_30d\"],\n    entity_df=transactions,\n    timestamp=\"transaction_time\"  # Enables asof join\n)\n</code></pre>"},{"location":"user-guide/point-in-time/#how-asof-joins-work","title":"How Asof Joins Work","text":"<p>Given:</p> <ul> <li> <p>Entity DataFrame (transactions):   <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 transaction_time     \u2502 label \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 2024-01-05 10:00:00  \u2502 0     \u2502\n\u2502 u1      \u2502 2024-01-15 14:00:00  \u2502 1     \u2502\n\u2502 u2      \u2502 2024-01-10 09:00:00  \u2502 0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> </li> <li> <p>Feature DataFrame (user_spend_mean_30d):   <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 feature_timestamp    \u2502 user_spend_mean_30d\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 2024-01-01 00:00:00  \u2502 20.0               \u2502\n\u2502 u1      \u2502 2024-01-10 00:00:00  \u2502 25.0               \u2502\n\u2502 u1      \u2502 2024-01-20 00:00:00  \u2502 30.0               \u2502\n\u2502 u2      \u2502 2024-01-05 00:00:00  \u2502 15.0               \u2502\n\u2502 u2      \u2502 2024-01-15 00:00:00  \u2502 18.0               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> </li> </ul> <p>The asof join produces:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id \u2502 transaction_time     \u2502 label \u2502 user_spend_mean_30d\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u1      \u2502 2024-01-05 10:00:00  \u2502 0     \u2502 20.0               \u2502  \u2190 2024-01-01 feature\n\u2502 u1      \u2502 2024-01-15 14:00:00  \u2502 1     \u2502 25.0               \u2502  \u2190 2024-01-10 feature\n\u2502 u2      \u2502 2024-01-10 09:00:00  \u2502 0     \u2502 15.0               \u2502  \u2190 2024-01-05 feature\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each transaction gets the most recent feature value available before its timestamp.</p>"},{"location":"user-guide/point-in-time/#mixed-features","title":"Mixed Features","text":"<p>You can mix temporal and non-temporal features in a single retrieval:</p> <pre><code>training_data = get_training_data(\n    features=[\n        \"user_age\",             # Non-temporal (standard join)\n        \"user_spend_mean_30d\",  # Temporal (asof join)\n    ],\n    entity_df=transactions,\n    timestamp=\"transaction_time\"\n)\n</code></pre> <ul> <li><code>user_age</code>: Joined with standard left join on <code>user_id</code></li> <li><code>user_spend_mean_30d</code>: Joined with asof join on <code>user_id</code> and <code>transaction_time</code></li> </ul>"},{"location":"user-guide/point-in-time/#complete-example","title":"Complete Example","text":"<pre><code># features.py\nfrom mlforge import feature, entity_key\nimport polars as pl\n\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n@feature(\n    keys=[\"user_id\"],\n    source=\"data/transactions.parquet\",\n    timestamp=\"feature_timestamp\",\n    description=\"User 7-day rolling average spend\"\n)\ndef user_spend_mean_7d(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return (\n        df\n        .pipe(with_user_id)\n        .with_columns(\n            pl.col(\"trans_date_trans_time\")\n            .str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n            .alias(\"trans_dt\")\n        )\n        .sort(\"trans_dt\")\n        .group_by_dynamic(\n            \"trans_dt\",\n            every=\"1d\",\n            period=\"7d\",\n            by=\"user_id\"\n        )\n        .agg(pl.col(\"amt\").mean().alias(\"user_spend_mean_7d\"))\n        .rename({\"trans_dt\": \"feature_timestamp\"})\n    )\n</code></pre> <pre><code># definitions.py\nfrom mlforge import Definitions, LocalStore\nimport features\n\ndefs = Definitions(\n    name=\"fraud-detection\",\n    features=[features],\n    offline_store=LocalStore(\"./feature_store\")\n)\n</code></pre> <pre><code># Build features\nmlforge build definitions.py\n</code></pre> <pre><code># train.py\nfrom mlforge import get_training_data, entity_key\nimport polars as pl\n\n# Load and prepare entity data\ntransactions = (\n    pl.read_parquet(\"data/transactions.parquet\")\n    .with_columns(\n        pl.col(\"trans_date_trans_time\")\n        .str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n        .alias(\"transaction_time\")\n    )\n)\n\n# Define entity transform\nwith_user_id = entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# Retrieve features with PIT correctness\ntraining_data = get_training_data(\n    features=[\"user_spend_mean_7d\"],\n    entity_df=transactions,\n    entities=[with_user_id],\n    timestamp=\"transaction_time\"\n)\n\n# Train model\nX = training_data.select(\"user_spend_mean_7d\")\ny = training_data.select(\"is_fraud\")\n</code></pre>"},{"location":"user-guide/point-in-time/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"user-guide/point-in-time/#1-forgetting-to-rename-timestamp","title":"1. Forgetting to Rename Timestamp","text":"<pre><code># WRONG - timestamp not renamed\n@feature(\n    keys=[\"user_id\"],\n    source=\"data/events.parquet\",\n    timestamp=\"feature_timestamp\"\n)\ndef user_event_count(df):\n    return (\n        df\n        .group_by_dynamic(\"event_time\", every=\"1d\", by=\"user_id\")\n        .agg(pl.col(\"event_id\").count())\n        # Missing: .rename({\"event_time\": \"feature_timestamp\"})\n    )\n</code></pre> <p>Fix: Always rename to <code>feature_timestamp</code>:</p> <pre><code>.rename({\"event_time\": \"feature_timestamp\"})\n</code></pre>"},{"location":"user-guide/point-in-time/#2-timestamp-type-mismatch","title":"2. Timestamp Type Mismatch","text":"<pre><code># Entity timestamp is String\nentities = pl.DataFrame({\n    \"user_id\": [\"u1\"],\n    \"event_time\": [\"2024-01-05 10:00:00\"]  # String type\n})\n\n# Feature timestamp is Datetime\n# Asof join will fail!\n</code></pre> <p>Fix: Convert to datetime before retrieval:</p> <pre><code>entities = entities.with_columns(\n    pl.col(\"event_time\").str.to_datetime().alias(\"event_time\")\n)\n</code></pre>"},{"location":"user-guide/point-in-time/#3-not-sorting-data","title":"3. Not Sorting Data","text":"<p>Asof joins require sorted data. mlforge handles this automatically, but if you see warnings:</p> <pre><code>UserWarning: Sortedness of columns cannot be checked\n</code></pre> <p>It's safe to ignore\u2014mlforge sorts internally.</p>"},{"location":"user-guide/point-in-time/#4-using-future-data","title":"4. Using Future Data","text":"<p>Ensure feature timestamps don't include future events:</p> <pre><code># WRONG - includes future transactions\n.group_by_dynamic(\"trans_dt\", every=\"1d\", period=\"30d\", by=\"user_id\")\n\n# Event on 2024-01-05 computes features from 2024-01-05 to 2024-02-04\n# This includes future data!\n</code></pre> <p>Fix: Use past-looking windows:</p> <pre><code># Compute trailing 30 days BEFORE each date\n.rolling(\n    index_column=\"trans_dt\",\n    period=\"30d\",\n    closed=\"left\",  # Exclude current day\n    by=\"user_id\"\n)\n</code></pre>"},{"location":"user-guide/point-in-time/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/point-in-time/#1-always-use-timestamps-for-time-series-features","title":"1. Always Use Timestamps for Time-Series Features","text":"<p>If your feature uses temporal data, declare a timestamp:</p> <pre><code>@feature(\n    keys=[\"user_id\"],\n    source=\"data/events.parquet\",\n    timestamp=\"feature_timestamp\"  # Required for temporal features\n)\n</code></pre>"},{"location":"user-guide/point-in-time/#2-test-for-data-leakage","title":"2. Test for Data Leakage","text":"<p>Verify features don't include future data:</p> <pre><code># Check feature values\nfeature_df = store.read(\"user_spend_mean_30d\")\n\n# For each row, verify feature_timestamp &lt;= latest event used\n</code></pre>"},{"location":"user-guide/point-in-time/#3-use-consistent-datetime-format","title":"3. Use Consistent Datetime Format","text":"<p>Convert timestamps to datetime early:</p> <pre><code>df = (\n    pl.read_parquet(\"data/events.parquet\")\n    .with_columns(\n        pl.col(\"event_time\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n    )\n)\n</code></pre>"},{"location":"user-guide/point-in-time/#4-document-temporal-logic","title":"4. Document Temporal Logic","text":"<p>Add comments explaining window logic:</p> <pre><code># Compute 7-day trailing average (excluding current day)\n.rolling(index_column=\"date\", period=\"7d\", closed=\"left\", by=\"user_id\")\n</code></pre>"},{"location":"user-guide/point-in-time/#next-steps","title":"Next Steps","text":"<ul> <li>Retrieving Features - Complete retrieval guide</li> <li>Building Features - Materialize temporal features</li> <li>API Reference - Retrieval - Detailed API docs</li> </ul>"},{"location":"user-guide/retrieving-features/","title":"Retrieving Features","text":"<p>mlforge provides two retrieval functions for different use cases:</p> Function Use Case Store Type Point-in-Time <code>get_training_data()</code> Training, batch scoring Offline (LocalStore, S3Store) Yes <code>get_online_features()</code> Real-time inference Online (RedisStore) No (latest only)"},{"location":"user-guide/retrieving-features/#offline-retrieval-training","title":"Offline Retrieval (Training)","text":"<p>Use <code>get_training_data()</code> to join features to your entity DataFrame for training or batch scoring.</p>"},{"location":"user-guide/retrieving-features/#basic-usage","title":"Basic Usage","text":"<pre><code>import mlforge as mlf\nimport polars as pl\n\n# Load your entity data (e.g., labels, predictions)\nentities = pl.read_parquet(\"data/labels.parquet\")\n\n# Get features joined to entities\ntraining_data = mlf.get_training_data(\n    features=[\"user_total_spend\", \"user_avg_spend\"],\n    entity_df=entities\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#function-signature","title":"Function Signature","text":"<pre><code>def get_training_data(\n    features: list[str],\n    entity_df: pl.DataFrame,\n    store: str | Path | Store = \"./feature_store\",\n    entities: list[EntityKeyTransform] | None = None,\n    timestamp: str | None = None,\n) -&gt; pl.DataFrame\n</code></pre>"},{"location":"user-guide/retrieving-features/#parameters","title":"Parameters","text":""},{"location":"user-guide/retrieving-features/#features-required","title":"features (required)","text":"<p>List of feature names to retrieve. Must match the names of built features.</p> <pre><code>training_data = mlf.get_training_data(\n    features=[\"user_age\", \"user_tenure_days\"],\n    entity_df=entities\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#entity_df-required","title":"entity_df (required)","text":"<p>DataFrame containing entity keys and optionally timestamps. This is typically your:</p> <ul> <li>Training labels</li> <li>Prediction entities</li> <li>Evaluation dataset</li> </ul> <pre><code>entities = pl.DataFrame({\n    \"user_id\": [\"u1\", \"u2\", \"u3\"],\n    \"label\": [0, 1, 0]\n})\n</code></pre>"},{"location":"user-guide/retrieving-features/#store-optional","title":"store (optional)","text":"<p>Path to feature store or a <code>Store</code> instance. Defaults to <code>\"./feature_store\"</code>.</p> <pre><code>import mlforge as mlf\n\n# Using default path\ntraining_data = mlf.get_training_data(\n    features=[\"user_age\"],\n    entity_df=entities\n)\n\n# Custom path\ntraining_data = mlf.get_training_data(\n    features=[\"user_age\"],\n    entity_df=entities,\n    store=\"./my_features\"\n)\n\n# Store instance\nstore = mlf.LocalStore(\"./my_features\")\ntraining_data = mlf.get_training_data(\n    features=[\"user_age\"],\n    entity_df=entities,\n    store=store\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#entities-optional","title":"entities (optional)","text":"<p>List of entity key transforms to apply to <code>entity_df</code> before joining. Use this when your entity DataFrame doesn't have the required keys.</p> <pre><code>import mlforge as mlf\n\n# Define transform\nwith_user_id = mlf.entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# Apply during retrieval\ntraining_data = mlf.get_training_data(\n    features=[\"user_spend_stats\"],\n    entity_df=raw_entities,\n    entities=[with_user_id]  # Adds user_id column\n)\n</code></pre> <p>See Entity Keys for details.</p>"},{"location":"user-guide/retrieving-features/#timestamp-optional","title":"timestamp (optional)","text":"<p>Column name in <code>entity_df</code> to use for point-in-time joins. When specified, features with timestamps are joined using asof joins.</p> <pre><code>training_data = mlf.get_training_data(\n    features=[\"user_spend_mean_30d\"],\n    entity_df=transactions,\n    timestamp=\"transaction_time\"  # Point-in-time correct\n)\n</code></pre> <p>See Point-in-Time Correctness for details.</p>"},{"location":"user-guide/retrieving-features/#join-behavior","title":"Join Behavior","text":""},{"location":"user-guide/retrieving-features/#standard-joins","title":"Standard Joins","text":"<p>When <code>timestamp</code> is not specified, features are joined using standard left joins:</p> <pre><code>entities = pl.DataFrame({\n    \"user_id\": [\"u1\", \"u2\", \"u3\"],\n    \"label\": [0, 1, 0]\n})\n\ntraining_data = mlf.get_training_data(\n    features=[\"user_total_spend\"],\n    entity_df=entities\n)\n\n# Joins on common columns (user_id)\n</code></pre>"},{"location":"user-guide/retrieving-features/#point-in-time-joins","title":"Point-in-Time Joins","text":"<p>When <code>timestamp</code> is specified and features have timestamps, asof joins are used:</p> <pre><code>transactions = pl.DataFrame({\n    \"user_id\": [\"u1\", \"u1\", \"u2\"],\n    \"transaction_time\": [\"2024-01-05\", \"2024-01-15\", \"2024-01-10\"],\n    \"label\": [0, 1, 0]\n})\n\ntraining_data = mlf.get_training_data(\n    features=[\"user_spend_mean_30d\"],  # Has feature_timestamp\n    entity_df=transactions,\n    timestamp=\"transaction_time\"  # Asof join\n)\n\n# Features reflect data available at each transaction_time\n</code></pre>"},{"location":"user-guide/retrieving-features/#join-key-detection","title":"Join Key Detection","text":"<p>Join keys are automatically detected from common columns:</p> <pre><code># entity_df has: user_id, merchant_id, label\n# feature has: user_id, merchant_id, total_spend\n\n# Joins on: user_id, merchant_id\ntraining_data = mlf.get_training_data(\n    features=[\"user_merchant_spend\"],\n    entity_df=entities\n)\n</code></pre> <p>Timestamp columns are excluded from join keys when performing asof joins.</p>"},{"location":"user-guide/retrieving-features/#complete-example","title":"Complete Example","text":"<pre><code>import mlforge as mlf\nimport polars as pl\n\n# 1. Load entity data\ntransactions = pl.read_parquet(\"data/transactions.parquet\")\n\n# 2. Define entity transform\nwith_user_id = mlf.entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# 3. Retrieve features with point-in-time correctness\ntraining_data = mlf.get_training_data(\n    features=[\"user_spend_mean_30d\", \"user_total_spend\"],\n    entity_df=transactions,\n    entities=[with_user_id],\n    timestamp=\"trans_date_trans_time\",\n    store=\"./feature_store\"\n)\n\n# 4. Use in training\nfrom sklearn.ensemble import RandomForestClassifier\n\nX = training_data.select([\"user_spend_mean_30d\", \"user_total_spend\"])\ny = training_data.select(\"label\")\n\nmodel = RandomForestClassifier()\nmodel.fit(X.to_pandas(), y.to_pandas())\n</code></pre>"},{"location":"user-guide/retrieving-features/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/retrieving-features/#missing-features","title":"Missing Features","text":"<p>If a requested feature hasn't been built:</p> <pre><code>import mlforge as mlf\n\ntry:\n    training_data = mlf.get_training_data(\n        features=[\"nonexistent_feature\"],\n        entity_df=entities\n    )\nexcept ValueError as e:\n    print(e)\n    # Feature 'nonexistent_feature' not found. Run `mlforge build` first.\n</code></pre> <p>Build the feature first:</p> <pre><code>mlforge build\n</code></pre>"},{"location":"user-guide/retrieving-features/#missing-join-keys","title":"Missing Join Keys","text":"<p>If entity_df and features don't share common columns:</p> <pre><code># entity_df has: customer_id, label\n# feature has: user_id, total_spend\n\ntry:\n    training_data = mlf.get_training_data(\n        features=[\"user_total_spend\"],\n        entity_df=entities\n    )\nexcept ValueError as e:\n    print(e)\n    # No common columns to join 'user_total_spend'.\n</code></pre> <p>Solution: Use entity transforms to add required keys:</p> <pre><code>with_user_id = mlf.entity_key(\"customer_id\", alias=\"user_id\")\n\ntraining_data = mlf.get_training_data(\n    features=[\"user_total_spend\"],\n    entity_df=entities,\n    entities=[with_user_id]\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#timestamp-type-mismatch","title":"Timestamp Type Mismatch","text":"<p>For asof joins, timestamp columns must have matching data types:</p> <pre><code># entity_df[\"event_time\"] is String\n# feature[\"feature_timestamp\"] is Datetime\n\ntry:\n    training_data = mlf.get_training_data(\n        features=[\"temporal_feature\"],\n        entity_df=entities,\n        timestamp=\"event_time\"\n    )\nexcept ValueError as e:\n    print(e)\n    # Timestamp dtype mismatch: entity_df['event_time'] is String,\n    # but feature has Datetime.\n</code></pre> <p>Solution: Convert timestamps before calling <code>mlf.get_training_data()</code>:</p> <pre><code>entities = entities.with_columns(\n    pl.col(\"event_time\").str.to_datetime().alias(\"event_time\")\n)\n\ntraining_data = mlf.get_training_data(\n    features=[\"temporal_feature\"],\n    entity_df=entities,\n    timestamp=\"event_time\"\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#multiple-feature-stores","title":"Multiple Feature Stores","text":"<p>You can retrieve features from different stores by calling <code>mlf.get_training_data()</code> multiple times:</p> <pre><code>import mlforge as mlf\n\n# Features from store A\ntraining_data = mlf.get_training_data(\n    features=[\"user_age\", \"user_tenure\"],\n    entity_df=entities,\n    store=\"./store_a\"\n)\n\n# Add features from store B\ntraining_data = mlf.get_training_data(\n    features=[\"user_spend_stats\"],\n    entity_df=training_data,\n    store=\"./store_b\"\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/retrieving-features/#1-convert-timestamps-early","title":"1. Convert Timestamps Early","text":"<p>Always convert timestamp columns to proper datetime types before retrieval:</p> <pre><code>entities = (\n    pl.read_parquet(\"data/labels.parquet\")\n    .with_columns(\n        pl.col(\"event_time\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n    )\n)\n\ntraining_data = mlf.get_training_data(\n    features=[\"temporal_features\"],\n    entity_df=entities,\n    timestamp=\"event_time\"\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#2-use-type-hints","title":"2. Use Type Hints","text":"<p>Add type hints for clarity:</p> <pre><code>import polars as pl\nimport mlforge as mlf\n\nentities: pl.DataFrame = pl.read_parquet(\"data/labels.parquet\")\n\ntraining_data: pl.DataFrame = mlf.get_training_data(\n    features=[\"user_age\"],\n    entity_df=entities\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#3-verify-features-exist","title":"3. Verify Features Exist","text":"<p>Check built features before retrieval:</p> <pre><code>mlforge list\n</code></pre>"},{"location":"user-guide/retrieving-features/#4-handle-missing-values","title":"4. Handle Missing Values","text":"<p>Features may have nulls for entities not in the feature source:</p> <pre><code>training_data = mlf.get_training_data(\n    features=[\"user_total_spend\"],\n    entity_df=entities\n)\n\n# Fill nulls if needed\ntraining_data = training_data.with_columns(\n    pl.col(\"user_total_spend\").fill_null(0)\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#online-retrieval-inference","title":"Online Retrieval (Inference)","text":"<p>For real-time inference, use <code>get_online_features()</code> to retrieve features from an online store like Redis.</p>"},{"location":"user-guide/retrieving-features/#basic-usage_1","title":"Basic Usage","text":"<pre><code>import mlforge as mlf\nfrom mlforge.online import RedisStore\nimport polars as pl\n\n# Connect to Redis\nstore = RedisStore(host=\"localhost\", port=6379)\n\n# Define entity transform (same as training)\nwith_user_id = mlf.entity_key(\"first\", \"last\", \"dob\", alias=\"user_id\")\n\n# Inference request\nrequest_df = pl.DataFrame({\n    \"request_id\": [\"req_001\", \"req_002\"],\n    \"first\": [\"John\", \"Jane\"],\n    \"last\": [\"Doe\", \"Smith\"],\n    \"dob\": [\"1990-01-15\", \"1985-06-20\"],\n})\n\n# Retrieve features\nfeatures_df = mlf.get_online_features(\n    features=[\"user_spend\"],\n    entity_df=request_df,\n    store=store,\n    entities=[with_user_id],\n)\n</code></pre>"},{"location":"user-guide/retrieving-features/#function-signature_1","title":"Function Signature","text":"<pre><code>def get_online_features(\n    features: list[str],\n    entity_df: pl.DataFrame,\n    store: OnlineStore,\n    entities: list[EntityKeyTransform] | None = None,\n) -&gt; pl.DataFrame\n</code></pre>"},{"location":"user-guide/retrieving-features/#key-differences-from-training-retrieval","title":"Key Differences from Training Retrieval","text":"Aspect <code>get_training_data()</code> <code>get_online_features()</code> Versioning Supports <code>(\"feature\", \"1.0.0\")</code> No versioning (latest only) Point-in-time Uses <code>timestamp</code> parameter Not applicable Store parameter Path string or Store instance OnlineStore instance required Missing entities Returns null Returns null"},{"location":"user-guide/retrieving-features/#prerequisites","title":"Prerequisites","text":"<p>Before using online retrieval:</p> <ol> <li> <p>Configure online store in your definitions:    <pre><code>from mlforge.online import RedisStore\n\ndefs = mlf.Definitions(\n    name=\"my-project\",\n    features=[user_spend],\n    offline_store=mlf.LocalStore(\"./feature_store\"),\n    online_store=RedisStore(host=\"localhost\"),\n)\n</code></pre></p> </li> <li> <p>Build to online store:    <pre><code>mlforge build --online\n</code></pre></p> </li> <li> <p>Ensure Redis is running:    <pre><code>docker run -d -p 6379:6379 redis:7-alpine\n</code></pre></p> </li> </ol> <p>See Online Stores for detailed setup instructions.</p>"},{"location":"user-guide/retrieving-features/#next-steps","title":"Next Steps","text":"<ul> <li>Entity Keys - Work with surrogate keys and transforms</li> <li>Point-in-Time Correctness - Understand temporal joins</li> <li>Online Stores - Set up Redis for real-time serving</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"user-guide/storage-backends/","title":"Storage Backends","text":"<p>mlforge supports multiple storage backends for persisting built features. Choose the backend that best fits your deployment environment.</p>"},{"location":"user-guide/storage-backends/#localstore","title":"LocalStore","text":"<p>The default storage backend that writes features to the local filesystem as Parquet files.</p>"},{"location":"user-guide/storage-backends/#basic-usage","title":"Basic Usage","text":"<pre><code>import mlforge as mlf\nimport features\n\ndefs = mlf.Definitions(\n    name=\"my-project\",\n    features=[features],\n    offline_store=mlf.LocalStore(path=\"./feature_store\")\n)\n</code></pre>"},{"location":"user-guide/storage-backends/#configuration","title":"Configuration","text":"<pre><code>import mlforge as mlf\nfrom pathlib import Path\n\n# Using string path\nstore = mlf.LocalStore(path=\"./feature_store\")\n\n# Using Path object\nstore = mlf.LocalStore(path=Path(\"./feature_store\"))\n\n# Custom location\nstore = mlf.LocalStore(path=Path.home() / \"ml_projects\" / \"features\")\n</code></pre>"},{"location":"user-guide/storage-backends/#storage-format","title":"Storage Format","text":"<p>Features are stored in a versioned directory structure (since v0.5.0):</p> <pre><code>feature_store/\n\u251c\u2500\u2500 user_total_spend/\n\u2502   \u251c\u2500\u2500 1.0.0/\n\u2502   \u2502   \u251c\u2500\u2500 data.parquet          # Feature data\n\u2502   \u2502   \u2514\u2500\u2500 .meta.json             # Version metadata\n\u2502   \u251c\u2500\u2500 1.0.1/\n\u2502   \u2502   \u251c\u2500\u2500 data.parquet\n\u2502   \u2502   \u2514\u2500\u2500 .meta.json\n\u2502   \u251c\u2500\u2500 _latest.json               # Pointer to latest version\n\u2502   \u2514\u2500\u2500 .gitignore                 # Auto-generated (ignores data.parquet)\n\u251c\u2500\u2500 user_avg_spend/\n\u2502   \u251c\u2500\u2500 1.0.0/\n\u2502   \u2502   \u251c\u2500\u2500 data.parquet\n\u2502   \u2502   \u2514\u2500\u2500 .meta.json\n\u2502   \u251c\u2500\u2500 _latest.json\n\u2502   \u2514\u2500\u2500 .gitignore\n\u2514\u2500\u2500 product_popularity/\n    \u251c\u2500\u2500 1.0.0/\n        \u251c\u2500\u2500 data.parquet\n        \u2514\u2500\u2500 .meta.json\n    \u251c\u2500\u2500 _latest.json\n    \u2514\u2500\u2500 .gitignore\n</code></pre> <p>Key files:</p> <ul> <li><code>data.parquet</code> - Materialized feature data</li> <li><code>.meta.json</code> - Metadata for this version (schema, config, hashes, timestamps)</li> <li><code>_latest.json</code> - Pointer to the latest version (e.g., <code>{\"version\": \"1.0.1\"}</code>)</li> <li><code>.gitignore</code> - Auto-generated file that ignores <code>*/data.parquet</code></li> </ul> <p>Versioning:</p> <ul> <li>Each build creates or updates a semantic version (e.g., <code>1.0.0</code>, <code>1.0.1</code>, <code>1.1.0</code>)</li> <li>Versions are automatically bumped based on detected changes:<ul> <li>MAJOR (2.0.0) - Columns removed or dtype changed</li> <li>MINOR (1.1.0) - Columns added or config changed</li> <li>PATCH (1.0.1) - Data refresh only</li> </ul> </li> <li>Use <code>--version X.Y.Z</code> to override automatic versioning</li> </ul> <p>Git Integration:</p> <p>LocalStore automatically creates <code>.gitignore</code> files to exclude data files:</p> <pre><code># Auto-generated by mlforge\n# Data files are rebuilt from source; commit .meta.json and _latest.json only\n*/data.parquet\n</code></pre> <p>This allows you to commit feature metadata to Git while excluding large data files that can be rebuilt from source using <code>mlforge sync</code>.</p>"},{"location":"user-guide/storage-backends/#when-to-use","title":"When to Use","text":"<ul> <li>Local development - Fast iteration and debugging</li> <li>Small datasets - Features fit on local disk</li> <li>Single machine deployments - No distributed infrastructure needed</li> <li>CI/CD pipelines - Ephemeral feature stores for testing</li> </ul>"},{"location":"user-guide/storage-backends/#s3store","title":"S3Store","text":"<p>Cloud storage backend for Amazon S3, supporting distributed access and production deployments.</p>"},{"location":"user-guide/storage-backends/#basic-usage_1","title":"Basic Usage","text":"<pre><code>import mlforge as mlf\nimport features\n\ndefs = mlf.Definitions(\n    name=\"my-project\",\n    features=[features],\n    offline_store=mlf.S3Store(\n        bucket=\"mlforge-features\",\n        prefix=\"prod/features\"\n    )\n)\n</code></pre>"},{"location":"user-guide/storage-backends/#configuration_1","title":"Configuration","text":"<pre><code>import mlforge as mlf\n\n# With prefix (recommended for organization)\nstore = mlf.S3Store(\n    bucket=\"my-bucket\",\n    prefix=\"prod/features\"  # Features stored at s3://my-bucket/prod/features/\n)\n\n# Without prefix (bucket root)\nstore = mlf.S3Store(\n    bucket=\"my-bucket\",\n    prefix=\"\"  # Features stored at s3://my-bucket/\n)\n\n# With explicit region\nstore = mlf.S3Store(\n    bucket=\"my-bucket\",\n    prefix=\"prod/features\",\n    region=\"us-west-2\"\n)\n</code></pre>"},{"location":"user-guide/storage-backends/#aws-credentials","title":"AWS Credentials","text":"<p>S3Store uses standard AWS credential resolution:</p> Environment VariablesAWS CLI ConfigurationIAM Role (EC2/ECS/Lambda) <pre><code>export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre> <pre><code>aws configure\n# AWS Access Key ID: AKIAIOSFODNN7EXAMPLE\n# AWS Secret Access Key: ****\n# Default region name: us-east-1\n# Default output format: json\n</code></pre> <p>When running on AWS services, use IAM roles instead of static credentials:</p> <pre><code>import mlforge as mlf\n\n# No credentials needed - uses instance/task role\nstore = mlf.S3Store(bucket=\"mlforge-features\", prefix=\"prod\")\n</code></pre>"},{"location":"user-guide/storage-backends/#storage-format_1","title":"Storage Format","text":"<p>Features are stored in S3 with the same Parquet format:</p> <pre><code>s3://mlforge-features/prod/features/\n\u251c\u2500\u2500 user_total_spend.parquet\n\u251c\u2500\u2500 user_avg_spend.parquet\n\u2514\u2500\u2500 product_popularity.parquet\n</code></pre>"},{"location":"user-guide/storage-backends/#iam-policy","title":"IAM Policy","text":"<p>Your AWS credentials need appropriate S3 permissions. Here's a minimal IAM policy:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"mlforgeFeatureStoreAccess\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::mlforge-features\",\n        \"arn:aws:s3:::mlforge-features/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/storage-backends/#read-only-access","title":"Read-Only Access","text":"<p>For production environments where only feature retrieval is needed:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"mlforgeFeatureStoreReadOnly\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::mlforge-features\",\n        \"arn:aws:s3:::mlforge-features/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/storage-backends/#write-only-access","title":"Write-Only Access","text":"<p>For feature build pipelines that only write features:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"mlforgeFeatureStoreWriteOnly\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::mlforge-features\",\n        \"arn:aws:s3:::mlforge-features/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/storage-backends/#prefix-based-access-control","title":"Prefix-Based Access Control","text":"<p>Restrict access to specific prefixes (e.g., <code>prod/</code> vs <code>dev/</code>):</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"mlforgeProductionAccess\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::mlforge-features\",\n        \"arn:aws:s3:::mlforge-features/prod/*\"\n      ],\n      \"Condition\": {\n        \"StringLike\": {\n          \"s3:prefix\": [\"prod/*\"]\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/storage-backends/#when-to-use_1","title":"When to Use","text":"<ul> <li>Production deployments - Centralized, durable storage</li> <li>Team collaboration - Shared feature store across multiple users</li> <li>Large datasets - Features too large for local disk</li> <li>Multi-environment workflows - Separate dev/staging/prod prefixes</li> <li>CI/CD pipelines - Build features in one environment, use in another</li> </ul>"},{"location":"user-guide/storage-backends/#error-handling","title":"Error Handling","text":"<pre><code>import mlforge as mlf\n\ntry:\n    store = mlf.S3Store(bucket=\"nonexistent-bucket\", prefix=\"features\")\nexcept ValueError as e:\n    print(f\"Bucket error: {e}\")\n    # Bucket 'nonexistent-bucket' does not exist or is not accessible.\n    # Ensure the bucket is created and credentials have appropriate permissions.\n</code></pre> <p>Common issues:</p> <ol> <li>Bucket doesn't exist - Create the bucket first using AWS Console or CLI</li> <li>Missing permissions - Verify IAM policy allows required S3 actions</li> <li>Wrong region - Specify <code>region</code> parameter if bucket is in non-default region</li> <li>Invalid credentials - Check AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY</li> </ol>"},{"location":"user-guide/storage-backends/#complete-example","title":"Complete Example","text":""},{"location":"user-guide/storage-backends/#local-development","title":"Local Development","text":"<pre><code>import mlforge as mlf\nimport polars as pl\n\n@mlf.feature(keys=[\"user_id\"], source=\"data/transactions.parquet\")\ndef user_total_spend(df):\n    return df.group_by(\"user_id\").agg(\n        pl.col(\"amount\").sum().alias(\"total_spend\")\n    )\n\n# Local development - fast iteration\ndefs = mlf.Definitions(\n    name=\"user-features\",\n    features=[user_total_spend],\n    offline_store=mlf.LocalStore(\"./dev_features\")\n)\n\ndefs.build()\n</code></pre>"},{"location":"user-guide/storage-backends/#production-deployment","title":"Production Deployment","text":"<pre><code>import mlforge as mlf\nimport polars as pl\nimport os\n\n@mlf.feature(keys=[\"user_id\"], source=\"s3://my-bucket/data/transactions.parquet\")\ndef user_total_spend(df):\n    return df.group_by(\"user_id\").agg(\n        pl.col(\"amount\").sum().alias(\"total_spend\")\n    )\n\n# Production - S3 storage\nenvironment = os.getenv(\"ENVIRONMENT\", \"dev\")\n\ndefs = mlf.Definitions(\n    name=\"user-features\",\n    features=[user_total_spend],\n    offline_store=mlf.S3Store(\n        bucket=\"mlforge-features\",\n        prefix=f\"{environment}/features\"  # dev/features or prod/features\n    )\n)\n\ndefs.build()\n</code></pre>"},{"location":"user-guide/storage-backends/#multi-environment-setup","title":"Multi-Environment Setup","text":"<pre><code>import mlforge as mlf\nimport os\n\ndef get_store():\n    \"\"\"Get appropriate store based on environment.\"\"\"\n    env = os.getenv(\"ENVIRONMENT\", \"local\")\n\n    if env == \"local\":\n        return mlf.LocalStore(\"./feature_store\")\n    elif env == \"dev\":\n        return mlf.S3Store(bucket=\"mlforge-features\", prefix=\"dev/features\")\n    elif env == \"staging\":\n        return mlf.S3Store(bucket=\"mlforge-features\", prefix=\"staging/features\")\n    elif env == \"prod\":\n        return mlf.S3Store(bucket=\"mlforge-features-prod\", prefix=\"features\")\n    else:\n        raise ValueError(f\"Unknown environment: {env}\")\n\ndefs = mlf.Definitions(\n    name=\"user-features\",\n    features=[...],\n    offline_store=get_store()\n)\n</code></pre> <p>Usage:</p> <pre><code># Local development\nENVIRONMENT=local python build_features.py\n\n# Dev environment\nENVIRONMENT=dev python build_features.py\n\n# Production\nENVIRONMENT=prod python build_features.py\n</code></pre>"},{"location":"user-guide/storage-backends/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/storage-backends/#localstore_1","title":"LocalStore","text":"<p>Pros: - Fastest for small datasets (no network overhead) - Simple setup (no credentials required) - Works offline</p> <p>Cons: - Limited by local disk space - Not suitable for distributed deployments - No built-in backup/versioning</p>"},{"location":"user-guide/storage-backends/#s3store_1","title":"S3Store","text":"<p>Pros: - Virtually unlimited storage - High durability (99.999999999%) - Built-in versioning (if enabled on bucket) - Accessible from anywhere</p> <p>Cons: - Network latency for read/write operations - Data transfer costs for large datasets - Requires AWS credentials</p>"},{"location":"user-guide/storage-backends/#optimization-tips","title":"Optimization Tips","text":"<p>For S3Store:</p> <ol> <li>Use same region - Colocate compute and S3 bucket to minimize latency</li> <li>Enable S3 Transfer Acceleration - For cross-region access</li> <li>Batch operations - Materialize multiple features in one run</li> <li>Use IAM roles - Avoid credential management overhead</li> </ol> <pre><code># Good: Batch materialization\ndefs.build()  # Builds all features\n\n# Less efficient: Individual feature builds\nfor feature_name in [\"feature1\", \"feature2\", \"feature3\"]:\n    defs.build(feature_names=[feature_name])\n</code></pre>"},{"location":"user-guide/storage-backends/#next-steps","title":"Next Steps","text":"<ul> <li>Building Features - Build features to storage</li> <li>Retrieving Features - Read features from storage</li> <li>Store API Reference - Detailed API documentation</li> </ul>"},{"location":"user-guide/validators/","title":"Feature Validation","text":"<p>mlforge provides a validation system to ensure data quality before features are materialized. Validators run on the output of your feature function, before any metrics are computed.</p>"},{"location":"user-guide/validators/#why-validate","title":"Why Validate?","text":"<p>Validation helps catch data quality issues early:</p> <ul> <li>Prevent bad data from entering your feature store</li> <li>Document data expectations in code</li> <li>Fail fast instead of discovering issues in production</li> <li>Track validation rules in metadata for auditing</li> </ul>"},{"location":"user-guide/validators/#using-built-in-validators","title":"Using Built-in Validators","text":"<p>mlforge includes common validators for typical data quality checks.</p>"},{"location":"user-guide/validators/#basic-example","title":"Basic Example","text":"<pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"merchant_id\"],\n    source=\"data/transactions.parquet\",\n    validators={\n        \"merchant_id\": [mlf.not_null()],\n        \"amount\": [mlf.not_null(), mlf.greater_than_or_equal(0)],\n    }\n)\ndef merchant_transactions(df):\n    return df.select([\"merchant_id\", \"amount\", \"transaction_date\"])\n</code></pre> <p>If validation fails, the build will stop and report which validations failed:</p> <pre><code>ERROR: Feature validation failed for merchant_transactions\n  - Column 'amount': 3 values &lt; 0 (greater_than_or_equal(0))\n</code></pre>"},{"location":"user-guide/validators/#available-validators","title":"Available Validators","text":""},{"location":"user-guide/validators/#null-checks","title":"Null Checks","text":"<pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"users.parquet\",\n    validators={\n        \"user_id\": [mlf.not_null()],\n        \"email\": [mlf.not_null()],\n    }\n)\ndef user_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#uniqueness","title":"Uniqueness","text":"<pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"users.parquet\",\n    validators={\n        \"user_id\": [mlf.unique()],  # Ensure no duplicate user IDs\n    }\n)\ndef user_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#numeric-comparisons","title":"Numeric Comparisons","text":"<pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"product_id\"],\n    source=\"products.parquet\",\n    validators={\n        \"price\": [mlf.greater_than(0)],\n        \"discount_pct\": [mlf.greater_than_or_equal(0), mlf.less_than_or_equal(100)],\n        \"stock\": [mlf.greater_than_or_equal(0)],\n    }\n)\ndef product_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#range-validation","title":"Range Validation","text":"<pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"users.parquet\",\n    validators={\n        \"age\": [mlf.in_range(18, 120)],  # inclusive by default\n        \"score\": [mlf.in_range(0, 100, inclusive=True)],\n    }\n)\ndef user_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#pattern-matching","title":"Pattern Matching","text":"<pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"users.parquet\",\n    validators={\n        \"email\": [mlf.matches_regex(r\"^\\w+@\\w+\\.\\w+$\")],\n        \"phone\": [mlf.matches_regex(r\"^\\+?1?\\d{9,15}$\")],\n    }\n)\ndef user_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#set-membership","title":"Set Membership","text":"<pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"transaction_id\"],\n    source=\"transactions.parquet\",\n    validators={\n        \"status\": [mlf.is_in([\"pending\", \"approved\", \"rejected\"])],\n        \"payment_method\": [mlf.is_in([\"card\", \"bank\", \"wallet\"])],\n    }\n)\ndef transaction_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#combining-validators","title":"Combining Validators","text":"<p>You can apply multiple validators to a single column:</p> <pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"users.parquet\",\n    validators={\n        \"age\": [\n            mlf.not_null(),           # Must have a value\n            mlf.greater_than(0),      # Must be positive\n            mlf.less_than(150),       # Must be reasonable\n        ],\n    }\n)\ndef user_features(df):\n    return df\n</code></pre> <p>Validators run in order. If any validator fails, validation stops and reports the failure.</p>"},{"location":"user-guide/validators/#creating-custom-validators","title":"Creating Custom Validators","text":"<p>You can create custom validators for domain-specific validation logic.</p>"},{"location":"user-guide/validators/#basic-custom-validator","title":"Basic Custom Validator","text":"<p>A validator is a function that returns a <code>Validator</code> object:</p> <pre><code>from mlforge.validators import Validator, ValidationResult\nimport polars as pl\n\ndef is_valid_email() -&gt; Validator:\n    \"\"\"Validate email addresses using custom logic.\"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        # Implement your validation logic\n        invalid_emails = series.filter(\n            ~series.str.contains(\"@\") | series.is_null()\n        )\n\n        if len(invalid_emails) &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{len(invalid_emails)} invalid email addresses\",\n                failed_count=len(invalid_emails),\n            )\n\n        return ValidationResult(passed=True)\n\n    return Validator(name=\"is_valid_email\", fn=validate)\n</code></pre> <p>Use it like any built-in validator:</p> <pre><code>import mlforge as mlf\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"users.parquet\",\n    validators={\n        \"email\": [is_valid_email()],\n    }\n)\ndef user_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#parameterized-custom-validators","title":"Parameterized Custom Validators","text":"<p>Create validators that accept parameters:</p> <pre><code>from mlforge.validators import Validator, ValidationResult\nimport polars as pl\n\ndef min_length(length: int) -&gt; Validator:\n    \"\"\"Validate that string values have minimum length.\"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        too_short = series.filter(series.str.lengths() &lt; length)\n\n        if len(too_short) &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{len(too_short)} values shorter than {length}\",\n                failed_count=len(too_short),\n            )\n\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"min_length({length})\", fn=validate)\n\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"users.parquet\",\n    validators={\n        \"username\": [min_length(3)],\n        \"password_hash\": [min_length(60)],  # bcrypt hashes are 60 chars\n    }\n)\ndef user_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#business-logic-validators","title":"Business Logic Validators","text":"<p>Implement complex business rules:</p> <pre><code>from mlforge.validators import Validator, ValidationResult\nimport polars as pl\n\ndef is_valid_transaction() -&gt; Validator:\n    \"\"\"\n    Validate transaction business rules.\n\n    - Amount must be &gt; 0\n    - Refunds (negative amounts) must have a parent transaction\n    - High-value transactions must have approval\n    \"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        # This validator works on the entire DataFrame row context\n        # For row-level validation, you'd need to implement custom logic\n\n        invalid = series.filter(series == 0)  # No zero-amount transactions\n\n        if len(invalid) &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{len(invalid)} transactions with zero amount\",\n                failed_count=len(invalid),\n            )\n\n        return ValidationResult(passed=True)\n\n    return Validator(name=\"is_valid_transaction\", fn=validate)\n</code></pre>"},{"location":"user-guide/validators/#statistical-validators","title":"Statistical Validators","text":"<p>Validate statistical properties:</p> <pre><code>from mlforge.validators import Validator, ValidationResult\nimport polars as pl\n\ndef within_std_devs(n_std: float) -&gt; Validator:\n    \"\"\"Validate values are within n standard deviations of mean.\"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        mean = series.mean()\n        std = series.std()\n\n        if std is None or mean is None:\n            return ValidationResult(passed=True)  # Skip if insufficient data\n\n        lower = mean - (n_std * std)\n        upper = mean + (n_std * std)\n\n        outliers = series.filter((series &lt; lower) | (series &gt; upper))\n\n        if len(outliers) &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{len(outliers)} outliers beyond {n_std} std devs\",\n                failed_count=len(outliers),\n            )\n\n        return ValidationResult(passed=True)\n\n    return Validator(name=f\"within_std_devs({n_std})\", fn=validate)\n\n\n@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"transactions.parquet\",\n    validators={\n        \"amount\": [within_std_devs(3)],  # Catch extreme outliers\n    }\n)\ndef transaction_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#multi-column-validators","title":"Multi-Column Validators","text":"<p>For validators that need to check relationships between columns, implement the logic in your feature function and validate the result:</p> <pre><code>import mlforge as mlf\nfrom mlforge.validators import Validator, ValidationResult\nimport polars as pl\n\ndef discount_less_than_price() -&gt; Validator:\n    \"\"\"Validate discount is always less than price.\"\"\"\n\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        # Assumes series contains a boolean column from the feature function\n        invalid = series.filter(~series)\n\n        if len(invalid) &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{len(invalid)} rows where discount &gt;= price\",\n                failed_count=len(invalid),\n            )\n\n        return ValidationResult(passed=True)\n\n    return Validator(name=\"discount_less_than_price\", fn=validate)\n\n\n@mlf.feature(\n    keys=[\"product_id\"],\n    source=\"products.parquet\",\n    validators={\n        \"valid_pricing\": [discount_less_than_price()],\n    }\n)\ndef product_features(df):\n    return df.with_columns([\n        (pl.col(\"discount\") &lt; pl.col(\"price\")).alias(\"valid_pricing\")\n    ])\n</code></pre>"},{"location":"user-guide/validators/#validation-in-metadata","title":"Validation in Metadata","text":"<p>Validators are tracked in feature metadata for auditing and documentation:</p> <pre><code>{\n  \"name\": \"merchant_transactions\",\n  \"columns\": [\n    {\n      \"name\": \"merchant_id\",\n      \"dtype\": \"String\"\n    },\n    {\n      \"name\": \"amount\",\n      \"dtype\": \"Float64\",\n      \"validators\": [\n        {\n          \"validator\": \"not_null\"\n        },\n        {\n          \"validator\": \"greater_than_or_equal\",\n          \"value\": 0\n        }\n      ]\n    }\n  ],\n  \"features\": [...]\n}\n</code></pre> <p>This metadata shows: - Which columns have validators - What validation rules are applied - Parameters for each validator</p>"},{"location":"user-guide/validators/#validation-cli-command","title":"Validation CLI Command","text":"<p>Run validation without building features:</p> <pre><code># Validate all features\nmlforge validate\n\n# Validate specific features\nmlforge validate --features merchant_transactions\n\n# Validate by tag\nmlforge validate --tags transactions\n</code></pre> <p>This is useful for: - Testing new validators before building - Validating source data changes - CI/CD data quality checks</p>"},{"location":"user-guide/validators/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/validators/#1-validate-at-the-source","title":"1. Validate at the Source","text":"<p>Add validators close to where data enters your feature store:</p> <pre><code>@mlf.feature(\n    keys=[\"user_id\"],\n    source=\"raw/users.parquet\",\n    validators={\n        \"user_id\": [mlf.not_null(), mlf.unique()],\n        \"created_at\": [mlf.not_null()],\n        \"email\": [mlf.not_null(), mlf.matches_regex(r\"^\\w+@\\w+\\.\\w+$\")],\n    }\n)\ndef user_base_features(df):\n    return df\n</code></pre>"},{"location":"user-guide/validators/#2-use-validators-for-assumptions","title":"2. Use Validators for Assumptions","text":"<p>Document assumptions your feature logic makes:</p> <pre><code>@mlf.feature(\n    keys=[\"transaction_id\"],\n    source=\"transactions.parquet\",\n    validators={\n        \"amount\": [mlf.greater_than(0)],  # Feature assumes positive amounts\n    }\n)\ndef transaction_features(df):\n    # This logic assumes amount &gt; 0\n    return df.with_columns([\n        (100.0 / pl.col(\"amount\")).alias(\"amount_inverse\")\n    ])\n</code></pre>"},{"location":"user-guide/validators/#3-keep-validators-simple","title":"3. Keep Validators Simple","text":"<p>Each validator should check one thing:</p> <pre><code># Good: Each validator checks one aspect\nvalidators={\n    \"age\": [mlf.not_null(), mlf.greater_than(0), mlf.less_than(150)]\n}\n\n# Bad: Complex multi-condition validator\nvalidators={\n    \"age\": [validate_age_is_valid_and_reasonable_and_not_null()]\n}\n</code></pre>"},{"location":"user-guide/validators/#4-provide-clear-error-messages","title":"4. Provide Clear Error Messages","text":"<p>Make validation failures actionable:</p> <pre><code>def is_valid_phone_number() -&gt; Validator:\n    def validate(series: pl.Series) -&gt; ValidationResult:\n        invalid = series.filter(~series.str.contains(r\"^\\+?\\d{10,15}$\"))\n\n        if len(invalid) &gt; 0:\n            return ValidationResult(\n                passed=False,\n                message=f\"{len(invalid)} invalid phone numbers (expected format: +1234567890)\",\n                failed_count=len(invalid),\n            )\n\n        return ValidationResult(passed=True)\n\n    return Validator(name=\"is_valid_phone_number\", fn=validate)\n</code></pre>"},{"location":"user-guide/validators/#5-test-your-validators","title":"5. Test Your Validators","text":"<p>Write tests for custom validators:</p> <pre><code>def test_min_length_validator():\n    series = pl.Series([\"abc\", \"ab\", \"a\"])\n    validator = min_length(3)\n    result = validator(series)\n\n    assert not result.passed\n    assert result.failed_count == 2  # \"ab\" and \"a\" are too short\n</code></pre>"},{"location":"user-guide/validators/#see-also","title":"See Also","text":"<ul> <li>Validators API Reference - Complete API documentation</li> <li>Validation API Reference - Validation execution internals</li> <li>Building Features - How validation fits into the build process</li> <li>Feature Metadata - How validators appear in metadata</li> </ul>"}]}
# v0.2.0 Roadmap

## Overview

v0.2.0 focuses on improving SDK usability and enabling team workflows. No new compute backends or online storesâ€”just making the existing system more practical for real projects.

## Timeline

Target: 4-6 weeks from v0.1.0 release

## Features

### 1. Feature Tags

**What:** Add tags to features for grouping and selective builds.

**SDK:**
```python
@feature(
    keys=["user_id"],
    source=SOURCE,
    tags=["users", "spending"],
)
def user_total_spend(df: pl.DataFrame) -> pl.DataFrame:
    ...
```

**CLI:**
```bash
# Build only features tagged "users"
uv run mlforge build definitions.py --tag users

# Build multiple tags
uv run mlforge build definitions.py --tag users --tag accounts

# List features by tag
uv run mlforge list definitions.py --tag users
```

**Definition of Done:**
- [ ] `tags` parameter added to `@feature` decorator
- [ ] `--tag` flag added to `build` command
- [ ] `--tag` flag added to `list` command
- [ ] Multiple tags supported (feature matches if ANY tag matches)
- [ ] Unit tests for tag filtering
- [ ] Documentation updated

---

### 2. S3 Storage Backend

**What:** Store materialized features in S3 for team access.

**SDK:**
```python
from mlforge import Definitions, S3Store

defs = Definitions(
    name="my-project",
    features=[users],
    offline_store=S3Store(
        bucket="my-bucket",
        prefix="features/",
        region="us-east-1",  # optional
    ),
)
```

**Retrieval:**
```python
from mlforge import get_training_data, S3Store

training_df = get_training_data(
    features=["user_total_spend"],
    entity_df=entity_df,
    store=S3Store(bucket="my-bucket", prefix="features/"),
)
```

**Definition of Done:**
- [ ] `S3Store` class implementing `Store` ABC
- [ ] Read/write Parquet to S3 via `s3fs` or `boto3`
- [ ] Credentials via environment variables or AWS profile
- [ ] Works with `mlforge build` and `get_training_data`
- [ ] Integration test with localstack or mocked S3
- [ ] Documentation with IAM policy example

---

### 3. Preconfigured Aggregations

**What:** Helper functions for common rolling/windowed aggregations.

**SDK:**
```python
from mlforge.agg import rolling_sum, rolling_mean, rolling_count, count_distinct

@feature(keys=["user_id"], source=SOURCE, timestamp="event_time")
def user_spending_features(df: pl.DataFrame) -> pl.DataFrame:
    return (
        df
        .pipe(with_user_id)
        .sort("event_time")
        .group_by("user_id")
        .agg(
            rolling_sum("amt", window="30d"),
            rolling_mean("amt", window="7d"),
            count_distinct("merchant", window="30d"),
        )
    )
```

**Available Functions:**
| Function | Description |
|----------|-------------|
| `rolling_sum(col, window)` | Sum over window |
| `rolling_mean(col, window)` | Mean over window |
| `rolling_min(col, window)` | Min over window |
| `rolling_max(col, window)` | Max over window |
| `rolling_count(col, window)` | Count over window |
| `count_distinct(col, window)` | Distinct count over window |

**Definition of Done:**
- [ ] `mlforge/agg.py` module with helper functions
- [ ] Functions return Polars expressions
- [ ] Window parameter accepts string ("30d", "7d") or timedelta
- [ ] Unit tests for each aggregation
- [ ] Documentation with examples

---

### 4. Feature Metadata

**What:** Automatically capture and store metadata when features are materialized.

**Metadata Schema:**
```json
{
    "name": "user_total_spend",
    "version": "0.2.0",
    "rows": 50000,
    "columns": ["user_id", "total_spend"],
    "schema": {
        "user_id": "str",
        "total_spend": "float64"
    },
    "materialized_at": "2024-12-11T10:30:00Z",
    "source": "data/transactions.csv",
    "tags": ["users", "spending"],
    "keys": ["user_id"],
    "timestamp": null
}
```

**Storage:**
- LocalStore: `./feature_store/user_total_spend.meta.json`
- S3Store: `s3://bucket/features/user_total_spend.meta.json`

**CLI:**
```bash
# View feature metadata
uv run mlforge inspect user_total_spend

# Output:
# user_total_spend
#   rows: 50,000
#   columns: user_id (str), total_spend (float64)
#   materialized: 2024-12-11 10:30:00
#   source: data/transactions.csv
#   tags: users, spending
```
---

### 5. Feature Validation

**What:** Optional validation functions that run after feature materialization to catch data quality issues early.

**SDK:**
```python
from mlforge import feature
from mlforge.validators import unique, not_null, positive, in_range

@feature(
    keys=["user_id"],
    source=SOURCE,
    validate=[unique("user_id"), not_null("user_id"), positive("total_spend")],
)
def user_total_spend(df: pl.DataFrame) -> pl.DataFrame:
    ...

# Single validator also supported
@feature(
    keys=["user_id"],
    source=SOURCE,
    validate=unique("user_id"),
)
def user_feature(df: pl.DataFrame) -> pl.DataFrame:
    ...

# Custom validators via lambda or function
@feature(
    keys=["user_id"],
    source=SOURCE,
    validate=lambda df: df["amount"].max() < 1_000_000,
)
def user_feature(df: pl.DataFrame) -> pl.DataFrame:
    ...
```

**Built-in Validators:**
| Validator | Description |
|-----------|-------------|
| `unique(col)` | Column values are unique |
| `not_null(col)` | Column has no null values |
| `positive(col)` | Column values are > 0 |
| `non_negative(col)` | Column values are >= 0 |
| `in_range(col, min, max)` | Column values within range |
| `accepted_values(col, values)` | Column values in allowed set |

**CLI:**
```bash
# Validation runs automatically during build
uv run mlforge build definitions.py

# Skip validation
uv run mlforge build definitions.py --skip-validation
```

**Error Output:**
```
ValidationError: Feature 'user_total_spend' failed validation
  - unique("user_id"): 47 duplicate values found
  - positive("total_spend"): 3 values <= 0
```

**Definition of Done:**
- [ ] `validate` parameter added to `@feature` decorator (single or list)
- [ ] `mlforge/validators.py` module with built-in validators
- [ ] Validators run after materialization, before write
- [ ] `--skip-validation` flag added to `build` command
- [ ] Clear error messages showing which validators failed and why
- [ ] Custom validators supported (any callable taking DataFrame, returning bool or raising)
- [ ] Unit tests for each built-in validator
- [ ] Documentation with examples

---

## Out of Scope for v0.2.0

The following are explicitly **not** in scope:

| Feature | Reason |
|---------|--------|
| DuckDB/Spark compute | Polars sufficient for v0.2.0 |
| Online store (Redis) | Different use case, adds complexity |
| Unity Catalog / ClickHouse | Enterprise feature, needs user demand |
| Async/parallel builds | Optimization, not core functionality |
| AI feature suggestions | Nice to have, not essential |
| Scheduling / orchestration | Users can use external tools |
| GCS storage | S3 first, GCS in v0.3.0 if needed |
| Feature versioning | Complex, defer until patterns clear |
| Web UI | CLI sufficient for now |

---

## Testing Requirements

All features must have:
- Unit tests with >80% coverage
- Integration test where applicable (S3)
- Documentation with examples

---

## Documentation Requirements

- Update README with new features
- Add examples for each new feature
- Update CLI help text
- Add S3 setup guide

---

## Release Checklist

- [ ] All features implemented and tested
- [ ] `just check` passes
- [ ] Documentation updated
- [ ] CHANGELOG generated via commitizen
- [ ] Version bumped to 0.2.0
- [ ] Tagged and pushed
- [ ] Published to PyPI
- [ ] Docs published
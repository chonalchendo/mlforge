# v0.5.0 - Compute Backends, Versioning & Online Store

**Target Release:** Q1 2025
**Status:** Planned
**Breaking Changes:** Yes

## Overview

v0.5.0 introduces three major capabilities:

1. **Feature Versioning** - Automated semantic versioning with content-hash tracking
2. **DuckDB Compute Backend** - Alternative to Polars for large dataset computation
3. **Redis Online Store** - Real-time feature serving via Redis

This release establishes mlforge as a complete feature store with both offline (batch) and online (real-time) capabilities.

---

## Features

### 1. Feature Versioning

Automated semantic versioning based on schema and configuration changes, inspired by DVC's content-addressable approach but with human-readable version directories.

**Motivation:**
- Enable model reproducibility by pinning feature versions
- Support A/B testing with different feature versions
- Allow rollback to previous feature versions
- Track feature evolution over time
- Automatic version bumps based on detected changes

**Design Principles:**
- **Version-based directories** for human readability (e.g., `user_spend/1.0.0/`)
- **Content hashes stored in metadata** for integrity verification and change detection
- **Automatic version detection** based on schema/config hash comparison
- **Explicit version override** when users need control

**Auto-Versioning Logic:**

| Change Type | Detection Method | Version Bump |
|-------------|------------------|--------------|
| First build | No previous version exists | `1.0.0` |
| Schema breaking (columns removed) | Compare output schema | MAJOR |
| Schema breaking (dtype changed) | Compare output schema | MAJOR |
| Metrics removed | Compare config hash | MAJOR |
| Schema additive (columns added) | Compare output schema | MINOR |
| Metrics added (new windows/aggs) | Compare config hash | MINOR |
| Config change (interval, keys) | Compare config hash | MINOR |
| Data refresh (same schema/config) | Hashes match | PATCH |

**API:**

```python
# Build with auto-versioning (default)
# Detects changes and increments version appropriately
result = defs.build(features=["user_spend"])
# → Creates 1.0.0 (first), 1.1.0 (schema change), or 1.0.1 (data refresh)

# Build with explicit version override
defs.build(features=["user_spend"], version="2.0.0")

# Read specific version
store.read("user_spend", version="1.0.0")

# Read latest (default)
store.read("user_spend")

# List versions
store.list_versions("user_spend")  # → ["1.0.0", "1.0.1", "1.1.0"]

# Get training data with specific versions
get_training_data(
    spine=spine_df,
    features=[
        "user_spend",                    # latest
        ("merchant_features", "1.0.0"),  # specific version
    ],
    store=store,
)
```

**CLI:**

```bash
# Build with auto-versioning (default)
mlforge build --features user_spend

# Build with explicit version
mlforge build --features user_spend --version 2.0.0

# List versions
mlforge versions user_spend

# Inspect specific version
mlforge inspect user_spend --version 1.0.0
```

**Storage Layout:**

```
feature_store/
├── user_spend/
│   ├── 1.0.0/
│   │   ├── data.parquet
│   │   └── .meta.json
│   ├── 1.1.0/
│   │   ├── data.parquet
│   │   └── .meta.json
│   └── _latest.json          # {"version": "1.1.0"}
└── _metadata/
    └── manifest.json
```

**Metadata Schema (.meta.json):**

```json
{
  "name": "user_spend",
  "version": "1.1.0",
  "content_hash": "abc123def456",
  "schema_hash": "def789xyz012",
  "config_hash": "123abc456def",
  "created_at": "2025-01-15T10:00:00Z",
  "updated_at": "2025-01-15T10:00:00Z",
  "row_count": 150000,
  "entity_keys": ["user_id"],
  "timestamp": "event_time",
  "interval": "1d",
  "columns": [
    {"name": "user_id", "dtype": "String"},
    {"name": "event_time", "dtype": "Datetime"},
    {"name": "amount__sum__7d", "dtype": "Float64", "aggregation": "sum", "window": "7d"}
  ],
  "change_summary": {
    "bump_type": "minor",
    "reason": "added_columns",
    "details": ["amount__sum__30d", "amount__mean__30d"]
  }
}
```

**Metadata Changes from v0.4.0:**
- Add `version: str` field
- Add `content_hash: str` field (hash of data.parquet)
- Add `schema_hash: str` field (hash of column names + dtypes)
- Add `config_hash: str` field (hash of keys, timestamp, interval, metrics config)
- Add `created_at: str` field (ISO 8601 timestamp)
- Rename `last_updated` → `updated_at` for consistency
- Add `change_summary: dict` field (documents why version was bumped)

---

### 2. DuckDB Compute Backend

Drop-in replacement for Polars engine, optimized for large datasets.

**Motivation:**
- Superior performance on large datasets without requiring Spark
- SQL-based window functions for efficient rolling aggregations
- Ability to spill to disk for datasets larger than memory
- Local data warehouse capabilities

**Design Principles:**
- Same Python API - features work with both engines without code changes
- User functions receive Polars DataFrames (DuckDB converts internally)
- DuckDB handles source loading and metrics computation (the expensive parts)
- Native DuckDB window functions for rolling metrics

**API:**

```python
from mlforge import Definitions
from mlforge.store import LocalStore

store = LocalStore(path="./feature_store")

# Default (Polars)
defs = Definitions(store=store)

# DuckDB engine
defs = Definitions(store=store, engine="duckdb")

# Per-build override
defs.build(features=["user_spend"], engine="duckdb")
```

**Feature Definition (unchanged):**

```python
import polars as pl
from mlforge import feature
from mlforge.metrics import Rolling
import mlforge.validators as v

@feature(
    keys=["user_id"],
    source="transactions.parquet",
    timestamp="event_time",
    metrics=[
        Rolling(
            windows=["7d", "30d"],
            aggregations={"amount": ["sum", "mean"]},
        ),
    ],
    validators={"user_id": [v.not_null()], "amount": [v.greater_than(0)]},
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    # Same code works with both Polars and DuckDB engines
    return df.select(
        pl.col("user_id"),
        pl.col("event_time"),
        pl.col("amount"),
    )
```

**Rolling Metrics Compilation:**

DuckDB uses native SQL window functions with `RANGE BETWEEN`:

```sql
SELECT
    user_id,
    event_time,
    SUM(amount) OVER w_7d AS "user_spend__amount__sum__1d__7d",
    AVG(amount) OVER w_7d AS "user_spend__amount__mean__1d__7d",
    SUM(amount) OVER w_30d AS "user_spend__amount__sum__1d__30d",
    AVG(amount) OVER w_30d AS "user_spend__amount__mean__1d__30d"
FROM feature_data
WINDOW
    w_7d AS (PARTITION BY user_id ORDER BY event_time
             RANGE BETWEEN INTERVAL '7 days' PRECEDING AND CURRENT ROW),
    w_30d AS (PARTITION BY user_id ORDER BY event_time
              RANGE BETWEEN INTERVAL '30 days' PRECEDING AND CURRENT ROW)
```

**Aggregation Mapping:**

| mlforge | DuckDB SQL |
|---------|------------|
| `count` | `COUNT` |
| `sum` | `SUM` |
| `mean` | `AVG` |
| `min` | `MIN` |
| `max` | `MAX` |
| `std` | `STDDEV_SAMP` |
| `median` | `MEDIAN` |

**Note on Polars vs DuckDB Parity:**

Polars uses `group_by_dynamic()` which operates on discrete time intervals, while DuckDB's `RANGE BETWEEN` is a continuous sliding window. Results may differ slightly depending on data distribution. Parity tests will validate differences are within acceptable tolerance. If significant differences are found, adjustments will be made to align behavior.

---

### 2.1 Unified Type System

A canonical type system that normalizes engine-specific types (Polars, DuckDB, future Spark) into a consistent representation for metadata, schema hashing, and type comparisons.

**Motivation:**
- Polars uses `Int64`, `Float64`, `Utf8` while DuckDB uses `BIGINT`, `DOUBLE`, `VARCHAR`
- Same logical schema produces different metadata depending on which engine built it
- Schema hashing is inconsistent across engines (affects versioning)
- Type comparisons in point-in-time joins fail on minor type differences
- Future engines (Spark) will introduce more type variations

**Design Principles:**
- **Canonical types** - Engine-agnostic type representation inspired by Apache Arrow
- **Bidirectional mapping** - Convert to/from Polars, DuckDB, and future engines
- **Immutable & hashable** - Type objects can be used as dict keys and in sets
- **JSON serializable** - Human-readable format for metadata storage
- **Backward compatible** - Existing metadata remains valid

**Research Basis:**

| Project | Approach | Key Insight |
|---------|----------|-------------|
| Apache Arrow | Class-based, language-agnostic | Industry standard for cross-engine interop |
| Ibis | Class-based with conversion methods | 20+ backends with unified types |
| Feast | Enum-based (ValueType) | Simple but limited expressiveness |

**Recommended Approach:** Hybrid class-based system inspired by Ibis, using Arrow-compatible type names.

**Type System Design:**

```python
# src/mlforge/types.py (new file)

from enum import Enum
from dataclasses import dataclass
from typing import Any

class TypeKind(Enum):
    """Canonical type kinds for mlforge."""
    # Integer types
    INT8 = "int8"
    INT16 = "int16"
    INT32 = "int32"
    INT64 = "int64"
    UINT8 = "uint8"
    UINT16 = "uint16"
    UINT32 = "uint32"
    UINT64 = "uint64"
    # Float types
    FLOAT32 = "float32"
    FLOAT64 = "float64"
    # String types
    STRING = "string"
    # Boolean
    BOOLEAN = "boolean"
    # Temporal types
    DATE = "date"
    DATETIME = "datetime"
    TIME = "time"
    DURATION = "duration"
    # Complex types (future)
    DECIMAL = "decimal"
    LIST = "list"
    STRUCT = "struct"
    # Fallback
    UNKNOWN = "unknown"

@dataclass(frozen=True)
class DataType:
    """Immutable, hashable type representation."""
    kind: TypeKind
    nullable: bool = True
    timezone: str | None = None  # For DATETIME
    precision: int | None = None  # For DECIMAL
    scale: int | None = None      # For DECIMAL

    def to_json(self) -> dict[str, Any]:
        """Serialize to JSON-compatible dict."""
        result = {"kind": self.kind.value, "nullable": self.nullable}
        if self.timezone:
            result["timezone"] = self.timezone
        if self.precision is not None:
            result["precision"] = self.precision
        if self.scale is not None:
            result["scale"] = self.scale
        return result

    @classmethod
    def from_json(cls, data: dict[str, Any]) -> "DataType":
        """Deserialize from JSON dict."""
        return cls(
            kind=TypeKind(data["kind"]),
            nullable=data.get("nullable", True),
            timezone=data.get("timezone"),
            precision=data.get("precision"),
            scale=data.get("scale"),
        )

# Convenience constructors
def int64(nullable: bool = True) -> DataType:
    return DataType(TypeKind.INT64, nullable)

def float64(nullable: bool = True) -> DataType:
    return DataType(TypeKind.FLOAT64, nullable)

def string(nullable: bool = True) -> DataType:
    return DataType(TypeKind.STRING, nullable)

def datetime(timezone: str | None = None, nullable: bool = True) -> DataType:
    return DataType(TypeKind.DATETIME, nullable, timezone=timezone)
```

**Type Mapping Tables:**

| mlforge Canonical | Polars | DuckDB | Arrow | Spark |
|-------------------|--------|--------|-------|-------|
| `int8` | `Int8` | `TINYINT` | `int8` | `ByteType` |
| `int16` | `Int16` | `SMALLINT` | `int16` | `ShortType` |
| `int32` | `Int32` | `INTEGER` | `int32` | `IntegerType` |
| `int64` | `Int64` | `BIGINT` | `int64` | `LongType` |
| `uint8` | `UInt8` | `UTINYINT` | `uint8` | N/A |
| `uint16` | `UInt16` | `USMALLINT` | `uint16` | N/A |
| `uint32` | `UInt32` | `UINTEGER` | `uint32` | N/A |
| `uint64` | `UInt64` | `UBIGINT` | `uint64` | N/A |
| `float32` | `Float32` | `FLOAT` | `float32` | `FloatType` |
| `float64` | `Float64` | `DOUBLE` | `float64` | `DoubleType` |
| `string` | `Utf8` | `VARCHAR` | `string` | `StringType` |
| `boolean` | `Boolean` | `BOOLEAN` | `bool` | `BooleanType` |
| `date` | `Date` | `DATE` | `date32` | `DateType` |
| `datetime` | `Datetime` | `TIMESTAMP` | `timestamp` | `TimestampType` |

**Conversion Functions:**

```python
# Type normalization from engine-specific types
def from_polars(dtype: pl.DataType) -> DataType:
    """Convert Polars dtype to canonical DataType."""
    mapping = {
        pl.Int8: TypeKind.INT8,
        pl.Int16: TypeKind.INT16,
        pl.Int32: TypeKind.INT32,
        pl.Int64: TypeKind.INT64,
        pl.UInt8: TypeKind.UINT8,
        pl.UInt16: TypeKind.UINT16,
        pl.UInt32: TypeKind.UINT32,
        pl.UInt64: TypeKind.UINT64,
        pl.Float32: TypeKind.FLOAT32,
        pl.Float64: TypeKind.FLOAT64,
        pl.Utf8: TypeKind.STRING,
        pl.Boolean: TypeKind.BOOLEAN,
        pl.Date: TypeKind.DATE,
    }
    # Handle Datetime with timezone
    if isinstance(dtype, pl.Datetime):
        return DataType(TypeKind.DATETIME, timezone=dtype.time_zone)
    return DataType(mapping.get(type(dtype), TypeKind.UNKNOWN))

def from_duckdb(dtype_str: str) -> DataType:
    """Convert DuckDB type string to canonical DataType."""
    mapping = {
        "TINYINT": TypeKind.INT8,
        "SMALLINT": TypeKind.INT16,
        "INTEGER": TypeKind.INT32,
        "BIGINT": TypeKind.INT64,
        "UTINYINT": TypeKind.UINT8,
        "USMALLINT": TypeKind.UINT16,
        "UINTEGER": TypeKind.UINT32,
        "UBIGINT": TypeKind.UINT64,
        "FLOAT": TypeKind.FLOAT32,
        "DOUBLE": TypeKind.FLOAT64,
        "VARCHAR": TypeKind.STRING,
        "BOOLEAN": TypeKind.BOOLEAN,
        "DATE": TypeKind.DATE,
        "TIMESTAMP": TypeKind.DATETIME,
    }
    return DataType(mapping.get(dtype_str.upper(), TypeKind.UNKNOWN))

def to_polars(dtype: DataType) -> pl.DataType:
    """Convert canonical DataType to Polars dtype."""
    # Implementation...

def to_duckdb(dtype: DataType) -> str:
    """Convert canonical DataType to DuckDB type string."""
    # Implementation...
```

**Aggregation Output Types:**

| Aggregation | Input Type | Output Type |
|-------------|-----------|-------------|
| `count` | Any | `int64` |
| `sum` | Integer | Same as input |
| `sum` | Float | `float64` |
| `mean` | Numeric | `float64` |
| `min` | Any | Same as input |
| `max` | Any | Same as input |
| `std` | Numeric | `float64` |
| `median` | Numeric | `float64` |

**Integration Points:**

| Location | Current | With Unified Types |
|----------|---------|-------------------|
| `engines/polars.py:70-72` | `str(dtype)` | `from_polars(dtype).to_json()` |
| `engines/duckdb.py:106-108` | `str(dtype)` | `from_polars(dtype).to_json()` |
| `results.py:128-132` | `str(dtype)` | `from_polars(dtype)` |
| `results.py:230-235` | `str(dtype)` | `from_duckdb(dtype)` |
| `manifest.py:24-63` | `dtype: str` | `dtype: str` + `dtype_canonical: dict` |
| `retrieval.py:184-192` | Direct comparison | Compare canonical types |
| `version.py:275-294` | Hash string repr | Hash canonical type |

**Metadata Schema Update:**

```json
{
  "columns": [
    {
      "name": "user_id",
      "dtype": "string",
      "dtype_raw": "Utf8"
    },
    {
      "name": "amount__sum__7d",
      "dtype": "float64",
      "dtype_raw": "Float64"
    }
  ]
}
```

- `dtype`: Canonical type (used for hashing, comparison)
- `dtype_raw`: Original engine-specific type (for debugging)

---

### 3. Redis Online Store

Real-time feature serving via Redis for low-latency inference.

**Motivation:**
- Enable real-time ML inference with feature retrieval
- Complete the offline → online feature store loop
- Provide foundation for future online stores (DynamoDB, etc.)

**API:**

```python
from mlforge import Definitions
from mlforge.store import LocalStore
from mlforge.online import RedisStore

offline_store = LocalStore(path="./feature_store")
online_store = RedisStore(host="localhost", port=6379, db=0)

defs = Definitions(
    store=offline_store,
    online_store=online_store,
)

# Build to offline store (default)
defs.build(features=["user_spend"])

# Build to online store
defs.build(features=["user_spend"], online=True)
```

**Reading from Online Store:**

```python
# Single entity
features = online_store.read(
    feature_name="user_spend",
    entity_keys={"user_id": "user_123"},
)
# Returns: {"amount__sum__7d": 1500.0, "amount__mean__7d": 214.28, ...}

# Batch read
features_batch = online_store.read_batch(
    feature_name="user_spend",
    entity_keys=[
        {"user_id": "user_123"},
        {"user_id": "user_456"},
    ],
)
```

**CLI:**

```bash
mlforge build --features user_spend --online
```

**Redis Data Model:**

```
Key: mlforge:{feature_name}:{entity_key_hash}
Value: JSON serialized feature values
TTL: Optional, configurable per RedisStore instance
```

**RedisStore Configuration:**

```python
RedisStore(
    host: str = "localhost",
    port: int = 6379,
    db: int = 0,
    password: str | None = None,
    ttl: int | None = None,  # seconds, None = no expiry
)
```

---

## Architecture Changes

### Package Restructure

Engines and compilers are reorganized into packages for extensibility:

```
src/mlforge/
├── engines/
│   ├── __init__.py          # Re-exports
│   ├── base.py              # Engine ABC
│   ├── polars_engine.py     # PolarsEngine
│   └── duckdb_engine.py     # DuckDBEngine (NEW)
├── compilers/
│   ├── __init__.py          # Re-exports
│   ├── base.py              # ComputeContext
│   ├── polars_compiler.py   # PolarsCompiler
│   └── duckdb_compiler.py   # DuckDBCompiler (NEW)
├── online.py                # OnlineStore ABC, RedisStore (NEW)
└── ...
```

### OnlineStore ABC

```python
class OnlineStore(ABC):
    @abstractmethod
    def write(self, feature_name: str, entity_keys: dict[str, str], values: dict[str, Any]) -> None: ...

    @abstractmethod
    def write_batch(self, feature_name: str, records: list[dict[str, Any]], entity_key_columns: list[str]) -> None: ...

    @abstractmethod
    def read(self, feature_name: str, entity_keys: dict[str, str]) -> dict[str, Any] | None: ...

    @abstractmethod
    def read_batch(self, feature_name: str, entity_keys: list[dict[str, str]]) -> list[dict[str, Any] | None]: ...
```

---

## Dependencies

New optional dependencies:

```toml
[project.optional-dependencies]
duckdb = ["duckdb>=1.0.0"]
redis = ["redis>=5.0.0"]
all = ["duckdb>=1.0.0", "redis>=5.0.0"]
```

**Installation:**

```bash
pip install mlforge           # Core only
pip install mlforge[duckdb]   # With DuckDB
pip install mlforge[redis]    # With Redis
pip install mlforge[all]      # Everything
```

---

## Breaking Changes

### Storage Layout

Feature storage now includes version directories. Existing features must be rebuilt:

```bash
# Migrate existing features
mlforge build --features user_spend --version 1.0.0
```

### Metadata Schema

- `FeatureMetadata.last_updated` renamed to `updated_at`
- New required field: `version: str`
- New required field: `created_at: str`
- New required field: `content_hash: str`
- New required field: `schema_hash: str`
- New required field: `config_hash: str`
- New optional field: `change_summary: dict`

Regenerate metadata after upgrade:

```bash
mlforge manifest --regenerate
```

### get_training_data() Signature

Features parameter now accepts version tuples:

```python
# Before (still works - uses latest)
get_training_data(spine, features=["user_spend"], store=store)

# After (with version)
get_training_data(
    spine,
    features=[
        "user_spend",                    # latest
        ("merchant_features", "1.0.0"),  # specific version
    ],
    store=store,
)
```

---

## Implementation Phases

| Phase | Description | Effort | Status |
|-------|-------------|--------|--------|
| 1 | Feature Versioning - Store layer | Medium | ✅ Done |
| 2 | Feature Versioning - Metadata & Hashing | Medium | ✅ Done |
| 3 | Feature Versioning - Auto-detection logic | Medium | ✅ Done |
| 4 | Feature Versioning - Core & CLI | Medium | ✅ Done |
| 5 | Feature Versioning - Retrieval | Low | ✅ Done |
| 6 | Refactor engines/compilers to packages | Low | ✅ Done |
| 7 | DuckDB Engine | Medium | ✅ Done |
| 8 | DuckDB Compiler | Medium-High | ✅ Done |
| 9 | DuckDB Result | Low | ✅ Done |
| 9.1 | **Unified Type System - Core types module** | Medium | ✅ Done |
| 9.2 | **Unified Type System - Engine integration** | Medium | ✅ Done |
| 9.3 | **Unified Type System - Metadata & hashing** | Low | ✅ Done |
| 9.4 | **Unified Type System - Tests** | Medium | ✅ Done |
| 10 | Online Store ABC | Low | ⏳ Planned |
| 11 | Redis Store implementation | Medium | ⏳ Planned |
| 12 | Online Store integration | Medium | ⏳ Planned |
| 13 | Dependencies | Low | ⏳ Planned |
| 14 | Tests (including Polars/DuckDB parity) | Medium | ✅ Done |
| 15 | Documentation | Low | ⏳ Planned |
| 16 | Examples | Low | ⏳ Planned |

### Unified Type System Implementation Details

**Phase 9.1: Core types module** (`src/mlforge/types.py`)
- [x] Create `TypeKind` enum with all canonical types
- [x] Create `DataType` dataclass (immutable, hashable)
- [x] Implement `to_json()` and `from_json()` methods
- [x] Add convenience constructors (`int64()`, `float64()`, `string()`, etc.)
- [x] Implement `from_polars()` conversion function
- [x] Implement `from_duckdb()` conversion function
- [x] Implement `to_polars()` conversion function
- [x] Implement `to_duckdb()` conversion function
- [x] Define `AGG_OUTPUT_TYPES` mapping for aggregation result types

**Phase 9.2: Engine integration**
- [x] Add `EngineResult.schema_canonical()` to return canonical types
- [x] Keep `EngineResult.schema()` for raw engine types (backward compat)
- [x] Update `PolarsResult` to use `from_polars()`
- [x] Update `DuckDBResult` to use `from_duckdb()`
- [x] Add `base_schema_canonical()` with correct source handling
- [x] Pass schema_source to derive_column_metadata

**Phase 9.3: Metadata & hashing**
- [x] Update `derive_column_metadata()` to convert to canonical types
- [x] Update `core.py` schema hashing to use canonical types
- [x] Update `retrieval.py` type comparison to use canonical types
- [x] Update timestamp detection to use canonical types

**Phase 9.4: Tests**
- [x] Create `tests/test_types.py` with comprehensive type tests (69 tests)
- [x] Test all Polars → canonical conversions
- [x] Test all DuckDB → canonical conversions
- [x] Test canonical → Polars conversions
- [x] Test canonical → DuckDB conversions
- [x] Test JSON serialization/deserialization
- [x] Test aggregation output type mapping
- [x] Test schema hashing consistency across engines
- [x] Update parity tests to verify type normalization

---

## Testing Strategy

### New Test Files

- `tests/test_versioning.py` - Auto-version detection, hash computation, version bumping
- `tests/test_engines.py` - DuckDB engine tests
- `tests/test_compilers.py` - DuckDB compiler, SQL generation
- `tests/test_online.py` - Redis store operations
- `tests/test_engine_parity.py` - Polars vs DuckDB result comparison

### Version Detection Testing

```python
def test_auto_version_major_bump():
    """Removing columns should trigger MAJOR version bump."""
    # Build v1.0.0 with columns [a, b, c]
    # Build again with columns [a, b] (c removed)
    # Assert new version is 2.0.0

def test_auto_version_minor_bump():
    """Adding columns should trigger MINOR version bump."""
    # Build v1.0.0 with columns [a, b]
    # Build again with columns [a, b, c] (c added)
    # Assert new version is 1.1.0

def test_auto_version_patch_bump():
    """Same schema/config should trigger PATCH version bump."""
    # Build v1.0.0
    # Build again with same schema/config but new data
    # Assert new version is 1.0.1
```

### Parity Testing

Compare Polars and DuckDB rolling metric outputs:

```python
def test_rolling_metrics_parity():
    """Ensure Polars and DuckDB produce equivalent results."""
    # Create test data with known values
    # Run same feature with both engines
    # Compare results within tolerance (e.g., 1e-6 for floats)
    # Flag and investigate significant differences
```

### Unified Type System Testing

```python
def test_polars_to_canonical():
    """Test Polars dtype to canonical type conversion."""
    assert from_polars(pl.Int64) == DataType(TypeKind.INT64)
    assert from_polars(pl.Float64) == DataType(TypeKind.FLOAT64)
    assert from_polars(pl.Utf8) == DataType(TypeKind.STRING)
    assert from_polars(pl.Datetime("us", "UTC")).timezone == "UTC"

def test_duckdb_to_canonical():
    """Test DuckDB type string to canonical type conversion."""
    assert from_duckdb("BIGINT") == DataType(TypeKind.INT64)
    assert from_duckdb("DOUBLE") == DataType(TypeKind.FLOAT64)
    assert from_duckdb("VARCHAR") == DataType(TypeKind.STRING)

def test_schema_hash_consistency():
    """Same logical schema produces same hash regardless of engine."""
    polars_schema = {"user_id": pl.Utf8, "amount": pl.Float64}
    duckdb_schema = {"user_id": "VARCHAR", "amount": "DOUBLE"}

    polars_canonical = {k: from_polars(v) for k, v in polars_schema.items()}
    duckdb_canonical = {k: from_duckdb(v) for k, v in duckdb_schema.items()}

    assert polars_canonical == duckdb_canonical
    assert hash_schema(polars_canonical) == hash_schema(duckdb_canonical)

def test_aggregation_output_types():
    """Aggregation output types are consistent across engines."""
    assert get_agg_output_type("count", DataType(TypeKind.FLOAT64)) == DataType(TypeKind.INT64)
    assert get_agg_output_type("mean", DataType(TypeKind.INT64)) == DataType(TypeKind.FLOAT64)
    assert get_agg_output_type("sum", DataType(TypeKind.FLOAT64)) == DataType(TypeKind.FLOAT64)

def test_type_json_roundtrip():
    """Types serialize and deserialize correctly."""
    original = DataType(TypeKind.DATETIME, timezone="UTC")
    serialized = original.to_json()
    deserialized = DataType.from_json(serialized)
    assert original == deserialized
```

---

## Future Considerations

Features deferred to future releases:

- **v0.5.x**: Feature function code hashing (detect logic changes)
- **v0.6.0**: Spark compute backend
- **v0.6.0+**: `mlforge serve` REST API (FastAPI)
- **v0.6.0+**: GCS storage backend
- **v0.7.0**: Airflow/Dagster operators
- **v0.8.0**: DynamoDB online store
- **Future**: Feature dependencies and lineage tracking
- **Future**: Feature registry with search/discovery

---

## Migration Guide

### From v0.4.0 to v0.5.0

1. **Rebuild features with versions:**
   ```bash
   # First build will create version 1.0.0
   mlforge build --features user_spend

   # Or explicitly set initial version
   mlforge build --features user_spend --version 1.0.0
   ```

2. **Regenerate manifest:**
   ```bash
   mlforge manifest --regenerate
   ```

3. **Update get_training_data() calls** (optional - for version pinning):
   ```python
   # Pin specific versions for reproducibility
   get_training_data(
       spine,
       features=[("user_spend", "1.0.0")],
       store=store,
   )
   ```

4. **Install optional dependencies** (if using new features):
   ```bash
   pip install mlforge[duckdb,redis]
   ```

---

## Design Decisions

### Why Version-Based Directories (not Hash-Based)?

| Aspect | Hash-Based | Version-Based (Chosen) |
|--------|------------|------------------------|
| **Readability** | `abc123def456` is meaningless | `1.1.0` is clear |
| **Sorting** | Random order in file browser | Natural version ordering |
| **Debugging** | Need to look up hash → version | Immediately obvious |
| **CLI output** | `Built: abc123def456` | `Built: 1.1.0` |
| **Conversations** | "Use hash abc123..." | "Use version 1.1.0" |

Content hashes are still stored in metadata for integrity verification and change detection.

### Why Auto-Versioning by Default?

- Reduces cognitive load on users
- Ensures versions have semantic meaning (breaking vs non-breaking)
- Aligns with DVC's philosophy of automatic change detection
- Explicit override available when users need control

### Why Skip Code Hashing for v0.5.0?

- Simpler initial implementation
- Schema/config changes cover most meaningful cases
- Code hashing is sensitive to whitespace/formatting
- Can be added in v0.5.x as enhancement

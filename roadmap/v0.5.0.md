# v0.5.0 - Compute Backends, Versioning & Online Store

**Target Release:** Q1 2025
**Status:** Planned
**Breaking Changes:** Yes

## Overview

v0.5.0 introduces three major capabilities:

1. **Feature Versioning** - Automated semantic versioning with content-hash tracking
2. **DuckDB Compute Backend** - Alternative to Polars for large dataset computation
3. **Redis Online Store** - Real-time feature serving via Redis

This release establishes mlforge as a complete feature store with both offline (batch) and online (real-time) capabilities.

---

## Features

### 1. Feature Versioning

Automated semantic versioning based on schema and configuration changes, inspired by DVC's content-addressable approach but with human-readable version directories.

**Motivation:**
- Enable model reproducibility by pinning feature versions
- Support A/B testing with different feature versions
- Allow rollback to previous feature versions
- Track feature evolution over time
- Automatic version bumps based on detected changes

**Design Principles:**
- **Version-based directories** for human readability (e.g., `user_spend/1.0.0/`)
- **Content hashes stored in metadata** for integrity verification and change detection
- **Automatic version detection** based on schema/config hash comparison
- **Explicit version override** when users need control

**Auto-Versioning Logic:**

| Change Type | Detection Method | Version Bump |
|-------------|------------------|--------------|
| First build | No previous version exists | `1.0.0` |
| Schema breaking (columns removed) | Compare output schema | MAJOR |
| Schema breaking (dtype changed) | Compare output schema | MAJOR |
| Metrics removed | Compare config hash | MAJOR |
| Schema additive (columns added) | Compare output schema | MINOR |
| Metrics added (new windows/aggs) | Compare config hash | MINOR |
| Config change (interval, keys) | Compare config hash | MINOR |
| Data refresh (same schema/config) | Hashes match | PATCH |

**API:**

```python
# Build with auto-versioning (default)
# Detects changes and increments version appropriately
result = defs.build(features=["user_spend"])
# → Creates 1.0.0 (first), 1.1.0 (schema change), or 1.0.1 (data refresh)

# Build with explicit version override
defs.build(features=["user_spend"], version="2.0.0")

# Read specific version
store.read("user_spend", version="1.0.0")

# Read latest (default)
store.read("user_spend")

# List versions
store.list_versions("user_spend")  # → ["1.0.0", "1.0.1", "1.1.0"]

# Get training data with specific versions
get_training_data(
    spine=spine_df,
    features=[
        "user_spend",                    # latest
        ("merchant_features", "1.0.0"),  # specific version
    ],
    store=store,
)
```

**CLI:**

```bash
# Build with auto-versioning (default)
mlforge build --features user_spend

# Build with explicit version
mlforge build --features user_spend --version 2.0.0

# List versions
mlforge versions user_spend

# Inspect specific version
mlforge inspect user_spend --version 1.0.0
```

**Storage Layout:**

```
feature_store/
├── user_spend/
│   ├── 1.0.0/
│   │   ├── data.parquet
│   │   └── .meta.json
│   ├── 1.1.0/
│   │   ├── data.parquet
│   │   └── .meta.json
│   └── _latest.json          # {"version": "1.1.0"}
└── _metadata/
    └── manifest.json
```

**Metadata Schema (.meta.json):**

```json
{
  "name": "user_spend",
  "version": "1.1.0",
  "content_hash": "abc123def456",
  "schema_hash": "def789xyz012",
  "config_hash": "123abc456def",
  "created_at": "2025-01-15T10:00:00Z",
  "updated_at": "2025-01-15T10:00:00Z",
  "row_count": 150000,
  "entity_keys": ["user_id"],
  "timestamp": "event_time",
  "interval": "1d",
  "columns": [
    {"name": "user_id", "dtype": "String"},
    {"name": "event_time", "dtype": "Datetime"},
    {"name": "amount__sum__7d", "dtype": "Float64", "aggregation": "sum", "window": "7d"}
  ],
  "change_summary": {
    "bump_type": "minor",
    "reason": "added_columns",
    "details": ["amount__sum__30d", "amount__mean__30d"]
  }
}
```

**Metadata Changes from v0.4.0:**
- Add `version: str` field
- Add `content_hash: str` field (hash of data.parquet)
- Add `schema_hash: str` field (hash of column names + dtypes)
- Add `config_hash: str` field (hash of keys, timestamp, interval, metrics config)
- Add `created_at: str` field (ISO 8601 timestamp)
- Rename `last_updated` → `updated_at` for consistency
- Add `change_summary: dict` field (documents why version was bumped)

---

### 2. DuckDB Compute Backend

Drop-in replacement for Polars engine, optimized for large datasets.

**Motivation:**
- Superior performance on large datasets without requiring Spark
- SQL-based window functions for efficient rolling aggregations
- Ability to spill to disk for datasets larger than memory
- Local data warehouse capabilities

**Design Principles:**
- Same Python API - features work with both engines without code changes
- User functions receive Polars DataFrames (DuckDB converts internally)
- DuckDB handles source loading and metrics computation (the expensive parts)
- Native DuckDB window functions for rolling metrics

**API:**

```python
from mlforge import Definitions
from mlforge.store import LocalStore

store = LocalStore(path="./feature_store")

# Default (Polars)
defs = Definitions(store=store)

# DuckDB engine
defs = Definitions(store=store, engine="duckdb")

# Per-build override
defs.build(features=["user_spend"], engine="duckdb")
```

**Feature Definition (unchanged):**

```python
import polars as pl
from mlforge import feature
from mlforge.metrics import Rolling
import mlforge.validators as v

@feature(
    keys=["user_id"],
    source="transactions.parquet",
    timestamp="event_time",
    metrics=[
        Rolling(
            windows=["7d", "30d"],
            aggregations={"amount": ["sum", "mean"]},
        ),
    ],
    validators={"user_id": [v.not_null()], "amount": [v.greater_than(0)]},
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    # Same code works with both Polars and DuckDB engines
    return df.select(
        pl.col("user_id"),
        pl.col("event_time"),
        pl.col("amount"),
    )
```

**Rolling Metrics Compilation:**

DuckDB uses native SQL window functions with `RANGE BETWEEN`:

```sql
SELECT
    user_id,
    event_time,
    SUM(amount) OVER w_7d AS "user_spend__amount__sum__1d__7d",
    AVG(amount) OVER w_7d AS "user_spend__amount__mean__1d__7d",
    SUM(amount) OVER w_30d AS "user_spend__amount__sum__1d__30d",
    AVG(amount) OVER w_30d AS "user_spend__amount__mean__1d__30d"
FROM feature_data
WINDOW
    w_7d AS (PARTITION BY user_id ORDER BY event_time
             RANGE BETWEEN INTERVAL '7 days' PRECEDING AND CURRENT ROW),
    w_30d AS (PARTITION BY user_id ORDER BY event_time
              RANGE BETWEEN INTERVAL '30 days' PRECEDING AND CURRENT ROW)
```

**Aggregation Mapping:**

| mlforge | DuckDB SQL |
|---------|------------|
| `count` | `COUNT` |
| `sum` | `SUM` |
| `mean` | `AVG` |
| `min` | `MIN` |
| `max` | `MAX` |
| `std` | `STDDEV_SAMP` |
| `median` | `MEDIAN` |

**Note on Polars vs DuckDB Parity:**

Polars uses `group_by_dynamic()` which operates on discrete time intervals, while DuckDB's `RANGE BETWEEN` is a continuous sliding window. Results may differ slightly depending on data distribution. Parity tests will validate differences are within acceptable tolerance. If significant differences are found, adjustments will be made to align behavior.

---

### 3. Redis Online Store

Real-time feature serving via Redis for low-latency inference.

**Motivation:**
- Enable real-time ML inference with feature retrieval
- Complete the offline → online feature store loop
- Provide foundation for future online stores (DynamoDB, etc.)

**API:**

```python
from mlforge import Definitions
from mlforge.store import LocalStore
from mlforge.online import RedisStore

offline_store = LocalStore(path="./feature_store")
online_store = RedisStore(host="localhost", port=6379, db=0)

defs = Definitions(
    store=offline_store,
    online_store=online_store,
)

# Build to offline store (default)
defs.build(features=["user_spend"])

# Build to online store
defs.build(features=["user_spend"], online=True)
```

**Reading from Online Store:**

```python
# Single entity
features = online_store.read(
    feature_name="user_spend",
    entity_keys={"user_id": "user_123"},
)
# Returns: {"amount__sum__7d": 1500.0, "amount__mean__7d": 214.28, ...}

# Batch read
features_batch = online_store.read_batch(
    feature_name="user_spend",
    entity_keys=[
        {"user_id": "user_123"},
        {"user_id": "user_456"},
    ],
)
```

**CLI:**

```bash
mlforge build --features user_spend --online
```

**Redis Data Model:**

```
Key: mlforge:{feature_name}:{entity_key_hash}
Value: JSON serialized feature values
TTL: Optional, configurable per RedisStore instance
```

**RedisStore Configuration:**

```python
RedisStore(
    host: str = "localhost",
    port: int = 6379,
    db: int = 0,
    password: str | None = None,
    ttl: int | None = None,  # seconds, None = no expiry
)
```

---

## Architecture Changes

### Package Restructure

Engines and compilers are reorganized into packages for extensibility:

```
src/mlforge/
├── engines/
│   ├── __init__.py          # Re-exports
│   ├── base.py              # Engine ABC
│   ├── polars_engine.py     # PolarsEngine
│   └── duckdb_engine.py     # DuckDBEngine (NEW)
├── compilers/
│   ├── __init__.py          # Re-exports
│   ├── base.py              # ComputeContext
│   ├── polars_compiler.py   # PolarsCompiler
│   └── duckdb_compiler.py   # DuckDBCompiler (NEW)
├── online.py                # OnlineStore ABC, RedisStore (NEW)
└── ...
```

### OnlineStore ABC

```python
class OnlineStore(ABC):
    @abstractmethod
    def write(self, feature_name: str, entity_keys: dict[str, str], values: dict[str, Any]) -> None: ...

    @abstractmethod
    def write_batch(self, feature_name: str, records: list[dict[str, Any]], entity_key_columns: list[str]) -> None: ...

    @abstractmethod
    def read(self, feature_name: str, entity_keys: dict[str, str]) -> dict[str, Any] | None: ...

    @abstractmethod
    def read_batch(self, feature_name: str, entity_keys: list[dict[str, str]]) -> list[dict[str, Any] | None]: ...
```

---

## Dependencies

New optional dependencies:

```toml
[project.optional-dependencies]
duckdb = ["duckdb>=1.0.0"]
redis = ["redis>=5.0.0"]
all = ["duckdb>=1.0.0", "redis>=5.0.0"]
```

**Installation:**

```bash
pip install mlforge           # Core only
pip install mlforge[duckdb]   # With DuckDB
pip install mlforge[redis]    # With Redis
pip install mlforge[all]      # Everything
```

---

## Breaking Changes

### Storage Layout

Feature storage now includes version directories. Existing features must be rebuilt:

```bash
# Migrate existing features
mlforge build --features user_spend --version 1.0.0
```

### Metadata Schema

- `FeatureMetadata.last_updated` renamed to `updated_at`
- New required field: `version: str`
- New required field: `created_at: str`
- New required field: `content_hash: str`
- New required field: `schema_hash: str`
- New required field: `config_hash: str`
- New optional field: `change_summary: dict`

Regenerate metadata after upgrade:

```bash
mlforge manifest --regenerate
```

### get_training_data() Signature

Features parameter now accepts version tuples:

```python
# Before (still works - uses latest)
get_training_data(spine, features=["user_spend"], store=store)

# After (with version)
get_training_data(
    spine,
    features=[
        "user_spend",                    # latest
        ("merchant_features", "1.0.0"),  # specific version
    ],
    store=store,
)
```

---

## Implementation Phases

| Phase | Description | Effort |
|-------|-------------|--------|
| 1 | Feature Versioning - Store layer | Medium |
| 2 | Feature Versioning - Metadata & Hashing | Medium |
| 3 | Feature Versioning - Auto-detection logic | Medium |
| 4 | Feature Versioning - Core & CLI | Medium |
| 5 | Feature Versioning - Retrieval | Low |
| 6 | Refactor engines/compilers to packages | Low |
| 7 | DuckDB Engine | Medium |
| 8 | DuckDB Compiler | Medium-High |
| 9 | DuckDB Result | Low |
| 10 | Online Store ABC | Low |
| 11 | Redis Store implementation | Medium |
| 12 | Online Store integration | Medium |
| 13 | Dependencies | Low |
| 14 | Tests (including Polars/DuckDB parity) | Medium |
| 15 | Documentation | Low |
| 16 | Examples | Low |

---

## Testing Strategy

### New Test Files

- `tests/test_versioning.py` - Auto-version detection, hash computation, version bumping
- `tests/test_engines.py` - DuckDB engine tests
- `tests/test_compilers.py` - DuckDB compiler, SQL generation
- `tests/test_online.py` - Redis store operations
- `tests/test_engine_parity.py` - Polars vs DuckDB result comparison

### Version Detection Testing

```python
def test_auto_version_major_bump():
    """Removing columns should trigger MAJOR version bump."""
    # Build v1.0.0 with columns [a, b, c]
    # Build again with columns [a, b] (c removed)
    # Assert new version is 2.0.0

def test_auto_version_minor_bump():
    """Adding columns should trigger MINOR version bump."""
    # Build v1.0.0 with columns [a, b]
    # Build again with columns [a, b, c] (c added)
    # Assert new version is 1.1.0

def test_auto_version_patch_bump():
    """Same schema/config should trigger PATCH version bump."""
    # Build v1.0.0
    # Build again with same schema/config but new data
    # Assert new version is 1.0.1
```

### Parity Testing

Compare Polars and DuckDB rolling metric outputs:

```python
def test_rolling_metrics_parity():
    """Ensure Polars and DuckDB produce equivalent results."""
    # Create test data with known values
    # Run same feature with both engines
    # Compare results within tolerance (e.g., 1e-6 for floats)
    # Flag and investigate significant differences
```

---

## Future Considerations

Features deferred to future releases:

- **v0.5.x**: Feature function code hashing (detect logic changes)
- **v0.6.0**: Spark compute backend
- **v0.6.0+**: `mlforge serve` REST API (FastAPI)
- **v0.6.0+**: GCS storage backend
- **v0.7.0**: Airflow/Dagster operators
- **v0.8.0**: DynamoDB online store
- **Future**: Feature dependencies and lineage tracking
- **Future**: Feature registry with search/discovery

---

## Migration Guide

### From v0.4.0 to v0.5.0

1. **Rebuild features with versions:**
   ```bash
   # First build will create version 1.0.0
   mlforge build --features user_spend

   # Or explicitly set initial version
   mlforge build --features user_spend --version 1.0.0
   ```

2. **Regenerate manifest:**
   ```bash
   mlforge manifest --regenerate
   ```

3. **Update get_training_data() calls** (optional - for version pinning):
   ```python
   # Pin specific versions for reproducibility
   get_training_data(
       spine,
       features=[("user_spend", "1.0.0")],
       store=store,
   )
   ```

4. **Install optional dependencies** (if using new features):
   ```bash
   pip install mlforge[duckdb,redis]
   ```

---

## Design Decisions

### Why Version-Based Directories (not Hash-Based)?

| Aspect | Hash-Based | Version-Based (Chosen) |
|--------|------------|------------------------|
| **Readability** | `abc123def456` is meaningless | `1.1.0` is clear |
| **Sorting** | Random order in file browser | Natural version ordering |
| **Debugging** | Need to look up hash → version | Immediately obvious |
| **CLI output** | `Built: abc123def456` | `Built: 1.1.0` |
| **Conversations** | "Use hash abc123..." | "Use version 1.1.0" |

Content hashes are still stored in metadata for integrity verification and change detection.

### Why Auto-Versioning by Default?

- Reduces cognitive load on users
- Ensures versions have semantic meaning (breaking vs non-breaking)
- Aligns with DVC's philosophy of automatic change detection
- Explicit override available when users need control

### Why Skip Code Hashing for v0.5.0?

- Simpler initial implementation
- Schema/config changes cover most meaningful cases
- Code hashing is sensitive to whitespace/formatting
- Can be added in v0.5.x as enhancement

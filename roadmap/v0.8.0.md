# v0.8.0 - Enterprise & Databricks

**Target Release:** Q2 2026
**Status:** Active
**Breaking Changes:** Yes (deprecates Rolling API, backwards compatible during transition)

## Overview

v0.8.0 focuses on **enterprise scale, Databricks integration, and developer experience** - enabling production-scale feature computation with PySpark, native Databricks storage, incremental builds, and a redesigned metrics API.

Key improvements:

1. **PySpark Engine** - Production-scale distributed compute for billions of rows
2. **Unity Catalog Store** - Databricks-native offline storage with governance and lineage
3. **Databricks Online Tables** - Low-latency feature serving with auto-sync from Delta
4. **Incremental Builds** - Process only new/changed data instead of full refreshes
5. **Tag-Based Scheduling** - Schedule builds using feature tags (`daily`, `hourly`, etc.)
6. **Metric Definitions API** - New composable `Aggregate` API replacing `Rolling`

## Features Summary

| Feature | Status | Effort | Breaking | Spec |
|---------|--------|--------|----------|------|
| [PySpark Engine](v0.8.0/pyspark-engine.md) | **Done** | High | No | [Link](v0.8.0/pyspark-engine.md) |
| [Unity Catalog Store](v0.8.0/unity-catalog-store.md) | **Done** | Medium | No | [Link](v0.8.0/unity-catalog-store.md) |
| [Databricks Online Tables](v0.8.0/databricks-online-tables.md) | **Done** | Medium | No | [Link](v0.8.0/databricks-online-tables.md) |
| [Incremental Builds](v0.8.0/incremental-builds.md) | **Done** | Medium | No | [Link](v0.8.0/incremental-builds.md) |
| [Tag-Based Scheduling](v0.8.0/tag-scheduling.md) | **Done** | Low | No | [Link](v0.8.0/tag-scheduling.md) |
| [Metric Definitions API](v0.8.0/metric-definitions-api.md) | Planned | High | Yes | [Link](v0.8.0/metric-definitions-api.md) |
| Code Organization (split cli.py) | Planned | Low | No | - |

## Theme: Databricks-Native

v0.8.0 enables a complete Databricks-native workflow:

```
Feature Definition (Python)
         │
         ▼
    PySpark Engine ──────────────────────────────┐
         │                                        │
         ▼                                        ▼
  Unity Catalog Store              Databricks Online Tables
  (Delta Lake offline)             (Low-latency serving)
         │                                        │
         ▼                                        ▼
  Training (Notebooks)              Inference (Model Serving)
```

## Implementation Progress

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | PySpark Engine | **Done** |
| 2 | PySpark Compiler (rolling metrics) | **Done** |
| 3 | Unity Catalog Store | **Done** |
| 4 | Version mapping (semver ↔ Delta) | **Done** |
| 5 | Databricks Online Tables | **Done** |
| 6 | Incremental Builds | **Done** |
| 7 | Tag-Based Scheduling | **Done** |
| 8 | Metric Definitions API (Field, Aggregate) | Planned |
| 9 | Profile integration | **Done** |
| 10 | Engine parity tests | **Done** |
| 11 | Documentation | In Progress |
| 12 | Databricks example | **Done** |

## Version Definition of Done

All items must be complete before v0.8.0 can be released:

### Features
- [x] PySpark Engine complete
- [x] PySpark Compiler (rolling metrics) complete
- [x] Unity Catalog Store complete
- [x] Databricks Online Tables complete
- [x] Incremental Builds complete (`--since`, `--until`, `--last`, `--full-refresh`)
- [x] Tag-Based Scheduling complete (`--schedule` flag)
- [ ] Metric Definitions API complete (Field, Aggregate, shorthand functions)
- [ ] Rolling() shows deprecation warning
- [x] Profile integration complete

### Quality
- [x] All tests passing
- [x] Coverage >= 80%
- [x] No ruff/ty errors
- [x] Engine parity tests passing (Polars vs DuckDB vs PySpark)
- [ ] Code review completed

### Documentation
- [ ] API docs updated for all features
- [ ] User guide updated
- [ ] Databricks deployment guide
- [x] Databricks example (`examples/databricks/`)
- [ ] CHANGELOG updated

### Verification
- [ ] Works on Databricks workspace
- [ ] Works with Databricks Connect
- [ ] All features verified end-to-end

## Feature Highlights

### PySpark Engine

Distributed compute for production-scale datasets:

```python
from pyspark.sql import DataFrame as SparkDataFrame

@mlf.feature(
    source=transactions,
    entities=[user],
    timestamp="event_time",
    interval="30d",
    metrics=[spend_metrics],
)
def user_spend(df: SparkDataFrame) -> SparkDataFrame:
    return df.select("user_id", "event_time", "amt")

# Use PySpark engine
defs = mlf.Definitions(
    name="my-project",
    features=[user_spend],
    offline_store=mlf.UnityCatalogStore(catalog="main", schema="features"),
    engine="pyspark",
)
```

Features:
- Explicit `pyspark.sql.DataFrame` type hints required
- Auto-creates SparkSession or uses existing (Databricks)
- Native Spark window functions for rolling metrics
- Databricks Connect supported

### Unity Catalog Store

Enterprise-grade feature storage with governance:

```python
offline_store = mlf.UnityCatalogStore(
    catalog="main",
    schema="features",
)
```

Features:
- Delta Lake storage with ACID transactions
- Hybrid versioning: mlforge semver + Delta time travel
- Single `_mlforge_metadata` table for all features
- Unity Catalog governance, lineage, access control

### Databricks Online Tables

Low-latency serving with auto-sync:

```python
online_store = mlf.DatabricksOnlineStore(
    catalog="main",
    schema="features_online",
    sync_mode="triggered",
)
```

Features:
- Requires Unity Catalog Store as offline store
- Auto-creates Online Tables if permissions allow
- Sync modes: snapshot, triggered, continuous
- Millisecond-latency point lookups

### Incremental Builds

Process only new or changed data:

```bash
# Process data since last build (future default)
mlforge build --features user_spend

# Process specific date range
mlforge build --features user_spend --since 2026-01-01 --until 2026-01-15

# Process last N days/hours
mlforge build --features user_spend --last 7d

# Force full refresh
mlforge build --features user_spend --full-refresh
```

Python API:
```python
defs.build(feature_names=["user_spend"], since="2026-01-01", until="2026-01-15")
defs.build(feature_names=["user_spend"], last="7d")
defs.build(feature_names=["user_spend"], full_refresh=True)
```

### Tag-Based Scheduling

Schedule builds using feature tags:

```python
@mlf.feature(
    source="data/transactions.parquet",
    keys=["user_id"],
    tags=["users", "daily"],  # Schedule tag
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    return df
```

```bash
# Build all daily features
mlforge build --schedule daily

# Build multiple schedules
mlforge build --schedule daily,hourly

# List features by schedule
mlforge list features --schedule daily
```

Reserved schedule tags: `realtime`, `hourly`, `daily`, `weekly`, `monthly`, `on-demand`

### Metric Definitions API

New composable API replacing `Rolling`:

```python
# Before (deprecated in v0.8.0)
metrics=[mlf.Rolling(windows=["7d", "30d"], aggregations={"amt": ["sum", "count"]})]

# After (preferred)
metrics=[
    mlf.Sum("amt", windows=["7d", "30d"], name="total_spend", unit="USD"),
    mlf.Count("amt", windows=["7d", "30d"], name="txn_count"),
    mlf.Mean("amt", windows=["7d", "30d"], name="avg_amt"),
]
```

Benefits:
- Explicit metadata (name, description, unit) per aggregation
- Self-documenting output column names (`total_spend_7d` vs `amt__sum__7d`)
- Shorthand functions: `Sum`, `Count`, `Mean`, `Min`, `Max`, `Std`, `Median`

## Complete Example

### Databricks Workflow

```python
# definitions.py
import mlforge as mlf
from pyspark.sql import DataFrame as SparkDataFrame
from my_project import features

defs = mlf.Definitions(
    name="fraud-detection",
    features=[features],
    offline_store=mlf.UnityCatalogStore(catalog="main", schema="fraud_features"),
    online_store=mlf.DatabricksOnlineStore(catalog="main", schema="fraud_online"),
    engine="pyspark",
)
```

```python
# features.py
import mlforge as mlf
from pyspark.sql import DataFrame as SparkDataFrame

transactions = mlf.Source("main.raw.transactions")  # Delta table

user = mlf.Entity(name="user", join_key="user_id")

spend_metrics = mlf.Rolling(
    windows=["1d", "7d", "30d"],
    aggregations={"amt": ["sum", "mean", "count"]},
)

@mlf.feature(
    source=transactions,
    entities=[user],
    timestamp="event_time",
    interval="1d",
    metrics=[spend_metrics],
)
def user_spend(df: SparkDataFrame) -> SparkDataFrame:
    return df.select("user_id", "event_time", "amt")
```

```bash
# Build features (offline)
mlforge build --profile databricks-prod

# Build and sync to Online Tables
mlforge build --profile databricks-prod --online
```

### Profile Configuration

```yaml
# mlforge.yaml
profiles:
  databricks-dev:
    offline_store:
      KIND: unity-catalog
      catalog: dev
      schema: features
    engine: pyspark
    
  databricks-prod:
    offline_store:
      KIND: unity-catalog
      catalog: prod
      schema: features
    online_store:
      KIND: databricks-online-tables
      catalog: prod
      schema: features_online
      sync_mode: triggered
    engine: pyspark
```

## Dependencies

New optional dependencies for v0.8.0:

```toml
[project.optional-dependencies]
pyspark = ["pyspark>=3.5.0"]
databricks = [
    "pyspark>=3.5.0",
    "databricks-sdk>=0.20.0",
    "delta-spark>=3.0.0",
]
all = [
    # ... existing deps
    "pyspark>=3.5.0",
    "databricks-sdk>=0.20.0",
    "delta-spark>=3.0.0",
]
```

**Installation:**

```bash
pip install mlforge[pyspark]     # PySpark only
pip install mlforge[databricks]  # Full Databricks support
```

## Migration Guide

### From v0.7.0 to v0.8.0

One breaking change: `Rolling()` is deprecated in favor of the new Metric Definitions API.

1. **Migrate from Rolling to Aggregate (recommended):**
   ```python
   # Before (deprecated, shows warning)
   metrics=[mlf.Rolling(windows=["7d", "30d"], aggregations={"amt": ["sum", "count"]})]
   
   # After (preferred)
   metrics=[
       mlf.Sum("amt", windows=["7d", "30d"], name="total_spend"),
       mlf.Count("amt", windows=["7d", "30d"], name="txn_count"),
   ]
   ```

2. **Optional: Use incremental builds:**
   ```bash
   # Process only recent data
   mlforge build --last 7d
   
   # Force full rebuild
   mlforge build --full-refresh
   ```

3. **Optional: Use tag-based scheduling:**
   ```python
   @mlf.feature(tags=["daily"])
   def user_spend(df): ...
   ```
   ```bash
   mlforge build --schedule daily
   ```

4. **Optional: Use PySpark for large datasets:**
   ```python
   # Before (Polars/DuckDB)
   defs = mlf.Definitions(..., engine="duckdb")
   
   # After (PySpark)
   defs = mlf.Definitions(..., engine="pyspark")
   ```

5. **Optional: Use Unity Catalog on Databricks:**
   ```python
   # Before (S3)
   offline_store = mlf.S3Store(bucket="my-bucket")
   
   # After (Unity Catalog)
   offline_store = mlf.UnityCatalogStore(catalog="main", schema="features")
   ```

6. **Optional: Use Databricks Online Tables:**
   ```python
   # Before (Redis/DynamoDB)
   online_store = mlf.RedisStore(host="localhost")
   
   # After (Databricks Online Tables)
   online_store = mlf.DatabricksOnlineStore(catalog="main", schema="features_online")
   ```

## Future Considerations

Features deferred to future releases:

- **v0.9.0**: Data quality metrics, drift detection, Prometheus metrics
- **v0.9.0**: Catalog generation, manifest search
- **v0.9.0**: Lineage tracking and visualization
- **v0.9.0**: Registry backend abstraction (SQLite, PostgreSQL)
- **v0.9.0**: `Rolling()` deprecation warning becomes more prominent
- **v0.10.0**: Apache Iceberg support
- **v0.10.0**: Text embeddings
- **v0.10.0+**: Deeper Databricks Connect integration (CLI commands, profile config)
- **v1.0.0**: Production ready, stable APIs, `Rolling()` removed

# v0.6.0 - Developer Ergonomics & Source Abstraction

**Target Release:** Q2 2025
**Status:** Planned
**Breaking Changes:** Yes (CLI `list` command requires subcommand)

## Overview

v0.6.0 focuses on **developer ergonomics** - reducing boilerplate in feature definitions by automating entity key generation, timestamp parsing, and providing a proper source abstraction. This release makes feature definitions cleaner and more declarative.

Key improvements:

1. **Source Abstraction** - First-class `Source` class with format auto-detection and typed configuration
2. **Entity Definition** - Explicit `Entity` objects with automatic surrogate key generation
3. **Timestamp Handling** - Auto-detect datetime formats with explicit override capability
4. **Definitions Enhancement** - Auto-collection of sources and entities with discovery methods
5. **GCS Storage Backend** - Google Cloud Storage support for source loading
6. **CLI Enhancements** - New subcommands for listing and inspecting entities, sources, and versions

---

## Motivation

Current feature definitions require repetitive boilerplate:

```python
# Current (v0.5.0) - verbose and manual
SOURCE = "data/transactions.parquet"
USER_KEY = "user_id"

with_user_id = mlf.entity_key("first", "last", "dob", alias=USER_KEY)

@mlf.feature(
    source=SOURCE,
    keys=[USER_KEY],
    timestamp="transaction_date",
    interval="30d",
    metrics=[spend_metrics],
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    return df.pipe(with_user_id).select(
        pl.col("user_id"),
        pl.col("trans_date_trans_time")
            .str.to_datetime("%Y-%m-%d %H:%M:%S")
            .alias("transaction_date"),
        pl.col("amt"),
    )
```

**Pain points:**
- Manual `pipe()` call for entity key generation in every feature function
- Manual datetime parsing with explicit format strings
- No IDE support for source configuration options
- No way to discover what entities/sources are in use

---

## Features

### 1. Source Abstraction

A single `Source` class that handles location (local, S3, GCS) and format (Parquet, CSV, Delta) with typed configuration objects.

**Design Principles:**
- **Location inferred from path** - `s3://`, `gs://`, or local path
- **Format auto-detected from extension** - `.parquet`, `.csv`, or explicit
- **Typed format options** - IDE autocomplete for format-specific settings
- **Simple common case** - `Source("path.parquet")` just works

**API:**

```python
import mlforge as mlf

# Simple case - auto-detect everything
transactions = mlf.Source("data/transactions.parquet")

# S3 source
s3_transactions = mlf.Source("s3://bucket/transactions.parquet")

# GCS source (NEW in v0.6.0)
gcs_transactions = mlf.Source("gs://bucket/transactions.parquet")

# With explicit name
events = mlf.Source(
    path="data/events.parquet",
    name="events",
)

# CSV with format options (typed - IDE autocomplete works)
csv_events = mlf.Source(
    path="data/events.csv",
    format=mlf.CSVFormat(
        delimiter="|",
        has_header=True,
        skip_rows=1,
    ),
)

# Delta Lake with specific version
delta_features = mlf.Source(
    path="s3://bucket/delta_table/",
    format=mlf.DeltaFormat(version=5),
)
```

**Format Classes:**

```python
@dataclass
class ParquetFormat:
    """Parquet format options."""
    row_groups: list[int] | None = None
    columns: list[str] | None = None

@dataclass
class CSVFormat:
    """CSV format options."""
    delimiter: str = ","
    has_header: bool = True
    quote_char: str = '"'
    skip_rows: int = 0

@dataclass
class DeltaFormat:
    """Delta Lake format options."""
    version: int | None = None  # None = latest
```

**Validation:**
- **Definition time:** Path syntax, required parameters (lightweight, no I/O)
- **Build time:** File exists, schema matches, credentials work (full validation)

---

### 2. Entity Definition

Explicit `Entity` objects that declare join keys and optional surrogate key generation from source columns.

**Design Principles:**
- **Reusable across features** - Define once, use in multiple features
- **Automatic key generation** - Surrogate keys created from `from_columns`
- **Explicit naming** - Both logical `name` and physical `join_key`

**API:**

```python
import mlforge as mlf

# Simple entity - direct column passthrough
merchant = mlf.Entity(
    name="merchant",
    join_key="merchant_id",
)

# Surrogate key from multiple columns
user = mlf.Entity(
    name="user",
    join_key="user_id",
    from_columns=["first", "last", "dob"],
)

# Composite key
user_merchant = mlf.Entity(
    name="user_merchant",
    join_key=["user_id", "merchant_id"],
)
```

**Behavior:**
- When `from_columns` is provided, the engine generates the `join_key` column using the existing `surrogate_key()` hashing logic
- Source columns remain in DataFrame for user access
- Feature function selects which columns to include in output

---

### 3. Timestamp Handling

Automatic datetime format detection with explicit override capability.

**Design Principles:**
- **Auto-detect common formats** - ISO 8601, common date patterns
- **Keep original column name** - No forced aliasing
- **Explicit override available** - `Timestamp` class for edge cases
- **Helpful error messages** - Suggest common formats on failure

**API:**

```python
import mlforge as mlf

# Auto-detect format (most common case)
@mlf.feature(
    source=transactions,
    entities=[user],
    timestamp="trans_date_trans_time",  # Auto-detect and parse
    ...
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    ...

# Explicit format when auto-detect fails
@mlf.feature(
    source=transactions,
    entities=[user],
    timestamp=mlf.Timestamp(
        column="trans_date_trans_time",
        format="%Y-%m-%d %H:%M:%S",
    ),
    ...
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    ...

# With alias (rename output column)
@mlf.feature(
    source=transactions,
    entities=[user],
    timestamp=mlf.Timestamp(
        column="trans_date_trans_time",
        format="%Y-%m-%d %H:%M:%S",
        alias="event_time",
    ),
    ...
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    ...
```

**Auto-Detection Strategy:**

1. Check if column is already datetime type → use as-is
2. Try common formats in order:
   - ISO 8601: `%Y-%m-%dT%H:%M:%S`, `%Y-%m-%d %H:%M:%S`
   - Date only: `%Y-%m-%d`, `%d/%m/%Y`, `%m/%d/%Y`
   - With timezone: `%Y-%m-%dT%H:%M:%S%z`
3. On failure, raise error with suggestions:

```
TimestampParseError: Could not auto-detect datetime format for column 'trans_date_trans_time'.

Sample values: ['2024-01-15 14:30:00', '2024-01-16 09:15:30']

Try specifying format explicitly:
    timestamp=mlf.Timestamp(
        column="trans_date_trans_time",
        format="%Y-%m-%d %H:%M:%S",  # Suggested based on sample
    )

Common formats:
    "%Y-%m-%d %H:%M:%S"    → 2024-01-15 14:30:00
    "%Y-%m-%dT%H:%M:%S"    → 2024-01-15T14:30:00
    "%d/%m/%Y %H:%M:%S"    → 15/01/2024 14:30:00
    "%Y-%m-%d"             → 2024-01-15
```

---

### 4. Definitions Enhancement

New discovery methods for auto-collected sources and entities.

**Design Principles:**
- **Auto-collection** - Sources and entities extracted from registered features
- **Discovery methods** - Introspect what's registered
- **Backwards compatible** - Existing API unchanged

**API:**

```python
import mlforge as mlf
from transactions import features

# Existing API - unchanged
defs = mlf.Definitions(
    name="Transactions features.",
    features=[features],  # Module auto-discovery still works
    offline_store=mlf.LocalStore(path="./feature_store"),
)

# NEW: Discovery methods
defs.list_features()   # → ["user_spend", "merchant_spend"]
defs.list_entities()   # → ["user", "merchant"] (auto-collected from features)
defs.list_sources()    # → ["transactions"] (auto-collected from features)
```

---

### 5. GCS Storage Backend

Google Cloud Storage support for source loading, completing the cloud storage story.

**API:**

```python
# GCS source
gcs_source = mlf.Source("gs://bucket/path/data.parquet")

# Works with all engines
@mlf.feature(
    source=mlf.Source("gs://my-bucket/transactions.parquet"),
    entities=[user],
    timestamp="event_time",
)
def user_spend(df): ...
```

**Implementation:**
- Polars: Uses `pl.read_parquet("gs://...")` (requires `gcsfs`)
- DuckDB: Uses `read_parquet('gs://...')` (requires `duckdb[gcs]`)

**Credential Handling:**
- Delegates to underlying library (Polars/DuckDB)
- Uses standard GCP credential resolution (env vars, service account, etc.)

---

### 6. CLI Enhancements

New subcommands for listing and inspecting entities, sources, and versions.

**Breaking Change:** `mlforge list` now requires an explicit subcommand.

**Commands:**

```bash
# List commands
mlforge list features              # List all features
mlforge list features --tags users # Filter features by tags
mlforge list entities              # List all entities
mlforge list sources               # List all sources
mlforge list versions <feature>    # List versions of a feature

# Inspect commands
mlforge inspect <feature>          # Existing: inspect feature metadata
mlforge inspect entity <name>      # NEW: inspect entity details
mlforge inspect source <name>      # NEW: inspect source details
```

**Example Output:**

```bash
$ mlforge list features

Features
┌────────────────┬──────────────┬─────────────────────────────┬──────────┐
│ Name           │ Keys         │ Source                      │ Tags     │
├────────────────┼──────────────┼─────────────────────────────┼──────────┤
│ user_spend     │ user_id      │ data/transactions.parquet   │ users    │
│ merchant_spend │ merchant_id  │ data/transactions.parquet   │ merchants│
└────────────────┴──────────────┴─────────────────────────────┴──────────┘

2 features found
```

```bash
$ mlforge list entities

Entities
┌──────────┬──────────────┬─────────────────────────┐
│ Name     │ Join Key     │ From Columns            │
├──────────┼──────────────┼─────────────────────────┤
│ user     │ user_id      │ first, last, dob        │
│ merchant │ merchant_id  │ -                       │
└──────────┴──────────────┴─────────────────────────┘

2 entities found
```

```bash
$ mlforge list sources

Sources
┌──────────────┬───────────────────────────────┬──────────┬──────────┐
│ Name         │ Path                          │ Format   │ Location │
├──────────────┼───────────────────────────────┼──────────┼──────────┤
│ transactions │ data/transactions.parquet     │ parquet  │ local    │
└──────────────┴───────────────────────────────┴──────────┴──────────┘

1 source found
```

```bash
$ mlforge list versions user_spend

Versions for 'user_spend'
┌─────────┬─────────────────────┬───────────┬────────────┐
│ Version │ Created             │ Rows      │ Status     │
├─────────┼─────────────────────┼───────────┼────────────┤
│ 1.0.0   │ 2025-01-15 10:00:00 │ 150,000   │            │
│ 1.0.1   │ 2025-01-20 14:30:00 │ 155,000   │            │
│ 1.1.0   │ 2025-01-25 09:15:00 │ 160,000   │ latest     │
└─────────┴─────────────────────┴───────────┴────────────┘

3 versions found
```

```bash
$ mlforge inspect entity user

Entity: user
┌─────────────────┬─────────────────────────┐
│ Property        │ Value                   │
├─────────────────┼─────────────────────────┤
│ Name            │ user                    │
│ Join Key        │ user_id                 │
│ From Columns    │ first, last, dob        │
│ Used In         │ user_spend, user_txns   │
└─────────────────┴─────────────────────────┘
```

```bash
$ mlforge inspect source transactions

Source: transactions
┌─────────────────┬───────────────────────────────┐
│ Property        │ Value                         │
├─────────────────┼───────────────────────────────┤
│ Name            │ transactions                  │
│ Path            │ data/transactions.parquet     │
│ Format          │ parquet                       │
│ Location        │ local                         │
│ Used In         │ user_spend, merchant_spend    │
└─────────────────┴───────────────────────────────┘
```

---

## Complete Example

**Before (v0.5.0):**

```python
# features.py
import mlforge as mlf
import polars as pl
from datetime import timedelta

SOURCE = "data/transactions.parquet"
USER_KEY = "user_id"
MERCHANT_KEY = "merchant_id"

with_user_id = mlf.entity_key("first", "last", "dob", alias=USER_KEY)
with_merchant_id = mlf.entity_key("merchant", alias=MERCHANT_KEY)

spend_metrics = mlf.Rolling(
    windows=["7d", "30d"],
    aggregations={"amt": ["sum", "mean"]},
)

@mlf.feature(
    source=SOURCE,
    keys=[USER_KEY],
    timestamp="transaction_date",
    interval=timedelta(days=30),
    metrics=[spend_metrics],
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    return df.pipe(with_user_id).select(
        pl.col("user_id"),
        pl.col("trans_date_trans_time")
            .str.to_datetime("%Y-%m-%d %H:%M:%S")
            .alias("transaction_date"),
        pl.col("amt"),
    )

@mlf.feature(
    source=SOURCE,
    keys=[MERCHANT_KEY],
    timestamp="transaction_date",
    interval=timedelta(days=1),
    metrics=[spend_metrics],
)
def merchant_spend(df: pl.DataFrame) -> pl.DataFrame:
    return df.pipe(with_merchant_id).select(
        pl.col("merchant_id"),
        pl.col("trans_date_trans_time")
            .str.to_datetime("%Y-%m-%d %H:%M:%S")
            .alias("transaction_date"),
        pl.col("amt"),
    )
```

```python
# definitions.py
import mlforge as mlf
from transactions import features

defs = mlf.Definitions(
    name="Transactions features.",
    features=[features],
    offline_store=mlf.LocalStore(path="./feature_store"),
)
```

**After (v0.6.0):**

```python
# features.py
import mlforge as mlf
import polars as pl
from datetime import timedelta

# Sources
transactions = mlf.Source("data/transactions.parquet")

# Entities
user = mlf.Entity(
    name="user",
    join_key="user_id",
    from_columns=["first", "last", "dob"],
)

merchant = mlf.Entity(
    name="merchant",
    join_key="merchant_id",
    from_columns=["merchant"],
)

# Metrics
spend_metrics = mlf.Rolling(
    windows=["7d", "30d"],
    aggregations={"amt": ["sum", "mean"]},
)

# Features - cleaner, less boilerplate
@mlf.feature(
    source=transactions,
    entities=[user],
    timestamp="trans_date_trans_time",  # Auto-parsed
    interval=timedelta(days=30),
    metrics=[spend_metrics],
)
def user_spend(df: pl.DataFrame) -> pl.DataFrame:
    return df.select("user_id", "trans_date_trans_time", "amt")


@mlf.feature(
    source=transactions,
    entities=[merchant],
    timestamp="trans_date_trans_time",
    interval=timedelta(days=1),
    metrics=[spend_metrics],
)
def merchant_spend(df: pl.DataFrame) -> pl.DataFrame:
    return df.select("merchant_id", "trans_date_trans_time", "amt")
```

```python
# definitions.py - unchanged structure
import mlforge as mlf
from transactions import features

defs = mlf.Definitions(
    name="Transactions features.",
    features=[features],
    offline_store=mlf.LocalStore(path="./feature_store"),
)

# NEW: Discovery methods
defs.list_entities()  # → ["user", "merchant"]
defs.list_sources()   # → ["transactions"]
```

**Improvements:**
- No manual `pipe()` calls for entity keys
- No manual datetime parsing
- Cleaner, more readable feature functions
- Discovery methods for introspection
- CLI commands for exploring entities/sources/versions

---

## Backwards Compatibility

The current API is fully preserved for feature definitions. v0.6.0 extends it with new abstractions while maintaining compatibility:

**Compatibility Matrix:**

| Parameter | Old Style | New Style | Behavior |
|-----------|-----------|-----------|----------|
| `source` | `"path.parquet"` | `mlf.Source("path.parquet")` | String wrapped internally |
| `keys` | `["user_id"]` | `entities=[user]` | Old style = no auto-transform |
| `timestamp` | `"col_name"` | `mlf.Timestamp(...)` | String = assume datetime type |
| `features` | `[module]` or `[fn, fn]` | Same | No change |
| `offline_store` | Same | Same | No change |

**Migration is optional for Python API.** Existing code continues to work. New features can be defined using either style.

**CLI Breaking Change:** `mlforge list` now requires a subcommand (`features`, `entities`, `sources`, `versions`).

---

## Architecture Changes

### Package Structure

```
src/mlforge/
├── sources/
│   ├── __init__.py          # Re-exports Source, Format classes
│   ├── base.py              # Source class, Format ABC
│   └── formats.py           # ParquetFormat, CSVFormat, DeltaFormat
├── entities.py              # Entity class
├── timestamps.py            # Timestamp class, auto-detection logic
├── engines/                 # From v0.5.0
│   ├── __init__.py
│   ├── base.py
│   ├── polars_engine.py
│   └── duckdb_engine.py
├── compilers/               # From v0.5.0
│   └── ...
├── core.py                  # Updated @feature decorator, Definitions
├── cli.py                   # Updated with new subcommands
├── logging.py               # New table formatters
└── ...
```

### Engine Updates

Engines updated to handle new abstractions:

```python
class Engine(ABC):
    @abstractmethod
    def execute(self, feature: Feature) -> ResultKind:
        """Execute feature computation.

        Engine responsibilities (v0.6.0 additions):
        1. Load source data (using Source abstraction)
        2. Apply entity key transformations (from Entity definitions)
        3. Parse timestamps (auto-detect or from Timestamp config)
        4. Execute user's feature function
        5. Compute metrics
        6. Run validators
        """
        ...
```

---

## Dependencies

New optional dependencies for GCS and Delta:

```toml
[project.optional-dependencies]
gcs = ["gcsfs>=2024.1.0"]
delta = ["deltalake>=0.15.0"]
all = ["duckdb>=1.0.0", "redis>=5.0.0", "gcsfs>=2024.1.0", "deltalake>=0.15.0"]
```

**Installation:**

```bash
pip install mlforge[gcs]      # With GCS support
pip install mlforge[delta]    # With Delta Lake support
pip install mlforge[all]      # Everything
```

---

## Implementation Phases

| Phase | Description | Effort |
|-------|-------------|--------|
| 1 | Source abstraction (`Source`, `Format` classes) | Medium |
| 2 | Format auto-detection and validation | Low |
| 3 | Entity class implementation | Medium |
| 4 | Timestamp class and auto-detection | Medium |
| 5 | Update `@feature` decorator for new types | Medium |
| 6 | Update `Definitions` with discovery methods | Low |
| 7 | Engine updates (apply entity/timestamp transforms) | Medium |
| 8 | GCS storage backend | Low |
| 9 | Error messages (datetime format suggestions) | Low |
| 10 | Backwards compatibility layer | Low |
| 11 | CLI: Refactor `list` to use subcommands | Low |
| 12 | CLI: Add `list entities`, `list sources` | Low |
| 13 | CLI: Add `list versions` | Low |
| 14 | CLI: Add `inspect entity`, `inspect source` | Low |
| 15 | Logging: New table formatters | Low |
| 16 | Tests | Medium |
| 17 | Documentation | Low |
| 18 | Update examples | Low |

---

## Testing Strategy

### New Test Files

- `tests/test_sources.py` - Source class, format detection, format options
- `tests/test_entities.py` - Entity class, surrogate key generation
- `tests/test_timestamps.py` - Timestamp parsing, auto-detection, error messages

### Key Test Cases

```python
# Source tests
def test_source_format_auto_detection():
    """Format should be detected from file extension."""
    assert Source("data.parquet").format == ParquetFormat()
    assert Source("data.csv").format == CSVFormat()

def test_source_location_inference():
    """Location should be inferred from path prefix."""
    assert Source("data.parquet").location == "local"
    assert Source("s3://bucket/data.parquet").location == "s3"
    assert Source("gs://bucket/data.parquet").location == "gcs"

def test_source_csv_format_options():
    """CSV format options should be applied."""
    source = Source("data.csv", format=CSVFormat(delimiter="|"))
    # Verify delimiter is used during load

# Entity tests
def test_entity_surrogate_key_generation():
    """Entity with from_columns should generate surrogate key."""
    user = Entity(name="user", join_key="user_id", from_columns=["first", "last"])
    # Apply to DataFrame and verify user_id column is generated

def test_entity_passthrough():
    """Entity without from_columns should pass through column."""
    merchant = Entity(name="merchant", join_key="merchant_id")
    # Verify merchant_id is used directly

# Timestamp tests
def test_timestamp_auto_detect_iso8601():
    """Should auto-detect ISO 8601 format."""
    # DataFrame with "2024-01-15T10:30:00" strings
    # Verify automatic parsing

def test_timestamp_auto_detect_failure_message():
    """Should provide helpful error message on failure."""
    # DataFrame with unparseable format
    # Verify error includes suggestions

# Definitions tests
def test_definitions_list_entities():
    """Entities should be discoverable from features."""
    defs = Definitions(features=[user_spend], offline_store=store)
    assert "user" in defs.list_entities()

def test_definitions_list_sources():
    """Sources should be discoverable from features."""
    defs = Definitions(features=[user_spend], offline_store=store)
    assert "transactions" in defs.list_sources()

# CLI tests
def test_cli_list_features():
    """mlforge list features should display features table."""
    ...

def test_cli_list_entities():
    """mlforge list entities should display entities table."""
    ...

def test_cli_list_sources():
    """mlforge list sources should display sources table."""
    ...

def test_cli_list_versions():
    """mlforge list versions <feature> should display versions table."""
    ...
```

---

## Future Considerations

Features deferred to future releases:

- **v0.7.0**: Spark compute backend
- **v0.7.0**: `mlforge serve` REST API (FastAPI)
- **v0.8.0**: Airflow/Dagster operators
- **v0.8.0**: DynamoDB online store
- **Future**: Feature dependencies and lineage tracking
- **Future**: Feature registry with search/discovery
- **Future**: Stream sources (Kafka, Kinesis)

---

## Design Decisions

### Why Separate Format Classes (not kwargs on Source)?

| Aspect | Kwargs on Source | Format Classes (Chosen) |
|--------|------------------|-------------------------|
| **IDE support** | Shows all params (many irrelevant) | Shows only relevant params |
| **Type safety** | Mixed concerns in one class | Each Format has own params |
| **Deep modules** | Source becomes shallow | Source is clean, Formats focused |
| **Discoverability** | Many params to scan | `mlf.CSVFormat` is clear |

Per Ousterhout's design principles, we avoid "shallow modules" by keeping `Source` focused on location/name and delegating format-specific complexity to `Format` classes.

### Why Auto-Collection (not explicit source/entity registration)?

- **Simpler for users** - Just define features, sources/entities are discovered
- **DRY** - Don't repeat source/entity in two places
- **Dagster-inspired** - Define resources, use them, framework discovers dependencies

### Why Keep Original Timestamp Column Name?

- **Predictability** - Users know exactly which column contains timestamp
- **No surprises** - Avoids "where did my column go?" confusion
- **Explicit aliasing** - Users can alias via `Timestamp(alias=...)` when needed

### Why Require Subcommand for `mlforge list`?

- **Consistency** - All list operations follow same pattern
- **Extensibility** - Easy to add new list subcommands (versions, tags, etc.)
- **Clarity** - Explicit is better than implicit
